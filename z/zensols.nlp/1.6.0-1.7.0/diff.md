# Comparing `tmp/zensols.nlp-1.6.0-py3-none-any.whl.zip` & `tmp/zensols.nlp-1.7.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,24 +1,26 @@
-Zip file size: 45766 bytes, number of entries: 22
--rw-rw-r--  2.0 unx      155 b- defN 23-Apr-06 01:02 zensols/nlp/__init__.py
--rw-rw-r--  2.0 unx     9404 b- defN 23-Apr-06 01:02 zensols/nlp/combine.py
--rw-rw-r--  2.0 unx     6724 b- defN 23-Apr-06 01:02 zensols/nlp/component.py
--rw-rw-r--  2.0 unx    37063 b- defN 23-Apr-06 01:02 zensols/nlp/container.py
--rw-rw-r--  2.0 unx     1395 b- defN 23-Apr-06 01:02 zensols/nlp/dataframe.py
--rw-rw-r--  2.0 unx     1401 b- defN 23-Apr-06 01:02 zensols/nlp/decorate.py
--rw-rw-r--  2.0 unx     6143 b- defN 23-Apr-06 01:02 zensols/nlp/domain.py
--rw-rw-r--  2.0 unx    17169 b- defN 23-Apr-06 01:02 zensols/nlp/norm.py
--rw-rw-r--  2.0 unx    25308 b- defN 23-Apr-06 01:02 zensols/nlp/parser.py
--rw-rw-r--  2.0 unx    15306 b- defN 23-Apr-06 01:02 zensols/nlp/score.py
--rw-rw-r--  2.0 unx     5856 b- defN 23-Apr-06 01:02 zensols/nlp/serial.py
--rw-rw-r--  2.0 unx      581 b- defN 23-Apr-06 01:02 zensols/nlp/stemmer.py
--rw-rw-r--  2.0 unx    18057 b- defN 23-Apr-06 01:02 zensols/nlp/tok.py
--rw-r--r--  2.0 unx      362 b- defN 23-Apr-06 01:02 zensols/nlp/resources/component.conf
--rw-r--r--  2.0 unx      823 b- defN 23-Apr-06 01:02 zensols/nlp/resources/mapper.conf
--rw-rw-r--  2.0 unx     1359 b- defN 23-Apr-06 01:02 zensols/nlp/resources/obj.conf
--rw-r--r--  2.0 unx      839 b- defN 23-Apr-06 01:02 zensols/nlp/resources/score.yml
--rw-r--r--  2.0 unx      420 b- defN 23-Apr-06 01:02 zensols/nlp/resources/serial.conf
--rw-rw-r--  2.0 unx     5553 b- defN 23-Apr-06 01:02 zensols.nlp-1.6.0.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Apr-06 01:02 zensols.nlp-1.6.0.dist-info/WHEEL
--rw-rw-r--  2.0 unx       12 b- defN 23-Apr-06 01:02 zensols.nlp-1.6.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     1784 b- defN 23-Apr-06 01:02 zensols.nlp-1.6.0.dist-info/RECORD
-22 files, 155806 bytes uncompressed, 42894 bytes compressed:  72.5%
+Zip file size: 52884 bytes, number of entries: 24
+-rw-rw-r--  2.0 unx      131 b- defN 23-Jun-07 19:52 zensols/nlp/__init__.py
+-rw-rw-r--  2.0 unx     9404 b- defN 23-Jun-07 19:52 zensols/nlp/combine.py
+-rw-rw-r--  2.0 unx     6724 b- defN 23-Jun-07 19:52 zensols/nlp/component.py
+-rw-rw-r--  2.0 unx    43376 b- defN 23-Jun-07 19:52 zensols/nlp/container.py
+-rw-rw-r--  2.0 unx     1395 b- defN 23-Jun-07 19:52 zensols/nlp/dataframe.py
+-rw-rw-r--  2.0 unx     4261 b- defN 23-Jun-07 19:52 zensols/nlp/decorate.py
+-rw-rw-r--  2.0 unx     8382 b- defN 23-Jun-07 19:52 zensols/nlp/domain.py
+-rw-rw-r--  2.0 unx     6579 b- defN 23-Jun-07 19:52 zensols/nlp/nerscore.py
+-rw-rw-r--  2.0 unx    17214 b- defN 23-Jun-07 19:52 zensols/nlp/norm.py
+-rw-rw-r--  2.0 unx    27802 b- defN 23-Jun-07 19:52 zensols/nlp/parser.py
+-rw-rw-r--  2.0 unx    21290 b- defN 23-Jun-07 19:52 zensols/nlp/score.py
+-rw-rw-r--  2.0 unx     5856 b- defN 23-Jun-07 19:52 zensols/nlp/serial.py
+-rw-rw-r--  2.0 unx      581 b- defN 23-Jun-07 19:52 zensols/nlp/stemmer.py
+-rw-rw-r--  2.0 unx    18295 b- defN 23-Jun-07 19:52 zensols/nlp/tok.py
+-rw-r--r--  2.0 unx      398 b- defN 23-Jun-07 19:52 zensols/nlp/resources/component.conf
+-rw-r--r--  2.0 unx      659 b- defN 23-Jun-07 19:52 zensols/nlp/resources/decorator.conf
+-rw-r--r--  2.0 unx      823 b- defN 23-Jun-07 19:52 zensols/nlp/resources/mapper.conf
+-rw-rw-r--  2.0 unx     1328 b- defN 23-Jun-07 19:52 zensols/nlp/resources/obj.conf
+-rw-r--r--  2.0 unx     1441 b- defN 23-Jun-07 19:52 zensols/nlp/resources/score.yml
+-rw-r--r--  2.0 unx      420 b- defN 23-Jun-07 19:52 zensols/nlp/resources/serial.conf
+-rw-rw-r--  2.0 unx     5699 b- defN 23-Jun-07 19:52 zensols.nlp-1.7.0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Jun-07 19:52 zensols.nlp-1.7.0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       12 b- defN 23-Jun-07 19:52 zensols.nlp-1.7.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     1957 b- defN 23-Jun-07 19:52 zensols.nlp-1.7.0.dist-info/RECORD
+24 files, 184119 bytes uncompressed, 49742 bytes compressed:  73.0%
```

## zipnote {}

```diff
@@ -15,14 +15,17 @@
 
 Filename: zensols/nlp/decorate.py
 Comment: 
 
 Filename: zensols/nlp/domain.py
 Comment: 
 
+Filename: zensols/nlp/nerscore.py
+Comment: 
+
 Filename: zensols/nlp/norm.py
 Comment: 
 
 Filename: zensols/nlp/parser.py
 Comment: 
 
 Filename: zensols/nlp/score.py
@@ -36,32 +39,35 @@
 
 Filename: zensols/nlp/tok.py
 Comment: 
 
 Filename: zensols/nlp/resources/component.conf
 Comment: 
 
+Filename: zensols/nlp/resources/decorator.conf
+Comment: 
+
 Filename: zensols/nlp/resources/mapper.conf
 Comment: 
 
 Filename: zensols/nlp/resources/obj.conf
 Comment: 
 
 Filename: zensols/nlp/resources/score.yml
 Comment: 
 
 Filename: zensols/nlp/resources/serial.conf
 Comment: 
 
-Filename: zensols.nlp-1.6.0.dist-info/METADATA
+Filename: zensols.nlp-1.7.0.dist-info/METADATA
 Comment: 
 
-Filename: zensols.nlp-1.6.0.dist-info/WHEEL
+Filename: zensols.nlp-1.7.0.dist-info/WHEEL
 Comment: 
 
-Filename: zensols.nlp-1.6.0.dist-info/top_level.txt
+Filename: zensols.nlp-1.7.0.dist-info/top_level.txt
 Comment: 
 
-Filename: zensols.nlp-1.6.0.dist-info/RECORD
+Filename: zensols.nlp-1.7.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## zensols/nlp/__init__.py

```diff
@@ -1,7 +1,6 @@
 from .domain import *
 from .norm import *
 from .tok import *
 from .container import *
 from .parser import *
-from .decorate import *
 from .combine import *
```

## zensols/nlp/container.py

```diff
@@ -1,41 +1,52 @@
-from __future__ import annotations
 """Domain objects that define features associated with text.
 
 """
+from __future__ import annotations
 __author__ = 'Paul Landes'
-
-from typing import List, Tuple, Iterable, Dict, Type, Any, ClassVar, Set
+from typing import List, Tuple, Iterable, Dict, Type, Any, ClassVar, Set, Union
 from dataclasses import dataclass, field
 import dataclasses
 from abc import ABCMeta, abstractmethod
 import sys
 import logging
+import textwrap as tw
 import itertools as it
 from itertools import chain
 import copy
 from io import TextIOBase, StringIO
 from frozendict import frozendict
 from interlap import InterLap
 from spacy.tokens import Doc, Span, Token
-from zensols.persist import PersistableContainer, persisted
+from zensols.persist import PersistableContainer, persisted, PersistedWork
 from . import NLPError, TextContainer, FeatureToken, LexicalSpan
 
 logger = logging.getLogger(__name__)
 
 
 class TokenContainer(PersistableContainer, TextContainer, metaclass=ABCMeta):
-    """Each instance has the following attributes:
+    """A base class for token container classes such as
+    :class:`.FeatureSentence` and :class:`.FeatureDocument`.  In addition to the
+    defined methods, each instance has a ``text`` attribute, which is the
+    original text of the document.
 
     """
+    CANONICAL_DELIMITER: ClassVar[str] = '|'
+    """The token delimiter used in :obj:`canonical`."""
+
     _POST_SPACE_SKIP: ClassVar[Set[str]] = frozenset("""`‘“[({<-""")
     _PRE_SPACE_SKIP: ClassVar[Set[str]] = frozenset(
         "'s n't 'll 'm 've 'd 're -".split())
     _LONGEST_PRE_SPACE_SKIP: ClassVar[int] = max(map(len, _PRE_SPACE_SKIP))
 
+    def __post_init__(self):
+        super().__init__()
+        self._norm = PersistedWork('_norm', self, transient=True)
+        self._entities = PersistedWork('_entities', self, transient=True)
+
     @abstractmethod
     def token_iter(self, *args, **kwargs) -> Iterable[FeatureToken]:
         """Return an iterator over the token features.
 
         :param args: the arguments given to :meth:`itertools.islice`
 
         """
@@ -92,15 +103,15 @@
 
         :param args: the arguments given to :meth:`itertools.islice`
 
         """
         return map(lambda t: t.norm, self.token_iter(*args, **kwargs))
 
     @property
-    @persisted('_norm', transient=True)
+    @persisted('_norm')
     def norm(self) -> str:
         """The normalized version of the sentence."""
         return self._calc_norm()
 
     def _calc_norm(self) -> str:
         """Create a string that follows English spacing rules."""
         nsent: str
@@ -129,14 +140,25 @@
                 sio.write(norm)
             nsent = sio.getvalue()
         else:
             nsent = ' '.join(self.norm_token_iter())
         return nsent
 
     @property
+    @persisted('_canonical', transient=True)
+    def canonical(self) -> str:
+        """A canonical representation of the container, which are non-space
+        tokens separated by :obj:`CANONICAL_DELIMITER`.
+
+        """
+        return self.CANONICAL_DELIMITER.join(
+            map(lambda t: t.text,
+                filter(lambda t: not t.is_space, self.token_iter())))
+
+    @property
     @persisted('_tokens', transient=True)
     def tokens(self) -> Tuple[FeatureToken, ...]:
         """Return the token features as a tuple.
 
         """
         return tuple(self.token_iter())
 
@@ -160,44 +182,67 @@
 
     @persisted('_interlap', transient=True)
     def _get_interlap(self) -> InterLap:
         """Create an interlap with all tokens of the container added."""
         il = InterLap()
         # adding with tuple inline is ~3 times as fast than a list, and ~9 times
         # faster than an individual add in a for loop
-        il.add(tuple(map(lambda t: (t.lexspan.begin, t.lexspan.end - 1, t),
-                         self.token_iter())))
+        spans: Tuple[Tuple[int, int]] = tuple(
+            map(lambda t: (t.lexspan.begin, t.lexspan.end - 1, t),
+                self.token_iter()))
+        if len(spans) > 0:
+            il.add(spans)
         return il
 
-    def map_overlapping_tokens(self, spans: Iterable[LexicalSpan]) -> \
+    def map_overlapping_tokens(self, spans: Iterable[LexicalSpan],
+                               inclusive: bool = True) -> \
             Iterable[Tuple[FeatureToken, ...]]:
         """Return a tuple of tokens, each tuple in the range given by the
         respective span in ``spans``.
 
         :param spans: the document 0-index character based inclusive spans to
                       compare with :obj:`.FeatureToken.lexspan`
 
+        :param inclusive: whether to check include +1 on the end component
+
         :return: a tuple of matching tokens for the respective ``span`` query
 
         """
+        def map_span(s: LexicalSpan) -> Tuple[FeatureToken]:
+            toks = map(lambda m: m[2], il.find(s.astuple))
+            # we have to manually check non-inclusive right intervals since
+            # InterLap includes it
+            if not inclusive:
+                toks = filter(lambda t: t.lexspan.overlaps_with(s, False), toks)
+            return tuple(toks)
+
         il = self._get_interlap()
-        return map(lambda s: tuple(map(lambda m: m[2], il.find(s.astuple))),
-                   spans)
+        return map(map_span, spans)
 
-    def get_overlapping_tokens(self, span: LexicalSpan) -> \
+    def get_overlapping_tokens(self, span: LexicalSpan,
+                               inclusive: bool = True) -> \
             Iterable[FeatureToken]:
         """Get all tokens that overlap lexical span ``span``.
 
         :param span: the document 0-index character based inclusive span to
                      compare with :obj:`.FeatureToken.lexspan`
 
+        :param inclusive: whether to check include +1 on the end component
+
         :return: a token sequence containing the 0 index offset of ``span``
 
         """
-        return next(iter(self.map_overlapping_tokens((span,))))
+        return next(iter(self.map_overlapping_tokens((span,), inclusive)))
+
+    def get_overlapping_span(self, span: LexicalSpan,
+                             inclusive: bool = True) -> TokenContainer:
+        """Return a feature span that includes the lexical scope of ``span``."""
+        sent = FeatureSentence(tokens=self.tokens, text=self.text)
+        doc = FeatureDocument(sents=(sent,), text=self.text)
+        return doc.get_overlapping_document(span, inclusive=inclusive)
 
     @abstractmethod
     def to_sentence(self, limit: int = sys.maxsize,
                     contiguous_i_sent: bool = False) -> FeatureSentence:
         """Coerce this instance to a single sentence.  No tokens data is updated
         so :obj:`.FeatureToken.i_sent` keep their original indexes.  These
         sentence indexes will be inconsistent when called on
@@ -233,15 +278,15 @@
         :return: the cloned instance of this instance
 
         """
         cls = self.__class__ if cls is None else cls
         return cls(**kwargs)
 
     @property
-    @persisted('_entities', transient=True)
+    @persisted('_entities')
     def entities(self) -> Tuple[FeatureSpan, ...]:
         """The named entities of the container with each multi-word entity as
         elements.
 
         """
         return self._get_entities()
 
@@ -301,14 +346,29 @@
 
         """
         i: int
         ft: FeatureToken
         for i, ft in self.tokens_by_i.items():
             ft.i = i
 
+    @abstractmethod
+    def update_entity_spans(self, include_idx: bool = True):
+        """Update token entity to :obj:`norm` text.  This is helpful when
+        entities are embedded after splitting text, which becomes
+        :obj:`.FeatureToken.norm` values.  However, the token spans still index
+        the original entities that are multi-word, which leads to norms that are
+        not equal to the text spans.  This synchronizes the token span indexes
+        with the norms.
+
+        :param include_idx: whether to update :obj:`.SpacyFeatureToken.idx` as
+                            well
+
+        """
+        pass
+
     def write(self, depth: int = 0, writer: TextIOBase = sys.stdout,
               include_original: bool = False, include_normalized: bool = True,
               n_tokens: int = sys.maxsize, inline: bool = False):
         """Write the text container.
 
         :param include_original: whether to include the original text
 
@@ -319,21 +379,53 @@
         :param inline: whether to print the tokens on one line each
 
         """
         super().write(depth, writer,
                       include_original=include_original,
                       include_normalized=include_normalized)
         if n_tokens > 0:
-            self._write_line('tokens:', depth + 1, writer)
+            self._write_line('tokens:', depth, writer)
             for t in it.islice(self.token_iter(), n_tokens):
                 if inline:
-                    t.write_attributes(depth + 2, writer,
+                    t.write_attributes(depth + 1, writer,
                                        inline=True, include_type=False)
                 else:
-                    t.write(depth + 2, writer)
+                    t.write(depth + 1, writer)
+
+    def write_text(self, depth: int = 0, writer: TextIOBase = sys.stdout,
+                   include_original: bool = False,
+                   include_normalized: bool = True,
+                   limit: int = sys.maxsize):
+        """Write only the text of the container.
+
+        :param include_original: whether to include the original text
+
+        :param include_normalized: whether to include the normalized text
+
+        :param limit: the max number of characters to print
+
+        """
+        inc_both: bool = include_original and include_normalized
+        add_depth = 1 if inc_both else 0
+        if include_original:
+            if inc_both:
+                self._write_line('original:', depth, writer)
+            text: str = tw.shorten(self.text, limit)
+            self._write_wrap(text, depth + add_depth, writer)
+        if include_normalized:
+            if inc_both:
+                self._write_line('normalized:', depth, writer)
+            norm: str = tw.shorten(self.norm, limit)
+            self._write_wrap(norm, depth + add_depth, writer)
+
+    def __getitem__(self, key: Union[LexicalSpan, int]) -> \
+            Union[FeatureToken, TokenContainer]:
+        if isinstance(key, LexicalSpan):
+            return self.get_overlapping_span(key, inclusive=False)
+        return self.tokens[key]
 
     def __hash__(self) -> int:
         return hash(self.norm)
 
     def __str__(self):
         return TextContainer.__str__(self)
 
@@ -359,15 +451,15 @@
     spacy_span: Span = field(default=None, repr=False, compare=False)
     """The parsed spaCy span this feature set is based.
 
     :see: :meth:`.FeatureDocument.spacy_doc`
 
     """
     def __post_init__(self):
-        super().__init__()
+        super().__post_init__()
         if self.text is None:
             self.text = ' '.join(map(lambda t: t.text, self.tokens))
         # the _tokens setter is called to set the tokens before the the
         # spacy_span set; so call it again since now we have spacy_span set
         self._set_entity_spans()
 
     @property
@@ -378,14 +470,17 @@
     def _tokens(self, tokens: Tuple[FeatureToken, ...]):
         if not isinstance(tokens, tuple):
             raise NLPError(
                 f'Expecting tuple of tokens, but got {type(tokens)}')
         self._tokens_val = tokens
         self._ents: List[Tuple[int, int]] = []
         self._set_entity_spans()
+        if hasattr(self, '_norm'):
+            # the __post_init__ is called after this setter for EMPTY_SENTENCE
+            self._norm.clear()
 
     def _set_entity_spans(self):
         if self.spacy_span is not None:
             for ents in self.spacy_span.ents:
                 start, end = None, None
                 ents = iter(ents)
                 try:
@@ -396,25 +491,24 @@
                     pass
                 if start is not None:
                     self._ents.append((start.idx, end.idx))
 
     def _strip(self):
         self.tokens = tuple(self.strip_tokens(self.tokens))
         self.text = self.text.strip()
-        self._set_entity_spans()
 
     def to_sentence(self, limit: int = sys.maxsize,
                     contiguous_i_sent: bool = False) -> FeatureSentence:
         if limit == 0:
             return iter(())
         else:
             return self.clone(FeatureSentence)
 
     def to_document(self) -> FeatureDocument:
-        return FeatureDocument(self.to_sentence(),)
+        return FeatureDocument((self.to_sentence(),))
 
     def clone(self, cls: Type = None, **kwargs) -> TokenContainer:
         params = dict(kwargs)
         if 'tokens' not in params:
             params['tokens'] = tuple(
                 map(lambda t: t.clone(), self._tokens_val))
         if 'text' not in params:
@@ -504,14 +598,33 @@
     def update_indexes(self):
         super().update_indexes()
         i_sent: int
         ft: FeatureToken
         for i_sent, ft in self.tokens_by_i_sent.items():
             ft.i_sent = i_sent
 
+    def update_entity_spans(self, include_idx: bool = True):
+        split_ents: List[Tuple[int, int]] = []
+        fspan: FeatureSpan
+        for fspan in self.entities:
+            beg: int = fspan[0].idx
+            tok: FeatureToken
+            for tok in fspan:
+                ls: LexicalSpan = tok.lexspan
+                end: int = beg + len(tok.norm)
+                if ls.begin != beg or ls.end != end:
+                    ls = LexicalSpan(beg, end)
+                    tok.lexspan = ls
+                if include_idx:
+                    tok.idx = beg
+                split_ents.append((beg, beg))
+                beg = end + 1
+        self._ents = split_ents
+        self._entities.clear()
+
     def _branch(self, node: FeatureToken, toks: Tuple[FeatureToken, ...],
                 tid_to_idx: Dict[int, int]) -> \
             Dict[FeatureToken, List[FeatureToken]]:
         clds = {}
         for c in node.children:
             cix = tid_to_idx.get(c)
             if cix:
@@ -534,17 +647,14 @@
             return {}
 
     def _from_dictable(self, recurse: bool, readable: bool,
                        class_name_param: str = None) -> Dict[str, Any]:
         return {'text': self.text,
                 'tokens': self._from_object(self.tokens, recurse, readable)}
 
-    def __getitem__(self, key) -> FeatureToken:
-        return self.tokens[key]
-
     def __len__(self) -> int:
         return self.token_len
 
     def __iter__(self):
         return self.token_iter()
 
     def __hash__(self) -> int:
@@ -558,40 +668,53 @@
 @dataclass(eq=True, repr=False)
 class FeatureSentence(FeatureSpan):
     """A container class of tokens that make a sentence.  Instances of this class
     iterate over :class:`.FeatureToken` instances, and can create documents
     with :meth:`to_document`.
 
     """
+    EMPTY_SENTENCE: ClassVar[FeatureSentence]
+
     def to_sentence(self, limit: int = sys.maxsize) -> FeatureSentence:
         if limit == 0:
             return iter(())
         else:
             return self
 
     def to_document(self) -> FeatureDocument:
         return FeatureDocument((self,))
 
+    def get_overlapping_span(self, span: LexicalSpan,
+                             inclusive: bool = True) -> TokenContainer:
+        doc = FeatureDocument(sents=(self,), text=self.text)
+        return doc.get_overlapping_document(span, inclusive=inclusive)
+
     def __hash__(self) -> int:
         return hash(self.norm)
 
 
+FeatureSentence.EMPTY_SENTENCE = FeatureSentence(tokens=(), text='')
+
+
 @dataclass(eq=True, repr=False)
 class FeatureDocument(TokenContainer):
     """A container class of tokens that make a document.  This class contains a
     one to many of sentences.  However, it can be treated like any
     :class:`.TokenContainer` to fetch tokens.  Instances of this class iterate
     over :class:`.FeatureSentence` instances.
 
     :param sents: the sentences defined for this document
 
     .. document private functions
     .. automethod:: _combine_documents
 
     """
+    EMPTY_DOCUMENT: ClassVar[FeatureDocument] = None
+    """A zero length document."""
+
     _PERSITABLE_TRANSIENT_ATTRIBUTES = {'spacy_doc'}
     """Don't serialize the spacy document on persistance pickling."""
 
     sents: Tuple[FeatureSentence, ...] = field()
     """The sentences that make up the document."""
 
     text: str = field(default=None)
@@ -601,15 +724,15 @@
     """The parsed spaCy document this feature set is based.  As explained in
     :class:`~zensols.nlp.FeatureToken`, spaCy documents are heavy weight and
     problematic to pickle.  For this reason, this attribute is dropped when
     pickled, and only here for ad-hoc predictions.
 
     """
     def __post_init__(self):
-        super().__init__()
+        super().__post_init__()
         if self.text is None:
             self.text = ''.join(map(lambda s: s.text, self.sent_iter()))
         if not isinstance(self.sents, tuple):
             raise NLPError(
                 f'Expecting tuple of sentences, but got {type(self.sents)}')
 
     def set_spacy_doc(self, doc: Doc):
@@ -721,17 +844,24 @@
     def _get_tokens_by_i(self) -> Dict[int, FeatureToken]:
         by_i = {}
         for sent in self.sents:
             by_i.update(sent.tokens_by_i)
         return by_i
 
     def update_indexes(self):
+        sent: FeatureSentence
         for sent in self.sents:
             sent.update_indexes()
 
+    def update_entity_spans(self, include_idx: bool = True):
+        sent: FeatureSentence
+        for sent in self.sents:
+            sent.update_entity_spans(include_idx)
+        self._entities.clear()
+
     def sentence_index_for_token(self, token: FeatureToken) -> int:
         """Return index of the parent sentence having ``token``."""
         return self._id_to_sent()[token.idx]
 
     def sentence_for_token(self, token: FeatureToken) -> FeatureSentence:
         """Return the parent sentence that has ``token``."""
         six: int = self.sentence_index_for_token(token)
@@ -866,45 +996,61 @@
         else:
             return self
 
     def _get_entities(self) -> Tuple[FeatureSpan, ...]:
         return tuple(chain.from_iterable(
             map(lambda s: s.entities, self.sents)))
 
-    def get_overlapping_sentences(self, span: LexicalSpan) -> \
+    def get_overlapping_span(self, span: LexicalSpan,
+                             inclusive: bool = True) -> TokenContainer:
+        """Return a feature span that includes the lexical scope of ``span``."""
+        return self.get_overlapping_document(span, inclusive=inclusive)
+
+    def get_overlapping_sentences(self, span: LexicalSpan,
+                                  inclusive: bool = True) -> \
             Iterable[FeatureSentence]:
-        """Return sentences that overlaps with ``span`` from this document."""
+        """Return sentences that overlaps with ``span`` from this document.
+
+        :param span: indicates the portion of the document to retain
+
+        :param inclusive: whether to check include +1 on the end component
+
+        """
         for sent in self.sents:
             if sent.lexspan.overlaps_with(span):
                 yield sent
 
-    def get_overlapping_document(self, span: LexicalSpan) -> FeatureDocument:
+    def get_overlapping_document(self, span: LexicalSpan,
+                                 inclusive: bool = True) -> FeatureDocument:
         """Get the portion of the document that overlaps ``span``.  For
         sentences that are completely enclosed in the span, the sentences are
         copied.  Otherwise, new sentences are created from those tokens that
         overlap the span.
 
         :param span: indicates the portion of the document to retain
 
+        :param inclusive: whether to check include +1 on the end component
+
         :return: a new document that contains the 0 index offset of ``span``
 
         """
+        send: int = 1 if inclusive else 0
         doc = self.clone()
         if span != self.lexspan:
             doc_text: str = self.text
             sents: List[FeatureSentence] = []
             for sent in self.sent_iter():
-                toks = list(sent.get_overlapping_tokens(span))
+                toks = list(sent.get_overlapping_tokens(span, inclusive))
                 if len(toks) == 0:
                     continue
                 elif len(toks) == len(sent):
                     pass
                 else:
-                    text: str = doc_text[toks[0].idx:toks[-1].idx + 1]
-                    hang = (span.end + 1) - toks[-1].lexspan.end
+                    text: str = doc_text[toks[0].idx:toks[-1].idx + send]
+                    hang = (span.end + send) - toks[-1].lexspan.end
                     if hang < 0:
                         tok = toks[-1]
                         clone = copy.deepcopy(tok)
                         clone.norm = tok.norm[:hang]
                         clone.text = tok.text[:hang]
                         toks[-1] = clone
                     hang = toks[0].lexspan.begin - span.begin
@@ -913,18 +1059,19 @@
                         tok = toks[0]
                         clone = copy.deepcopy(tok)
                         clone.norm = tok.norm[hang:]
                         clone.text = tok.text[hang:]
                         toks[0] = clone
                     sent = sent.clone(tokens=tuple(toks), text=text)
                 sents.append(sent)
-            text: str = doc_text[span.begin:span.end + 1]
+            text: str = doc_text[span.begin:span.end + send]
             doc.sents = tuple(sents)
             doc.text = text
-            body_len = sum(1 for _ in doc.get_overlapping_tokens(span))
+            body_len = sum(
+                1 for _ in doc.get_overlapping_tokens(span, inclusive))
             assert body_len == doc.token_len
         return doc
 
     def from_sentences(self, sents: Iterable[FeatureSentence],
                        deep: bool = False) -> FeatureDocument:
         """Return a new cloned document using the given sentences.
 
@@ -955,38 +1102,44 @@
         :param include_original: whether to include the original text
 
         :param include_normalized: whether to include the normalized text
 
         """
         TextContainer.write(self, depth, writer,
                             include_original=include_original)
-        self._write_line('sentences:', depth + 1, writer)
+        self._write_line('sentences:', depth, writer)
         s: FeatureSentence
         for s in it.islice(self.sents, n_sents):
-            s.write(depth + 2, writer, n_tokens=n_tokens,
+            s.write(depth + 1, writer, n_tokens=n_tokens,
                     include_original=include_original)
 
     def _from_dictable(self, recurse: bool, readable: bool,
                        class_name_param: str = None) -> Dict[str, Any]:
         return {'text': self.text,
                 'sentences': self._from_object(self.sents, recurse, readable)}
 
-    def __getitem__(self, key):
+    def __getitem__(self, key: Union[LexicalSpan, int]) -> \
+            Union[FeatureSentence, TokenContainer]:
+        if isinstance(key, LexicalSpan):
+            return self.get_overlapping_span(key, inclusive=False)
         return self.sents[key]
 
     def __len__(self):
         return len(self.sents)
 
     def __iter__(self):
         return self.sent_iter()
 
     def __hash__(self) -> int:
         return hash(self.norm)
 
 
+FeatureDocument.EMPTY_DOCUMENT = FeatureDocument(sents=(), text='')
+
+
 @dataclass
 class TokenAnnotatedFeatureSentence(FeatureSentence):
     """A feature sentence that contains token annotations.
 
     """
     annotations: Tuple[Any, ...] = field(default=())
     """A token level annotation, which is one-to-one to tokens."""
```

## zensols/nlp/decorate.py

```diff
@@ -1,23 +1,24 @@
 """Contains useful classes for decorating feature sentences.
 
 """
 __author__ = 'Paul Landes'
 
 from typing import List, Tuple
-from dataclasses import dataclass
+from dataclasses import dataclass, field
 import re
-from spacy.tokens import Span
+from spacy.tokens import Span, Doc
 from . import (
-    LexicalSpan, FeatureToken, FeatureSentence, SpacyFeatureSentenceDecorator
+    LexicalSpan, FeatureToken, FeatureSentence, FeatureDocument,
+    FeatureSentenceDecorator, FeatureDocumentDecorator
 )
 
 
 @dataclass
-class SplitTokenSentenceDecorator(SpacyFeatureSentenceDecorator):
+class SplitTokenSentenceDecorator(FeatureSentenceDecorator):
     """A decorator that splits feature tokens by white space.
 
     """
     def _split_tok(self, ftok: FeatureToken, matches: Tuple[re.Match]):
         toks: List[FeatureToken] = []
         norm: str
         for match in matches:
@@ -25,17 +26,102 @@
             ctok.norm = match.group(0)
             ctok.lexspan = LexicalSpan(ftok.lexspan.begin + match.start(0),
                                        ftok.lexspan.begin + match.end(0))
             ctok.idx = ctok.lexspan.begin
             toks.append(ctok)
         return toks
 
-    def decorate(self, spacy_sent: Span, feature_sent: FeatureSentence):
+    def decorate(self, sent: FeatureSentence):
         split_toks: List[FeatureToken] = []
         tok: FeatureToken
-        for ftok in feature_sent.token_iter():
+        for ftok in sent.token_iter():
             tnorms: Tuple[str, ...] = tuple(re.finditer(r'\S+', ftok.norm))
             if len(tnorms) == 1:
                 split_toks.append(ftok)
             else:
                 split_toks.extend(self._split_tok(ftok, tnorms))
-        feature_sent.tokens = tuple(split_toks)
+        if sent.token_len != len(split_toks):
+            sent.tokens = tuple(split_toks)
+
+
+@dataclass
+class StripSentenceDecorator(FeatureSentenceDecorator):
+    """A decorator that strips whitespace from sentences.
+
+    :see: :meth:`.TokenContainer.strip`
+
+    """
+    def decorate(self, sent: FeatureSentence):
+        sent.strip()
+
+
+@dataclass
+class FilterTokenSentenceDecorator(FeatureSentenceDecorator):
+    """A decorator that strips whitespace from sentences.
+
+    :see: :meth:`.TokenContainer.strip`
+
+    """
+    remove_stop: bool = field(default=False)
+    remove_space: bool = field(default=False)
+    remove_pronouns: bool = field(default=False)
+    remove_punctuation: bool = field(default=False)
+    remove_determiners: bool = field(default=False)
+    remove_empty: bool = field(default=False)
+
+    def decorate(self, sent: FeatureSentence):
+        def filter_tok(t: FeatureToken) -> bool:
+            return \
+                (not self.remove_stop or not t.is_stop) and \
+                (not self.remove_space or not t.is_space) and \
+                (not self.remove_pronouns or not t.pos_ == 'PRON') and \
+                (not self.remove_punctuation or not t.is_punctuation) and \
+                (not self.remove_determiners or not t.tag_ == 'DT') and \
+                (not self.remove_empty or len(t.norm) > 0)
+        toks: Tuple[FeatureToken] = tuple(filter(filter_tok, sent))
+        if sent.token_len != len(toks):
+            sent.tokens = toks
+
+
+@dataclass
+class FilterEmptySentenceDocumentDecorator(FeatureDocumentDecorator):
+    """Filter zero length sentences.
+
+    """
+    filter_space: bool = field(default=True)
+    """Whether to filter space tokens when comparing zero length sentences."""
+
+    def _filter_empty_sentences(self, fsent: FeatureSentence) -> bool:
+        toks: Tuple[FeatureToken] = fsent.tokens
+        if self.filter_space:
+            toks = tuple(filter(lambda t: not t.is_space, fsent.token_iter()))
+        return len(toks) > 0
+
+    def decorate(self, doc: FeatureDocument):
+        olen: int = len(doc)
+        fsents: Tuple[FeatureSentence] = tuple(filter(
+            self._filter_empty_sentences, doc.sents))
+        nlen: int = len(fsents)
+        if olen != nlen:
+            doc.sents = fsents
+
+
+@dataclass
+class UpdateDocumentDecorator(FeatureDocumentDecorator):
+    """Updates document indexes and spans (see fields).
+
+    """
+    update_indexes: bool = field(default=True)
+    """Whether to update the document indexes with
+    :meth:`.FeatureDocument.update_indexes`.
+
+    """
+    update_entity_spans: bool = field(default=True)
+    """Whether to update the document indexes with
+    :meth:`.FeatureDocument.update_entity_spans`.
+
+    """
+    def decorate(self, doc: FeatureDocument):
+        if self.update_indexes:
+            doc.update_indexes()
+        if self.update_entity_spans:
+            doc.update_entity_spans()
```

## zensols/nlp/domain.py

```diff
@@ -1,14 +1,16 @@
 from __future__ import annotations
 """Interfaces, contracts and errors.
 
 """
 __author__ = 'Paul Landes'
 
-from typing import Tuple, Union, Optional, ClassVar, Set
+from typing import (
+    Tuple, Union, Optional, ClassVar, Set, Iterable, Sequence, List, Type
+)
 from abc import ABCMeta
 import sys
 from io import TextIOBase
 import textwrap as tw
 from spacy.tokens import Token
 from spacy.tokens import Span
 from spacy.tokens import Doc
@@ -56,14 +58,24 @@
 
     @property
     def astuple(self) -> Tuple[int, int]:
         """The span as a ``(begin, end)`` tuple."""
         return (self.begin, self.end)
 
     @classmethod
+    def from_tuples(cls: Type, tups: Iterable[Tuple[int, int]]) -> \
+            Iterable[LexicalSpan]:
+        """Create spans from tuples.
+
+        :param tups: an iterable of ``(<begin>, <end)`` tuples
+
+        """
+        return map(lambda t: cls(*t), tups)
+
+    @classmethod
     def from_token(cls, tok: Union[Token, Span]) -> Tuple[int, int]:
         """Create a span from a spaCy :class:`~spacy.tokens.Token` or
         :class:`~spacy.tokens.Span`.
 
         """
         if isinstance(tok, Span):
             doc: Doc = tok.doc
@@ -122,14 +134,66 @@
                 nar = self
             elif beg == other.begin and end == other.end:
                 nar = other
             else:
                 nar = LexicalSpan(beg, end)
         return nar
 
+    @staticmethod
+    def widen(others: Iterable[LexicalSpan]) -> Optional[LexicalSpan]:
+        """Take the span union by using the left most :obj:`begin` and the right
+        most :obj:`end`.
+
+        :param others: the spans to union
+
+        :return: the widest span that inclusively aggregates ``others``, or None
+                 if an empty sequence is passed
+
+        """
+        begs = sorted(others, key=lambda s: s.begin)
+        if len(begs) > 0:
+            ends = sorted(begs, key=lambda s: s.end)
+            return LexicalSpan(begs[0].begin, ends[-1].end)
+
+    @staticmethod
+    def gaps(spans: Iterable[LexicalSpan], end: Optional[int] = None,
+             nudge_begin: int = 0, nudge_end: int = 0) -> List[LexicalSpan]:
+        """Return the spans for the "holes" in ``spans``.  For example, if
+        ``spans`` is ``((0, 5), (10, 12), (15, 17))``, then return ``((5, 10),
+        (12, 15))``.
+
+        :param spans: the spans used to find gaps
+
+        :param end: an end position for the last gap so that if the last item in
+                    ``spans`` end does not match, another is added
+
+        :return: a list of spans that "fill" any holes in ``spans``
+
+        """
+        spans: List[LexicalSpan] = sorted(spans)
+        gaps: List[LexicalSpan] = []
+        spiter: Iterable[LexicalSpan] = iter(spans)
+        last: LexicalSpan = next(spiter)
+        if last.begin > 0:
+            last = LexicalSpan(0, last.begin)
+            gaps.append(last)
+            spiter = iter(spans)
+        ns: LexicalSpan
+        for ns in spiter:
+            gap: int = ns.begin - last.end
+            if gap > 0:
+                gs = LexicalSpan(last.end + nudge_begin, ns.begin + nudge_end)
+                gaps.append(gs)
+            last = ns
+        # add ending span if the last didn't cover it
+        if end is not None and last.end != end:
+            gaps.append(LexicalSpan(gaps[-1].end + nudge_begin,
+                                    end + nudge_end))
+        return gaps
+
     def write(self, depth: int = 0, writer: TextIOBase = sys.stdout):
         self._write_line(str(self), depth, writer)
 
     def _from_dictable(self, *args, **kwargs):
         # prettier printing
         return dict(super()._from_dictable(*args, **kwargs))
 
@@ -177,21 +241,20 @@
 
     """
     _DEFAULT_TOSTR_LEN: ClassVar[str] = 80
     """Default length of string when rendering :meth:`__str__`."""
 
     def write(self, depth: int = 0, writer: TextIOBase = sys.stdout,
               include_original: bool = True, include_normalized: bool = True):
-        self._write_line(f'{self.__class__.__name__}:', depth, writer)
         if (include_original or include_normalized) and self.text == self.norm:
-            self._write_line(self.text, depth + 1, writer)
+            self._write_line(self.text, depth, writer)
         else:
             if include_original:
-                self._write_line(f'original: {self.text}', depth + 1, writer)
+                self._write_line(f'original: {self.text}', depth, writer)
             if include_normalized:
-                self._write_line(f'normalized: {self.norm}', depth + 1, writer)
+                self._write_line(f'normalized: {self.norm}', depth, writer)
 
     def __str__(self):
         return f'<{tw.shorten(self.norm, width=self._DEFAULT_TOSTR_LEN-2)}>'
 
     def __repr__(self):
         return self.__str__()
```

## zensols/nlp/norm.py

```diff
@@ -242,25 +242,26 @@
     :class:`.SpacyFeatureToken` with spans that have the entity.
 
     """
     token_unit_type: bool = field(default=False)
     """Whether to generate tokens for each split span or a one token span."""
 
     copy_attributes: Tuple[str, ...] = field(default=('label', 'label_'))
+    """Attributes to copy from the span to the split token."""
 
     def map_tokens(self, token_tups: Iterable[Tuple[Token, str]]) -> \
             Iterable[Tuple[Token, str]]:
         def map_tup(tup):
             if logger.isEnabledFor(logging.DEBUG):
                 logger.debug(f'setm: mapping tup: {tup}')
             if isinstance(tup[0], Span):
                 span = tup[0]
                 for tix in range(span.end - span.start):
                     if not token_unit_type:
-                        tok = span[tix:tix+1]
+                        tok = span[tix:tix + 1]
                     else:
                         tok = span[tix]
                     for attr in cp_attribs:
                         setattr(tok, attr, getattr(span, attr))
                     if logger.isEnabledFor(logging.DEBUG):
                         logger.debug(f'setm: split: {tok}')
                     yield (tok, tok.orth_)
@@ -415,16 +416,15 @@
 
     def __post_init__(self):
         self.regex = re.compile(eval(self.regex))
 
     def map_tokens(self, token_tups: Iterable[Tuple[Token, str]]) -> \
             Iterable[Tuple[Token, str]]:
         return (map(lambda x: (x[0], re.sub(
-            self.regex, self.replace_char, x[1])),
-                    token_tups),)
+            self.regex, self.replace_char, x[1])), token_tups),)
 
 
 @dataclass
 class LambdaTokenMapper(TokenMapper):
     """Use a lambda expression to map a token tuple.
 
     This is handy for specialized behavior that can be added directly to a
```

## zensols/nlp/parser.py

```diff
@@ -1,34 +1,35 @@
 from  __future__ import annotations
 """Parse documents and generate features in an organized taxonomy.
 
 """
 __author__ = 'Paul Landes'
 
 from typing import (
-    Tuple, Dict, Any, Sequence, Set, List, Iterable, Type, ClassVar
+    Tuple, Dict, Any, Sequence, Set, List, Iterable, Type, ClassVar, Union
 )
 from dataclasses import dataclass, field
 from abc import abstractmethod, ABCMeta, ABC
 import logging
 import itertools as it
 import sys
 import re
-from io import TextIOBase
+from io import TextIOBase, StringIO
 import spacy
 from spacy.symbols import ORTH
 from spacy.tokens import Doc, Span, Token
 from spacy.language import Language
 from zensols.util import Hasher
+from zensols.config import ImportIniConfig, ImportConfigFactory
 from zensols.persist import (
     persisted, PersistedWork, PersistableContainer, Stash
 )
 from zensols.config import Dictable, ConfigFactory
 from . import (
-    ParseError, TokenNormalizer, FeatureToken, SpacyFeatureToken,
+    NLPError, ParseError, TokenNormalizer, FeatureToken, SpacyFeatureToken,
     FeatureSentence, FeatureDocument,
 )
 
 logger = logging.getLogger(__name__)
 
 
 @dataclass
@@ -171,14 +172,26 @@
     """
     TOKEN_FEATURE_IDS: ClassVar[Set[str]] = FeatureToken.FEATURE_IDS
     """The default value for :obj:`token_feature_ids`."""
 
     def __post_init__(self):
         super().__init__()
 
+    @staticmethod
+    def default_instance() -> FeatureDocumentParser:
+        """Create the parser as configured in the resource library of the
+        package.
+
+        """
+        config: str = (
+            '[import]\n' +
+            'config_file = resource(zensols.nlp): resources/obj.conf')
+        factory = ImportConfigFactory(ImportIniConfig(StringIO(config)))
+        return factory('doc_parser')
+
     @abstractmethod
     def parse(self, text: str, *args, **kwargs) -> FeatureDocument:
         """Parse text or a text as a list of sentences.
 
         :param text: either a string or a list of strings; if the former a
                      document with one sentence will be created, otherwise a
                      document is returned with a sentence for each string in
@@ -195,30 +208,45 @@
         """Invoke :meth:`parse` with the context arguments.
 
         :see: :meth:`parse`
 
         """
         return self.parse(text, *args, **kwargs)
 
+        def __str__(self):
+            return f'model_name: {self.model_name}, lang: {self.lang}'
+
+        def __repr__(self):
+            return self.__str__()
+
 
-class SpacyFeatureTokenDecorator(ABC):
+class FeatureTokenDecorator(ABC):
     """Implementations can add, remove or modify features on a token.
 
     """
     @abstractmethod
-    def decorate(self, spacy_tok: Token, feature_token: FeatureToken):
+    def decorate(self, token: FeatureToken):
         pass
 
 
-class SpacyFeatureSentenceDecorator(ABC):
+class FeatureSentenceDecorator(ABC):
     """Implementations can add, remove or modify features on a sentence.
 
     """
     @abstractmethod
-    def decorate(self, spacy_sent: Span, feature_sent: FeatureSentence):
+    def decorate(self, sent: FeatureSentence):
+        pass
+
+
+class FeatureDocumentDecorator(ABC):
+    """Implementations can add, remove or modify features on a document.
+
+    """
+    @abstractmethod
+    def decorate(self, doc: FeatureDocument):
         pass
 
 
 @dataclass
 class SpacyFeatureDocumentParser(FeatureDocumentParser):
     """This langauge resource parses text in to Spacy documents.  Loaded spaCy
     models have attribute ``doc_parser`` set enable creation of factory
@@ -260,24 +288,30 @@
 
     :see: :obj:`TOKEN_FEATURE_IDS`
 
     """
     components: Sequence[Component] = field(default=())
     """Additional Spacy components to add to the pipeline."""
 
-    token_decorators: Sequence[SpacyFeatureTokenDecorator] = field(default=())
+    token_decorators: Sequence[FeatureTokenDecorator] = field(default=())
     """A list of decorators that can add, remove or modify features on a token.
 
     """
-    sentence_decorators: Sequence[SpacyFeatureSentenceDecorator] = field(
+    sentence_decorators: Sequence[FeatureSentenceDecorator] = field(
         default=())
     """A list of decorators that can add, remove or modify features on a
     sentence.
 
     """
+    document_decorators: Sequence[FeatureDocumentDecorator] = field(
+        default=())
+    """A list of decorators that can add, remove or modify features on a
+    document.
+
+    """
     disable_component_names: Sequence[str] = field(default=None)
     """Components to disable in the spaCy model when creating documents in
     :meth:`parse`.
 
     """
     token_normalizer: TokenNormalizer = field(default=None)
     """The token normalizer for methods that use it, i.e. ``features``."""
@@ -290,34 +324,38 @@
 
     sent_class: Type[FeatureSentence] = field(default=FeatureSentence)
     """The type of sentence instances to create."""
 
     token_class: Type[FeatureToken] = field(default=SpacyFeatureToken)
     """The type of document instances to create."""
 
-    sentence_filters: List[str] = field(default_factory=list)
-    """A list of functions that return a boolean used to filter sentences.
+    remove_empty_sentences: bool = field(default=None)
+    """Deprecated and will be removed from future versions.  Use
+    :class:`.FilterSentenceFeatureDocumentDecorator` instead.
 
     """
-    remove_empty_sentences: bool = field(default=False)
-    """If ``True``, remove sentences that only have space tokens."""
-
     reload_components: bool = field(default=False)
     """Removes, then re-adds components for cached models.  This is helpful for
     when there are component configurations that change on reruns with a
     difference application context but in the same Python interpreter session.
 
     A spaCy component can get other instances via :obj:`config_factory`, but if
     this is ``False`` it will be paired with the first instance of this class
     and not the new ones created with a new configuration factory.
 
     """
     def __post_init__(self):
         super().__post_init__()
         self._model = PersistedWork('_model', self)
+        if self.remove_empty_sentences is not None:
+            import warnings
+            warnings.warn(
+                'remove_empty_sentences is deprecated (use ' +
+                'FilterSentenceFeatureDocumentDecorator instead',
+                DeprecationWarning)
 
     def _create_model_key(self) -> str:
         """Create a unique key used for storing expensive-to-create spaCy language
         models in :obj:`_MODELS`.
 
         """
         comps = sorted(map(lambda c: f'{c.pipe_name}:{hash(c)}',
@@ -439,71 +477,56 @@
             logger.debug(f'kwargs: <{kwargs}>')
         tokens: Tuple[FeatureToken, ...] = \
             map(lambda tup: self._create_token(*tup, *args, **kwargs),
                 self.token_normalizer.normalize(doc))
         return tokens
 
     def _decorate_token(self, spacy_tok: Token, feature_token: FeatureToken):
-        decorator: SpacyFeatureTokenDecorator
+        decorator: FeatureTokenDecorator
         for decorator in self.token_decorators:
-            decorator.decorate(spacy_tok, feature_token)
+            decorator.decorate(feature_token)
 
     def _create_token(self, tok: Token, norm: Tuple[Token, str],
                       *args, **kwargs) -> FeatureToken:
         tp: Type[FeatureToken] = self.token_class
         ft: FeatureToken = tp(tok, norm, *args, **kwargs)
         self._decorate_token(tok, ft)
         if logger.isEnabledFor(logging.DEBUG):
             logger.debug(f'detaching using features: {self.token_feature_ids}')
         return ft.detach(self.token_feature_ids)
 
     def _decorate_sent(self, spacy_sent: Span, feature_sent: FeatureSentence):
-        decorator: SpacyFeatureSentenceDecorator
+        decorator: FeatureSentenceDecorator
         for decorator in self.sentence_decorators:
-            decorator.decorate(spacy_sent, feature_sent)
+            decorator.decorate(feature_sent)
 
     def _create_sent(self, spacy_sent: Span, stoks: Iterable[FeatureToken],
                      text: str) -> FeatureSentence:
         sent: FeatureSentence = self.sent_class(tuple(stoks), text, spacy_sent)
         self._decorate_sent(spacy_sent, sent)
         return sent
 
-    def _filter_sent(self, sent: Span, fsent: FeatureSentence) -> \
-            List[FeatureSentence]:
-        def remove_empty_sentences(sent, fsent) -> bool:
-            return len(sent) > 0 and not all(map(lambda t: t.is_space, sent))
-
-        filters = list(map(eval, self.sentence_filters))
-        if self.remove_empty_sentences:
-            filters.append(remove_empty_sentences)
-        if len(filters) == 0:
-            return True
-        else:
-            return all(map(lambda f: f(sent, fsent), filters))
-
     def _create_sents(self, doc: Doc) -> List[FeatureSentence]:
         """Create sentences from a spaCy doc."""
         toks: Tuple[FeatureToken, ...] = tuple(self._normalize_tokens(doc))
         sents: List[FeatureSentence] = []
-        ntoks = len(toks)
-        tix = 0
+        ntoks: int = len(toks)
+        tix: int = 0
         sent: Span
         for sent in doc.sents:
-            e = sent[-1].i
-            stoks = []
+            e: int = sent[-1].i
+            stoks: List[FeatureToken] = []
             while tix < ntoks:
                 tok = toks[tix]
                 if tok.i <= e:
                     stoks.append(tok)
                 else:
                     break
                 tix += 1
             fsent: FeatureSentence = self._create_sent(sent, stoks, sent.text)
-            if not self._filter_sent(sent, fsent):
-                continue
             sents.append(fsent)
         return sents
 
     def from_spacy_doc(self, doc: Doc, *args, text: str = None,
                        **kwargs) -> FeatureDocument:
         """Create s :class:`.FeatureDocument` from a spaCy doc.
 
@@ -526,20 +549,28 @@
         try:
             return self.doc_class(tuple(sents), text, doc, *args, **kwargs)
         except Exception as e:
             raise ParseError(
                 f'Could not parse <{text}> for {self.doc_class} ' +
                 f"with args {args} for parser '{self.name}'") from e
 
+    def _decorate_doc(self, spacy_doc: Span, feature_doc: FeatureDocument):
+        decorator: FeatureDocumentDecorator
+        for decorator in self.document_decorators:
+            decorator.decorate(feature_doc)
+
     def parse(self, text: str, *args, **kwargs) -> FeatureDocument:
         if not isinstance(text, str):
             raise ParseError(
                 f'Expecting string text but got: {text} ({type(str)})')
-        doc: Doc = self.parse_spacy_doc(text)
-        return self.from_spacy_doc(doc, *args, text=text, **kwargs)
+        sdoc: Doc = self.parse_spacy_doc(text)
+        fdoc: FeatureDocument = self.from_spacy_doc(
+            sdoc, *args, text=text, **kwargs)
+        self._decorate_doc(sdoc, fdoc)
+        return fdoc
 
     def to_spacy_doc(self, doc: FeatureDocument, norm: bool = True,
                      add_features: Set[str] = None) -> Doc:
         """Convert a feature document back in to a spaCy document.
 
         **Note**: not all data is copied--only text, ``pos_``, ``tag_``,
         ``lemma_`` and ``dep_``.
@@ -588,19 +619,60 @@
                 params['heads'] = [t.head_ for t in doc.token_iter()]
             if hasattr(tok, 'dep_') and 'dep' in add_features:
                 params['deps'] = [t.dep_ for t in doc.token_iter()]
             if hasattr(tok, 'ent_') and 'ent' in add_features:
                 params['ents'] = [conv_iob(t) for t in doc.token_iter()]
         return Doc(**params)
 
-    def __str__(self):
-        return f'model_name: {self.model_name}, lang: {self.lang}'
 
-    def __repr__(self):
-        return self.__str__()
+@dataclass
+class DecoratedFeatureDocumentParser(FeatureDocumentParser):
+    """This class adapts the spaCy parser adaptors to the general case using a
+    GoF decorator pattern.  This is useful for any post processing needed on
+    existing configured document parsers.
+
+    """
+    delegate: FeatureDocumentParser = field()
+    """Used to create the feature documents."""
+
+    token_decorators: Sequence[FeatureTokenDecorator] = field(default=())
+    """A list of decorators that can add, remove or modify features on a token.
+
+    """
+    sentence_decorators: Sequence[FeatureSentenceDecorator] = field(
+        default=())
+    """A list of decorators that can add, remove or modify features on a
+    sentence.
+
+    """
+    document_decorators: Sequence[FeatureDocumentDecorator] = field(
+        default=())
+    """A list of decorators that can add, remove or modify features on a
+    document.
+
+    """
+    def decorate(self, doc: FeatureDocument):
+        td: FeatureTokenDecorator
+        for td in self.token_decorators:
+            tok: FeatureToken
+            for tok in doc.token_iter():
+                td.decorate(tok)
+        sd: FeatureSentenceDecorator
+        for sd in self.sentence_decorators:
+            sent: FeatureSentence
+            for sent in doc.sents:
+                sd.decorate(sent)
+        dd: FeatureDocumentDecorator
+        for dd in self.document_decorators:
+            dd.decorate(doc)
+
+    def parse(self, text: str, *args, **kwargs) -> FeatureDocument:
+        doc: FeatureDocument = self.delegate.parse(text, *args, **kwargs)
+        self.decorate(doc)
+        return doc
 
 
 @dataclass
 class CachingFeatureDocumentParser(FeatureDocumentParser):
     """A document parser that persists previous parses using the hash of the
     text as a key.  Caching is optional given the value of :obj:`stash`, which
     is useful in cases this class is extended using other use cases other than
@@ -635,14 +707,17 @@
         if self.stash is not None:
             doc = self.stash.load(key)
         if doc is None:
             doc = self.delegate.parse(text, *args, **kwargs)
             if dump and self.stash is not None:
                 self.stash.dump(key, doc)
         else:
+            if doc.text != text:
+                raise NLPError(
+                    f'Document text does not match: <{text}> != >{doc.text}>')
             loaded = True
         return doc, key, loaded
 
     def parse(self, text: str, *args, **kwargs) -> FeatureDocument:
         return self._load_or_parse(text, True, *args, **kwargs)[0]
 
     def clear(self):
```

## zensols/nlp/score.py

```diff
@@ -1,31 +1,40 @@
 from __future__ import annotations
 """Produces matching scores.
 
 """
 __author__ = 'Paul Landes'
 
-from typing import Tuple, Set, Dict, Iterable, List, ClassVar, Union, Optional
+from typing import (
+    Tuple, Set, Dict, Iterable, List, ClassVar, Union, Optional, Type
+)
 from dataclasses import dataclass, field
 from abc import ABCMeta, ABC, abstractmethod
 import logging
 import sys
 from io import TextIOBase
 import nltk.translate.bleu_score as bleu
 import numpy as np
+from zensols.introspect import ClassImporter
 from zensols.config import Dictable
+from zensols.persist import persisted
 from zensols.nlp import TokenContainer
 from . import NLPError
 
 logger = logging.getLogger(__name__)
 
 
+class ScorerError(NLPError):
+    """Raised for any scoring errors (this module)."""
+    pass
+
+
 @dataclass
 class Score(Dictable, metaclass=ABCMeta):
-    """Individual scores returned from :class:`.ScoreMethod'.
+    """Individual scores returned from :class:`.ScoreMethod`.
 
     """
     def asrow(self, meth: str) -> Dict[str, float]:
         return {f'{meth}_{x[0]}': x[1] for x in self.asdict().items()}
 
 
 @dataclass(eq=False)
@@ -132,15 +141,16 @@
     pair in :obj:`.ScoreContext.sents`.  Each elemnt is a dictionary with the
     method are the keys with results as the values as output of the
     :class:`.ScoreMethod`.  This is created in :class:`.Scorer`.
 
     """
     correlation_id_col: str = field(default='id')
     """The column name for the :obj:`.ScoreResult.correlation_id` added to Numpy
-    arrays and Pandas dataframes.
+    arrays and Pandas dataframes.  If ``None``, then the correlation IDS are
+    used as the index.
 
     """
     def __len__(self) -> int:
         return len(self.results)
 
     def __iter__(self) -> Iterable[Dict[str, Tuple[Score]]]:
         return iter(self.results)
@@ -199,34 +209,48 @@
         import pandas as pd
         cols, arr = self.as_numpy(add_correlation=False)
         df = pd.DataFrame(arr, columns=cols)
         if add_correlation and self.has_correlation_id:
             # add as a dataframe, otherwise string correlation IDs cast the
             # numpy array to a string
             cid: str = self.correlation_id_col
-            cols: List[str] = df.columns.tolist()
-            df[cid] = tuple(map(lambda r: r.correlation_id, self.results))
-            cols.insert(0, cid)
-            df = df[cols]
+            cids: Tuple[Union[str, int]] = tuple(
+                map(lambda r: r.correlation_id, self.results))
+            if cid is None:
+                df.index = cids
+            else:
+                cols: List[str] = df.columns.tolist()
+                df[cid] = cids
+                cols.insert(0, cid)
+                df = df[cols]
         return df
 
     def write(self, depth: int = 0, writer: TextIOBase = sys.stdout):
         self._write_line('results:', depth, writer)
         self._write_iterable(self.results, depth + 1, writer)
 
 
 @dataclass
 class ScoreContext(Dictable):
     """Input needed to create score(s) using :class:`.Scorer'.
 
     """
     pairs: Tuple[Tuple[TokenContainer, TokenContainer]] = field()
     """Sentence, span or document pairs to score (order matters for some scoring
-    methods such as rouge).  For summarization use cases, the ordering of the
-    sentence pairs should be ``(<source>, <summary>)``.
+    methods such as rouge).  Depending on the scoring method the ordering of the
+    sentence pairs should be:
+
+      * ``(<summary>, <source>)``
+
+      * ``(<gold>, <prediction>)``
+
+      * ``(<references>, <candidates>)``
+
+    See :class:`.ScoreMethod` implementations for more information about pair
+    ordering.
 
     """
     methods: Set[str] = field(default=None)
     """A set of strings, each indicating the :class:`.ScoreMethod` used to score
     :obj:`pairs`.
 
     """
@@ -240,27 +264,49 @@
     """
     def __post_init__(self):
         self.validate()
 
     def validate(self):
         if self.correlation_ids is not None and \
            len(self.pairs) != len(self.correlation_ids):
-            raise NLPError(
+            raise ScorerError(
                 'Expecting same length pairs to correlation IDs but got: ' +
                 f'{len(self.pairs)} != {len(self.correlation_ids)}')
 
 
 @dataclass
 class ScoreMethod(ABC):
     """An abstract base class for scoring methods (bleu, rouge, etc).
 
     """
     reverse_sents: bool = field(default=False)
     """Whether to reverse the order of the sentences."""
 
+    @classmethod
+    def _get_external_modules(cls: Type) -> Tuple[str, ...]:
+        """Return a list of external module names needed by this method."""
+        return ()
+
+    @classmethod
+    def missing_modules(cls: Type) -> Tuple[str]:
+        """Return a list of missing modules neede by this score method."""
+        missing: List[str] = []
+        mod: str
+        for mod in cls._get_external_modules():
+            try:
+                ClassImporter.get_module(mod)
+            except ModuleNotFoundError:
+                missing.append(mod)
+        return missing
+
+    @classmethod
+    def is_available(cls: Type) -> bool:
+        """Whether or not this method is available on this system."""
+        return len(cls.missing_modules()) == 0
+
     @abstractmethod
     def _score(self, meth: str, context: ScoreContext) -> Iterable[Score]:
         """See :meth:`score`"""
         pass
 
     def score(self, meth: str, context: ScoreContext) -> Iterable[Score]:
         """Score the sentences in ``context`` using method identifer ``meth``.
@@ -273,14 +319,20 @@
                  :class:`.Score`
 
         """
         scores: Iterable[Score]
         if logger.isEnabledFor(logging.DEBUG):
             logger.debug(f'scoring meth: {meth}, ' +
                          f'reverse: {self.reverse_sents}')
+        if not isinstance(context.pairs[0][0], TokenContainer):
+            raise ScorerError(f'Wrong type: {type(context.pairs[0][0])} ' +
+                              f' for first item, expecting {TokenContainer}')
+        if not isinstance(context.pairs[0][1], TokenContainer):
+            raise ScorerError(f'Wrong type: {type(context.pairs[0][0])} ' +
+                              f' for second item, expecting {TokenContainer}')
         try:
             if self.reverse_sents:
                 prev_pairs = context.pairs
                 try:
                     context.pairs = tuple(map(
                         lambda x: (x[1], x[0]), context.pairs))
                     scores = self._score(meth, context)
@@ -308,23 +360,118 @@
             else:
                 s1t = tuple(map(lambda t: t.text, s1.token_iter()))
                 s2t = tuple(map(lambda t: t.text, s2.token_iter()))
             yield (s1t, s2t)
 
 
 @dataclass
+class ExactMatchScoreMethod(ScoreMethod):
+    """A scoring method that return 1 for exact matches and 0 otherwise.
+
+    """
+    equality_measure: str = field(default='norm')
+    """The method by which to compare, which is one of:
+
+        * ``norm``: compare with :meth:`.TokenContainer.norm`
+
+        * ``text``: compare with :obj:`.TokenContainer.text`
+
+        * ``equal``: compare using a Python object ``__eq__`` equal compare,
+                     which also compares the token values
+
+    """
+    def _score(self, meth: str, context: ScoreContext) -> Iterable[FloatScore]:
+        s1: TokenContainer
+        s2: TokenContainer
+        for s1t, s2t in context.pairs:
+            val: float
+            if self.equality_measure == 'norm':
+                val = 1. if s1t.norm == s2t.norm else 0.
+            elif self.equality_measure == 'text':
+                val = 1. if s1t.text == s2t.text else 0.
+            elif self.equality_measure == 'equal':
+                val = 1. if s1t == s2t else 0.
+            else:
+                raise ScorerError(
+                    f"No equality measure: '{self.equality_measure}'")
+            yield FloatScore(val)
+
+
+@dataclass
+class LevenshteinDistanceScoreMethod(ScoreMethod):
+    """A scoring method that computes the Levenshtein distance.
+
+    """
+    form: str = field(default='canon')
+    """The form of the of the text used for the evaluation, which is one of:
+
+        * ``text``: the original text with :obj:`.TokenContainer.text`
+
+        * ``norm``: the normalized text using :meth:`.TokenContainer.norm`
+
+        * ``canon``: :obj:`.TokenContainer.canonical` to normalize out
+          whitespace for better comparisons
+
+    """
+    normalize: bool = field(default=True)
+    """Whether to normalize  the return value as the *distince  / the max length
+    of both sentences*.
+
+    """
+    @classmethod
+    def _get_external_modules(cls: Type) -> Tuple[str, ...]:
+        return ('editdistance',)
+
+    def _score(self, meth: str, context: ScoreContext) -> Iterable[FloatScore]:
+        import editdistance
+
+        def container_to_str(container: TokenContainer) -> str:
+            return container.norm if self.use_norm else container.text
+
+        s1: TokenContainer
+        s2: TokenContainer
+        for s1t, s2t in context.pairs:
+            t1: str
+            t2: str
+            if self.form == 'text':
+                # use the normalized canonical form
+                t1, t2 = s1t.text, s2t.text
+            elif self.form == 'norm':
+                # use the normalized canonical form
+                t1, t2 = s1t.norm, s2t.norm
+            elif self.form == 'canon':
+                # use the normalized canonical form
+                t1, t2 = s1t.canonical, s2t.canonical
+            else:
+                raise ScorerError(f"No form: '{self.form}'")
+            val: int = editdistance.eval(t1, t2)
+            if self.normalize:
+                text_len: int = max(len(t1), len(t2))
+                val = 1. - (val / text_len)
+            val: float = val
+            yield FloatScore(val)
+
+
+@dataclass
 class BleuScoreMethod(ScoreMethod):
-    """The BLEU scoring method using the :mod:`nltk` package.
+    """The BLEU scoring method using the :mod:`nltk` package.  The first
+    sentences are the references and the second are the hypothesis.
 
     """
     smoothing_function: bleu.SmoothingFunction = field(default=None)
     """This is an implementation of the smoothing techniques for segment-level
-    BLEU scores that was presented in Boxing Chen and Collin Cherry (2014) A
-    Systematic Comparison of Smoothing Techniques for Sentence-Level BLEU. In
-    WMT14.  http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf
+    BLEU scores.
+
+    Citation:
+
+    .. code:: none
+
+      Boxing Chen and Collin Cherry (2014) A Systematic Comparison of Smoothing
+      Techniques for Sentence-Level BLEU. In WMT14.
+      http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf
 
     """
     weights: Tuple[float, ...] = field(default=(0.25, 0.25, 0.25, 0.25))
     """Weights for each n-gram.  For example: a tuple of float weights for
     unigrams, bigrams, trigrams and so on can be given: ``weights = (0.1, 0.3,
     0.5, 0.1)``.
 
@@ -339,14 +486,16 @@
             import warnings
             # silence the BLEU warning of n-grams not matching
             # The hypothesis contains 0 counts of 3-gram overlaps...
             warnings.filterwarnings(
                 'ignore', message='[.\n]+The hypothesis contains 0 counts.*')
 
     def _score(self, meth: str, context: ScoreContext) -> Iterable[FloatScore]:
+        s1: TokenContainer
+        s2: TokenContainer
         for s1t, s2t in self._tokenize(context):
             val: float = bleu.sentence_bleu(
                 [s1t], s2t,
                 weights=self.weights,
                 smoothing_function=self.smoothing_function)
             yield FloatScore(val)
 
@@ -357,35 +506,31 @@
 
     """
     feature_tokenizer: bool = field(default=True)
     """Whether to use the :class:`.TokenContainer` tokenization, otherwise use
     the :mod:`rouge_score` package.
 
     """
-    @staticmethod
-    def is_available() -> bool:
-        try:
-            import rouge_score
-            return True
-        except ModuleNotFoundError:
-            return False
+    @classmethod
+    def _get_external_modules(cls: Type) -> Tuple[str, ...]:
+        return ('rouge_score',)
 
     def _score(self, meth: str, context: ScoreContext) -> \
             Iterable[HarmonicMeanScore]:
         from rouge_score import rouge_scorer
 
         class Tokenizer(object):
             @staticmethod
             def tokenize(sent: TokenContainer) -> Tuple[str]:
                 return sents[id(sent)]
 
+        s1: TokenContainer
+        s2: TokenContainer
         if self.feature_tokenizer:
             scorer = rouge_scorer.RougeScorer([meth], tokenizer=Tokenizer)
-            s1: TokenContainer
-            s2: TokenContainer
             pairs = zip(context.pairs, self._tokenize(context))
             for (s1, s2), (s1t, s2t) in pairs:
                 sents = {id(s1): s1t, id(s2): s2t}
                 res: Dict[str, Score] = scorer.score(s1, s2)
                 yield HarmonicMeanScore(*res[meth])
         else:
             scorer = rouge_scorer.RougeScorer([meth])
@@ -402,32 +547,58 @@
 
     """
     methods: Dict[str, ScoreMethod] = field(default=None)
     """The registered scoring methods availale, which are accessed from
     :obj:`.ScoreContext.meth`.
 
     """
+    default_methods: Set[str] = field(default=None)
+    """Methods (keys from :obj:`methods`) to use when none are provided in the
+    :obj:`.ScoreContext.meth` in the call to :meth:`score`.
+
+    """
+    @persisted('_get_missing_modules_pw', cache_global=True)
+    def _get_missing_modules(self) -> Tuple[str]:
+        missing: List[str] = []
+        not_avail: List[str] = []
+        name: str
+        meth: ScoreMethod
+        for name, meth in self.methods.items():
+            missing_mods: Tuple[str] = meth.missing_modules()
+            if len(missing_mods) > 0:
+                logger.warning(f'method {meth} is not available: ' +
+                               f'missing {missing_mods}')
+                not_avail.append(name)
+                missing.extend(missing_mods)
+        for name in not_avail:
+            del self.methods[name]
+        return tuple(missing_mods)
+
     def score(self, context: ScoreContext) -> ScoreSet:
         """Score the sentences in ``context``.
 
         :param context: the context containing the data to score
 
         :return: the results for each method indicated in ``context``
 
         """
         by_meth: Dict[str, Tuple[Score]] = {}
         by_res: List[ScoreResult] = []
         meths: Iterable[str] = context.methods
         if meths is None:
-            meths = self.methods.keys()
+            if self.default_methods is None:
+                meths = self.methods.keys()
+            else:
+                meths = self.default_methods
+        self._get_missing_modules()
         meth: str
         for meth in meths:
             smeth: ScoreMethod = self.methods.get(meth)
             if smeth is None:
-                raise NLPError(f"No scoring method: '{meth}'")
+                raise ScorerError(f"No scoring method: '{meth}'")
             by_meth[meth] = tuple(smeth.score(meth, context))
         for i in range(len(context.pairs)):
             item_res: Dict[str, Score] = {}
             corr_id: str = None
             meth: str
             if context.correlation_ids is not None:
                 corr_id = context.correlation_ids[i]
```

## zensols/nlp/tok.py

```diff
@@ -57,14 +57,17 @@
     :meth:`.FeatureToken.write_attributes` to dump the type features.
 
     """
     FEATURE_IDS: ClassVar[Set[str]] = frozenset(
         reduce(lambda res, x: res | x, FEATURE_IDS_BY_TYPE.values()))
     """All default available feature IDs."""
 
+    SKIP_COMPARE_FEATURE_IDS: ClassVar[Set[str]] = set()
+    """A set of feature IDs to avoid comparing in :meth:`__eq__`."""
+
     WRITABLE_FEATURE_IDS: ClassVar[Tuple[str, ...]] = tuple(
         ('text norm idx sent_i i i_sent tag pos ' +
          'is_wh entity dep children').split())
     """Feature IDs that are dumped on :meth:`write` and :meth:`write_attributes`.
 
     """
     NONE: ClassVar[str] = '-<N>-'
@@ -284,14 +287,17 @@
         if id(self) == id(other):
             return True
         if self.i == other.i and self.idx == other.idx:
             a = dict(self.__dict__)
             b = dict(other.__dict__)
             del a['_detatched_feature_ids']
             del b['_detatched_feature_ids']
+            for attr in self.SKIP_COMPARE_FEATURE_IDS:
+                a.pop(attr)
+                b.pop(attr)
             return a == b
         return False
 
     def __lt__(self, other: FeatureToken) -> int:
         return self.idx < other.idx
 
     def __hash__(self) -> int:
```

## zensols/nlp/resources/component.conf

```diff
@@ -1,7 +1,10 @@
+# description: parsing components
+
+
 # declare a NLP pipeline component to remove sentence boundaries (effectively
 # skipping the sentence chunking step); handy for some tasks that have already
 # segmented sentences
 [remove_sent_boundaries_component]
 class_name = zensols.nlp.Component
 pipe_name = remove_sent_boundaries
 modules = list: zensols.nlp.component
```

## zensols/nlp/resources/obj.conf

```diff
@@ -35,8 +35,7 @@
 #
 # 'norm' is good for debuging, 'dep', 'children' and the rest are needed for
 # dep head tree features
 token_feature_ids = eval({'import': ['zensols.nlp as nlp']}):
   nlp.FeatureToken.FEATURE_IDS
 # remove empty sentences or sentences with only whitespace, which happens with
 # two space separated sentences starting with spaCey 3
-#remove_empty_sentences = True
```

## zensols/nlp/resources/score.yml

```diff
@@ -1,33 +1,58 @@
 # description: scorer configuration
 
 
-## Scorer
+## Score methods
 #
+nlp_exact_match_score_method:
+  class_name: zensols.nlp.score.ExactMatchScoreMethod
+
+nlp_levenshtein_score_method:
+  class_name: zensols.nlp.score.LevenshteinDistanceScoreMethod
+
 nlp_bleu_score_method:
   class_name: zensols.nlp.score.BleuScoreMethod
 
 nlp_rouge_score_method:
   class_name: zensols.nlp.score.RougeScoreMethod
 
+nlp_semeval_score_method:
+  class_name: zensols.nlp.nerscore.SemEvalScoreMethod
+
+
+## Method groupings
+#
 nlp_scorer_methods:
+  exact_match: >-
+    'exact_match': 'nlp_exact_match_score_method',
+  levenshtein: >-
+    'levenshtein': 'nlp_levenshtein_score_method',
   bleu: >-
     'bleu': 'nlp_bleu_score_method',
   rouge: >-
     'rouge1': 'nlp_rouge_score_method',
     'rouge2': 'nlp_rouge_score_method',
     'rouge3': 'nlp_rouge_score_method',
     'rouge4': 'nlp_rouge_score_method',
     'rouge5': 'nlp_rouge_score_method',
     'rouge6': 'nlp_rouge_score_method',
     'rouge7': 'nlp_rouge_score_method',
     'rouge8': 'nlp_rouge_score_method',
     'rouge9': 'nlp_rouge_score_method',
     'rougeL': 'nlp_rouge_score_method',
 
+nlp_ner_scorer_methods:
+  semeval: >-
+    'semeval': 'nlp_semeval_score_method'
+
+
+## Scorer
+#
 nlp_scorer:
   class_name: zensols.nlp.score.Scorer
   methods: >-
     instance: dict: {
+      ${nlp_scorer_methods:exact_match}
+      ${nlp_scorer_methods:levenshtein}
       ${nlp_scorer_methods:bleu}
       ${nlp_scorer_methods:rouge}
       }
```

## Comparing `zensols.nlp-1.6.0.dist-info/METADATA` & `zensols.nlp-1.7.0.dist-info/METADATA`

 * *Files 5% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 Metadata-Version: 2.1
 Name: zensols.nlp
-Version: 1.6.0
+Version: 1.7.0
 Summary: A utility library to assist in parsing natural language text.
 Home-page: https://github.com/plandes/nlparse
-Download-URL: https://github.com/plandes/nlparse/releases/download/v1.6.0/zensols.nlp-1.6.0-py3-none-any.whl
+Download-URL: https://github.com/plandes/nlparse/releases/download/v1.7.0/zensols.nlp-1.7.0-py3-none-any.whl
 Author: Paul Landes
 Author-email: landes@mailc.net
 Keywords: tooling
 Description-Content-Type: text/markdown
 Requires-Dist: spacy (~=3.2.0)
 Requires-Dist: msgpack (>=1.0.0)
 Requires-Dist: msgpack-numpy (>=0.4.7.1)
 Requires-Dist: smart-open (>=4.0.1)
 Requires-Dist: nltk (~=3.7)
-Requires-Dist: zensols.util (~=1.12.1)
 Requires-Dist: interlap (~=0.2.7)
+Requires-Dist: zensols.util (~=1.12.3)
 
 # Zensols Natural Language Parsing
 
 [![PyPI][pypi-badge]][pypi-link]
 [![Python 3.9][python39-badge]][python39-link]
 [![Python 3.10][python310-badge]][python310-link]
 [![Build Status][build-badge]][build-link]
@@ -45,32 +45,30 @@
 * [Framework documentation]
 * [Natural Language Parsing]
 * [List Token Normalizers and Mappers]
 
 
 ## Usage
 
-An example that provides ways to configure the parser is given
-[here](example/config).  See the `makefile` or `./run.py -h` for command line
-usage.
+A parser using the default configuration can be obtained by:
+```python
+from zensols.nlp import FeatureDocumentParser
+parser: FeatureDocumentParser = FeatureDocumentParser.default_instance()
+```
 
-A very [simple](example/simple.py) example is given below:
+However, minimal effort is needed to configure the parser using a [resource library]:
 ```python
 from io import StringIO
 from zensols.config import ImportIniConfig, ImportConfigFactory
 from zensols.nlp import FeatureDocument, FeatureDocumentParser
 
 CONFIG = """
-[import]
-sections = list: imp_conf
-
 # import the `zensols.nlp` library
-[imp_conf]
-type = importini
-config_files = list: resource(zensols.nlp): resources/obj.conf
+[import]
+config_file = resource(zensols.nlp): resources/obj.conf
 
 # override the parse to keep only the norm, ent
 [doc_parser]
 token_feature_ids = set: ent_, tag_
 """
 
 if (__name__ == '__main__'):
@@ -79,15 +77,16 @@
     sent = 'He was George Washington and first president of the United States.'
     doc: FeatureDocument = doc_parser(sent)
     for tok in doc.tokens:
         tok.write()
 ```
 
 This uses a [resource library] to source in the configuration from this package
-so minimal configuration is necessary.
+so minimal configuration is necessary.  More advanced configuration [examples]
+are also available.
 
 See the [feature documents] for more information.
 
 
 ## Obtaining / Installing
 
 1. The easist way to install the command line program is via the `pip`
@@ -141,14 +140,16 @@
 [python39-badge]: https://img.shields.io/badge/python-3.9-blue.svg
 [python39-link]: https://www.python.org/downloads/release/python-390
 [python310-badge]: https://img.shields.io/badge/python-3.10-blue.svg
 [python310-link]: https://www.python.org/downloads/release/python-310
 [build-badge]: https://github.com/plandes/nlparse/workflows/CI/badge.svg
 [build-link]: https://github.com/plandes/nlparse/actions
 
+[examples]: https://github.com/plandes/nlparse/tree/master/example/config
+
 [hierarchy]: https://plandes.github.io/nlparse/api/zensols.nlp.html#zensols.nlp.container.FeatureDocument
 [Parse and normalize]: https://plandes.github.io/nlparse/doc/parse.html
 [others]: https://plandes.github.io/nlparse/doc/normalizers.html
 [Detached features]: https://plandes.github.io/nlparse/doc/parse.html#detached-features
 [full documentation]: https://plandes.github.io/nlparse/
 [Framework documentation]: https://plandes.github.io/nlparse/
 [Natural Language Parsing]: https://plandes.github.io/nlparse/doc/parse.html
```

## Comparing `zensols.nlp-1.6.0.dist-info/RECORD` & `zensols.nlp-1.7.0.dist-info/RECORD`

 * *Files 17% similar despite different names*

```diff
@@ -1,22 +1,24 @@
-zensols/nlp/__init__.py,sha256=14Ia8fjYagZlH5QRJdQJFbsmR7prEVCniJ3qag5k77s,155
+zensols/nlp/__init__.py,sha256=xtpP6eSU8a_-c-NEYKnLlEN_BTf2EVb2fe139RSfDzA,131
 zensols/nlp/combine.py,sha256=cOYzSsbMr7wUMrJxkhKsS9JVAhrqkYvMfemWJCsDewU,9404
 zensols/nlp/component.py,sha256=rRj_NCIbMmbmZvaQSPo04jPDBHOeJYqNtbjS5-2OwGw,6724
-zensols/nlp/container.py,sha256=VTuazVC0Kh2wS259HGvY6IbxAww9BJUwyrlLvSBtEcc,37063
+zensols/nlp/container.py,sha256=XPWydC89SX5MSqI-JuzDcXM9DYwrCCj5FZTCF9_GJDU,43376
 zensols/nlp/dataframe.py,sha256=Lue37IquvfVX6_hG_5L2pr-P8KrR0YaUNWQDhjyGg20,1395
-zensols/nlp/decorate.py,sha256=etSTXDi7-2vVAzVJ1MEjcziEnWpuhGFiX9xbQs8oblg,1401
-zensols/nlp/domain.py,sha256=Aa_uQVvKI38Y1IhTwsRFhyexO60K1brBcu_JqCJ5e-4,6143
-zensols/nlp/norm.py,sha256=7Ah1e4TU4sEQC4Qi0ybtMzjHhwWaXO3ndIePzHvnAMM,17169
-zensols/nlp/parser.py,sha256=Xsb50V-9ksVyai1qv2C6QiAC7WBhDwBqLLkUxLf_y_k,25308
-zensols/nlp/score.py,sha256=amE88laCcIPoyjn278Y88acD0w9dX48_J9YXZCKseT4,15306
+zensols/nlp/decorate.py,sha256=ms6NhFjDXkzz44nuwCEJvl9_K2EBKmB6IjVii7kIp7M,4261
+zensols/nlp/domain.py,sha256=0-rxsEOfqlxdMbLJWdFXHPMjRieu5_YIRfby7viJ8WE,8382
+zensols/nlp/nerscore.py,sha256=dzm9leret_KCyMAfU2Dj_gbfkpB6b6SlYdajCwvJ31w,6579
+zensols/nlp/norm.py,sha256=GTgZQzmBNNn_ZaxgulGAdhLCcqwzaPDnKgFJnd8jCZk,17214
+zensols/nlp/parser.py,sha256=gvY6wsHnHav_RTrcTnwdR5rEplXQBqPHPDpnhz8GMUo,27802
+zensols/nlp/score.py,sha256=YS4yA4tpPZUYj3_Gb5-vSxExqjwBmYK3xjw4G5zd6q8,21290
 zensols/nlp/serial.py,sha256=hNmEJXybFw2ICnexBWJ5TWit7tRjBJMLUpWFVSmvvkc,5856
 zensols/nlp/stemmer.py,sha256=ZHIbU5UfzpYI8cMj096EqY_DRSof-EfjWWMN7_bCMMk,581
-zensols/nlp/tok.py,sha256=o8VVSs8lsTHTEgx7CjL-jY9U4yp-IJIFpH1cztEMpCo,18057
-zensols/nlp/resources/component.conf,sha256=m7Ge21vCl2DMMzmQ3mc30ChrTKJXnJh34ce9xMBkuRs,362
+zensols/nlp/tok.py,sha256=RHS0LM-3UIp4Ec8wTF8Jp8DSLhzyhUON3bp87xIfTk8,18295
+zensols/nlp/resources/component.conf,sha256=SU5Yx_pW8olTvOMmVBrWbUCQ1on5D8LvpBJFo48NPsA,398
+zensols/nlp/resources/decorator.conf,sha256=4vyu4BXvplCL8wzxL3dgHgUi7ErBkhUewxQTsBSxvoI,659
 zensols/nlp/resources/mapper.conf,sha256=OQtpye4uA8t_woIdUUsWfpM5cZ_2c-ZlYFmaCt9vMtM,823
-zensols/nlp/resources/obj.conf,sha256=xnvz6_o48oS3NZT_e2O7gbijIfDq04pHXRrz8nqHXgM,1359
-zensols/nlp/resources/score.yml,sha256=4jf4NQvErQpKYuElKQ6wQmkqBQJJj42xrdAF_CHP0IQ,839
+zensols/nlp/resources/obj.conf,sha256=vF93wu6kN5uIn7gw5uPZEPX0S6ncW1zbY3HkKygsc-M,1328
+zensols/nlp/resources/score.yml,sha256=wmr2OUrnYZL_T3K9MgyMJho3rHiYXeS3pQuF6t_kOo4,1441
 zensols/nlp/resources/serial.conf,sha256=PfvMIiWvk0dr08YQmCodlcxGdMgaX6AMFzrXIr0mgCw,420
-zensols.nlp-1.6.0.dist-info/METADATA,sha256=65d6gZetQHS7zXPRrYAC6ExLjcswuLawFR6iRO2CAOw,5553
-zensols.nlp-1.6.0.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-zensols.nlp-1.6.0.dist-info/top_level.txt,sha256=NgD-P1jf-0xvfkWIxho1IVPkpl2JLBaIjIssgP_pkw8,12
-zensols.nlp-1.6.0.dist-info/RECORD,,
+zensols.nlp-1.7.0.dist-info/METADATA,sha256=FiBk_8fw8XP8ZLRsabKoKfX8zRLjLQZgFr9ANHTB1KA,5699
+zensols.nlp-1.7.0.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+zensols.nlp-1.7.0.dist-info/top_level.txt,sha256=NgD-P1jf-0xvfkWIxho1IVPkpl2JLBaIjIssgP_pkw8,12
+zensols.nlp-1.7.0.dist-info/RECORD,,
```

