# Comparing `tmp/e2eqavn-2.1-py2.py3-none-any.whl.zip` & `tmp/e2eqavn-2.2-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,42 +1,42 @@
-Zip file size: 35444 bytes, number of entries: 40
--rw-rw-r--  2.0 unx      496 b- defN 23-May-25 09:03 e2eqavn/__init__.py
+Zip file size: 35719 bytes, number of entries: 40
+-rw-rw-r--  2.0 unx      496 b- defN 23-Jun-07 13:55 e2eqavn/__init__.py
 -rw-rw-r--  2.0 unx     7061 b- defN 23-May-25 09:03 e2eqavn/cli.py
 -rw-rw-r--  2.0 unx     2472 b- defN 23-May-25 07:49 e2eqavn/keywords.py
 -rw-rw-r--  2.0 unx     2973 b- defN 23-May-20 15:46 e2eqavn/datasets/MRCDataset.py
 -rw-rw-r--  2.0 unx     1386 b- defN 23-Apr-08 19:20 e2eqavn/datasets/TripletDataset.py
 -rw-rw-r--  2.0 unx      124 b- defN 23-May-02 14:04 e2eqavn/datasets/__init__.py
--rw-rw-r--  2.0 unx     3000 b- defN 23-May-17 06:00 e2eqavn/datasets/data_collator.py
+-rw-rw-r--  2.0 unx     3660 b- defN 23-May-30 15:21 e2eqavn/datasets/data_collator.py
 -rw-rw-r--  2.0 unx       85 b- defN 23-Apr-08 14:59 e2eqavn/documents/__init__.py
--rw-rw-r--  2.0 unx    12491 b- defN 23-May-25 08:18 e2eqavn/documents/corpus.py
+-rw-rw-r--  2.0 unx    12629 b- defN 23-Jun-07 13:50 e2eqavn/documents/corpus.py
 -rw-rw-r--  2.0 unx      251 b- defN 23-Apr-01 19:32 e2eqavn/documents/document_store.py
 -rw-rw-r--  2.0 unx       93 b- defN 23-May-02 08:29 e2eqavn/evaluate/__init__.py
 -rw-rw-r--  2.0 unx      283 b- defN 23-Apr-10 04:59 e2eqavn/evaluate/bm25_evaluate_retrieval.py
 -rw-rw-r--  2.0 unx     3408 b- defN 23-May-20 17:10 e2eqavn/evaluate/information_retrieval_evaluator_custom.py
 -rw-rw-r--  2.0 unx     1958 b- defN 23-May-17 02:36 e2eqavn/evaluate/mrc_evaluator.py
--rw-rw-r--  2.0 unx       83 b- defN 23-May-17 11:04 e2eqavn/mrc/__init__.py
+-rw-rw-r--  2.0 unx      110 b- defN 23-May-29 09:53 e2eqavn/mrc/__init__.py
 -rw-rw-r--  2.0 unx      797 b- defN 23-May-25 08:47 e2eqavn/mrc/base.py
--rw-rw-r--  2.0 unx    10744 b- defN 23-May-25 07:52 e2eqavn/mrc/mrc_model.py
+-rw-rw-r--  2.0 unx    10744 b- defN 23-May-30 14:28 e2eqavn/mrc/mrc_model.py
 -rw-rw-r--  2.0 unx       61 b- defN 23-Apr-09 14:32 e2eqavn/pipeline/__init__.py
 -rw-rw-r--  2.0 unx     2083 b- defN 23-May-25 09:03 e2eqavn/pipeline/e2e_question_answering.py
 -rw-rw-r--  2.0 unx     2090 b- defN 23-Apr-14 20:19 e2eqavn/pipeline/pipeline.py
 -rw-rw-r--  2.0 unx      145 b- defN 23-May-06 10:41 e2eqavn/processor/__init__.py
 -rw-rw-r--  2.0 unx     4462 b- defN 23-Apr-17 14:59 e2eqavn/processor/bm25.py
 -rw-rw-r--  2.0 unx     2248 b- defN 23-Apr-01 18:52 e2eqavn/processor/chunk.py
 -rw-rw-r--  2.0 unx     5945 b- defN 23-May-22 03:36 e2eqavn/processor/qa_ext_processor.py
 -rw-rw-r--  2.0 unx     6506 b- defN 23-Apr-17 15:11 e2eqavn/processor/retrieval_sampling.py
 -rw-rw-r--  2.0 unx      138 b- defN 23-Apr-08 20:11 e2eqavn/retrieval/__init__.py
 -rw-rw-r--  2.0 unx      644 b- defN 23-Apr-15 09:08 e2eqavn/retrieval/base.py
 -rw-rw-r--  2.0 unx     3322 b- defN 23-Apr-16 17:04 e2eqavn/retrieval/bm25_retrieval.py
 -rw-rw-r--  2.0 unx    10419 b- defN 23-May-25 09:03 e2eqavn/retrieval/sbert_retrieval.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Mar-18 08:56 e2eqavn/utils/__init__.py
 -rw-rw-r--  2.0 unx    12402 b- defN 23-May-20 16:33 e2eqavn/utils/calculate.py
--rw-rw-r--  2.0 unx      585 b- defN 23-May-01 13:31 e2eqavn/utils/io.py
+-rw-rw-r--  2.0 unx      603 b- defN 23-May-31 16:18 e2eqavn/utils/io.py
 -rw-rw-r--  2.0 unx      998 b- defN 23-May-03 09:44 e2eqavn/utils/preprocess.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Mar-19 07:27 test/__init__.py
 -rw-rw-r--  2.0 unx      334 b- defN 23-Apr-08 14:59 test/test_chunking.py
--rw-rw-r--  2.0 unx      649 b- defN 23-May-25 09:04 e2eqavn-2.1.dist-info/METADATA
--rw-rw-r--  2.0 unx      110 b- defN 23-May-25 09:04 e2eqavn-2.1.dist-info/WHEEL
--rw-rw-r--  2.0 unx       52 b- defN 23-May-25 09:04 e2eqavn-2.1.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx       13 b- defN 23-May-25 09:04 e2eqavn-2.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     3366 b- defN 23-May-25 09:04 e2eqavn-2.1.dist-info/RECORD
-40 files, 104277 bytes uncompressed, 30048 bytes compressed:  71.2%
+-rw-rw-r--  2.0 unx      649 b- defN 23-Jun-07 13:56 e2eqavn-2.2.dist-info/METADATA
+-rw-rw-r--  2.0 unx      110 b- defN 23-Jun-07 13:56 e2eqavn-2.2.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       52 b- defN 23-Jun-07 13:56 e2eqavn-2.2.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx       13 b- defN 23-Jun-07 13:56 e2eqavn-2.2.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     3367 b- defN 23-Jun-07 13:56 e2eqavn-2.2.dist-info/RECORD
+40 files, 105121 bytes uncompressed, 30323 bytes compressed:  71.2%
```

## zipnote {}

```diff
@@ -99,23 +99,23 @@
 
 Filename: test/__init__.py
 Comment: 
 
 Filename: test/test_chunking.py
 Comment: 
 
-Filename: e2eqavn-2.1.dist-info/METADATA
+Filename: e2eqavn-2.2.dist-info/METADATA
 Comment: 
 
-Filename: e2eqavn-2.1.dist-info/WHEEL
+Filename: e2eqavn-2.2.dist-info/WHEEL
 Comment: 
 
-Filename: e2eqavn-2.1.dist-info/entry_points.txt
+Filename: e2eqavn-2.2.dist-info/entry_points.txt
 Comment: 
 
-Filename: e2eqavn-2.1.dist-info/top_level.txt
+Filename: e2eqavn-2.2.dist-info/top_level.txt
 Comment: 
 
-Filename: e2eqavn-2.1.dist-info/RECORD
+Filename: e2eqavn-2.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## e2eqavn/__init__.py

```diff
@@ -16,9 +16,9 @@
 
 stream_handler.setFormatter(formatted)
 logger.addHandler(stream_handler)
 logger.setLevel(logging.INFO)
 logger.propagate = False
 
 __author__ = 'khanhdm'
-__version__ = '2.1'
+__version__ = '2.2'
```

## e2eqavn/datasets/data_collator.py

```diff
@@ -5,16 +5,17 @@
 from torch.nn.utils.rnn import pad_sequence
 from transformers import AutoTokenizer
 
 from e2eqavn.keywords import *
 
 
 class DataCollatorCustom:
-    def __init__(self, tokenizer):
+    def __init__(self, tokenizer, mode_triton: bool = False):
         self.tokenizer = tokenizer
+        self.mode_triton = mode_triton
 
     def __call__(self, batch):
 
         def collate_fn(list_tensor: List[Tensor], padding_value: int):
             return pad_sequence(
                 list_tensor,
                 padding_value=padding_value,
@@ -52,19 +53,34 @@
                 'attention_mask': attention_masks,
                 'start_positions': start_idxs,
                 'end_positions': end_idxs,
                 'words_length': words_length,
                 'span_answer_ids': span_answer_ids
             }
 
-        return {
+        data = {
             'input_ids': input_ids,
             'attention_mask': attention_masks,
             'words_length': words_length,
         }
+        if self.mode_triton:
+            batch_size = len(batch)
+            max_sub_word = input_ids.shape[1]
+            max_word = words_length.shape[1]
+            align_matrix = torch.zeros((batch_size, max_word, max_sub_word))
+
+            for i, sample_length in enumerate(words_length):
+                for j in range(len(sample_length)):
+                    start_idx = torch.sum(sample_length[:j])
+                    align_matrix[i][j][start_idx: start_idx + sample_length[j]] = 1 if sample_length[j] > 0 else 0
+
+            data['align_matrix'] = align_matrix
+
+        return data
+
 
 # def data_collator_fn(samples):
 #     def collate_fn(list_tensor: List[Tensor], padding_value: int):
 #         return pad_sequence(
 #             list_tensor,
 #             padding_value=padding_value,
 #             batch_first=True
```

## e2eqavn/documents/corpus.py

```diff
@@ -115,14 +115,15 @@
         self.n_document = len(self.list_document)
         self.n_pair_question_answer = 0
         self.list_document_context = [document.document_context for document in list_document]
         for document in self.list_document:
             self.n_pair_question_answer += len(document.list_pair_question_answers)
         self.__dict__.update(kwargs)
 
+
     @classmethod
     def get_documents(cls, context: Dict, doc_th: int = 0, **kwargs):
         context_key = kwargs.get(CONTEXT_KEY, cls.context_key)
         qas_key = kwargs.get(QAS_KEY, cls.qas_key)
         question_key = kwargs.get(QUESTION_KEY, cls.question_key)
         answers_key = kwargs.get(ANSWERS_KEY, cls.answers_key)
         answer_key = kwargs.get(ANSWER_KEY, cls.answer_key)
@@ -131,15 +132,15 @@
         answer_start = kwargs.get(ANSWER_START, cls.answer_start)
 
         list_document = []
         document_context = context[context_key]
         if not kwargs.get(MODE_CHUNKING, False):
             document_id = hashlib.sha1(str(document_context).encode('utf-8')).hexdigest()
             dict_question_answers = defaultdict(list)
-            if (not infer_mode) or is_vnsquad_eval:
+            if len(context[qas_key]) > 0:
                 for question in context[qas_key]:
                     if not is_vnsquad_eval:
                         for answer in question[answers_key]:
                             dict_question_answers[question[question_key]].append(
                                 {
                                     answer_key: answer[answer_key],
                                     answer_start: answer[answer_start]
@@ -208,20 +209,20 @@
                         index=doc_th
                     )
                 )
                 doc_th += 1
         return list_document, doc_th
 
     @classmethod
-    def init_corpus(cls, corpus: List[Dict], **kwargs):
+    def init_corpus(cls, path_data, **kwargs):
         """
         :param max_length: maximum number word for 1 document
         :param overlapping: overlapping size for 2  document adjacency pair
         :param mode_chunking: on or off mode chunking long document
-        :param corpus: dictionary context, question and answer
+        :param path_data: path to file data and  must have the below form
             Exammple:
             [
                 {
                     "text": "xin chào bạn"
                     "qas": [
                         {
                             "question" : "question1",
@@ -233,14 +234,15 @@
                     ]
 
                 }
             ]
 
         :return:
         """
+        corpus = load_json_data(path_data)
         list_documents = []
         doc_th = 0
         for context in corpus:
             tmp_list_documents, doc_th = cls.get_documents(context, doc_th, **kwargs)
             list_documents.extend(tmp_list_documents)
         return cls(list_document=list_documents, **kwargs)
 
@@ -273,22 +275,27 @@
                 tmp_list_documents, doc_th = cls.get_documents(paragraph, doc_th, **kwargs)
                 list_document.extend(
                     tmp_list_documents
                 )
 
         return cls(list_document=list_document, **kwargs)
 
+    @classmethod
+    def parser_normal(cls, path_data: str, **kwargs):
+        data = load_json_data(path_data)
+
     def save_corpus(self, path_file: str):
-        infor = {}
+        infor = []
         for document in self.list_document:
-            infor[document.document_id] = {
+            temp = {
                 "context": document.document_context,
                 "qas": []
             }
             for question_answer in document.list_pair_question_answers:
-                infor[document.document_id]['qas'].append(
+                temp['qas'].append(
                     {
                         "question": question_answer.question,
-                        "answers": question_answer.list_answers
+                        "answers": question_answer.list_dict_answer
                     }
                 )
+            infor.append(temp)
         write_json_file(infor, path_file)
```

## e2eqavn/mrc/__init__.py

```diff
@@ -1,2 +1,2 @@
 from e2eqavn.mrc.base import BaseReader
-from e2eqavn.mrc.mrc_model import MRCReader
+from e2eqavn.mrc.mrc_model import MRCReader, MRCQuestionAnsweringModel
```

## e2eqavn/utils/io.py

```diff
@@ -3,15 +3,15 @@
 import yaml
 from typing import *
 
 logger = logging.getLogger(__name__)
 
 
 def load_json_data(path_file: str):
-    with open(path_file, 'r') as file:
+    with open(path_file, 'r', encoding='utf-8') as file:
         data = json.load(file)
     return data
 
 
 def write_json_file(data: Union[Dict, List[Dict]], path_file: str):
     with open(path_file, 'w') as file:
         json.dump(data, file, ensure_ascii=False, indent=4)
```

## Comparing `e2eqavn-2.1.dist-info/METADATA` & `e2eqavn-2.2.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: e2eqavn
-Version: 2.1
+Version: 2.2
 Summary: e2eqavn is end to end pipeline for question answering
 Author: khanhdm
 Author-email: khanhc1k36@gmail.com
 Requires-Python: >3.6.0
 Requires-Dist: numpy
 Requires-Dist: PyYAML
 Requires-Dist: sentence-transformers
```

## Comparing `e2eqavn-2.1.dist-info/RECORD` & `e2eqavn-2.2.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,22 +1,22 @@
-e2eqavn/__init__.py,sha256=vGfBlDoQwQ-cGjM-0OmyRWhCBvWfq5rNZjUvsh7m5Gg,496
+e2eqavn/__init__.py,sha256=RtpYQtJhM6lSCIOlq0LV3qkwHGg92yhRC7d2hw4Qnuk,496
 e2eqavn/cli.py,sha256=VzQxgKWkDt-hiqJx3tSAzlzwRZ82MwvnZcX0JnHWO48,7061
 e2eqavn/keywords.py,sha256=HzbuUmSAh1lZemWm4GHiQo3VZ5tN4kBZIhJS9_BPvfo,2472
 e2eqavn/datasets/MRCDataset.py,sha256=q0TDZ0dOR3O63hzzD_b1mAkPSb7jpHpnh5ywuJIXOuA,2973
 e2eqavn/datasets/TripletDataset.py,sha256=UalsiPb-sVq79gE923C_Gu8mqih_KJ-NCAI2MEV-VCY,1386
 e2eqavn/datasets/__init__.py,sha256=PVd6KkEbmwFhGlSdv8lCnLcyIT6MrIOU2zeLEDnNBQM,124
-e2eqavn/datasets/data_collator.py,sha256=cFZgznCMdGKFU2E26ZctzaM6-TQj6Azi6LjQrEHMtWU,3000
+e2eqavn/datasets/data_collator.py,sha256=i1eE-WHBmLH2uI8CIv-WZpAjhzwbKTwTtgr9Ytw5FAA,3660
 e2eqavn/documents/__init__.py,sha256=KhJ-7DiMYhKRaNBXpMFooJ4EioOOXKxFqjTVB__1JG8,85
-e2eqavn/documents/corpus.py,sha256=s_4CNvZvGKPcsyevsrpPzxXlUI_8kwiBlF7zZUSl1N0,12491
+e2eqavn/documents/corpus.py,sha256=OK8lw6ux9i9YDyoi29sta8OtfWTq9_qXEjfiCGxAt88,12629
 e2eqavn/documents/document_store.py,sha256=SI0eQivEdTGucoRkEzOiHVXcL4IeP-3R9I8JC7iUvk0,251
 e2eqavn/evaluate/__init__.py,sha256=FoaGcO_g8rveNGpk_6_n4RFw4P_dpRxmMGk0B3T1DMI,93
 e2eqavn/evaluate/bm25_evaluate_retrieval.py,sha256=eyvV7MFGQcd2f5BXph8kn5B3wqdL2kzXOyXWzNJOrYc,283
 e2eqavn/evaluate/information_retrieval_evaluator_custom.py,sha256=qStzIgsOGq8Tu3nt6W2lu6ajZiXxNWaq2EQOv_pnSoo,3408
 e2eqavn/evaluate/mrc_evaluator.py,sha256=dTn_8L2nDLR8frBxZWBuY_bdcDZ7aXHSy7nRvrnf1V0,1958
-e2eqavn/mrc/__init__.py,sha256=BwTJ69pl2FRIbrvilityA9vj93sZaZ7YQNWJ4uRlpC4,83
+e2eqavn/mrc/__init__.py,sha256=7wccJ-QQrMqjYIPIPmOMcQzWhjFgne7QbPZlxp5njPU,110
 e2eqavn/mrc/base.py,sha256=YiACw2yzdkVuV8wtrTblhuiCWYYTpKVQfC5PfjPhZ7w,797
 e2eqavn/mrc/mrc_model.py,sha256=lpx1QLbUjwrgao_NzlyM7P_MbGkEidAzqnYu-JlnUFw,10744
 e2eqavn/pipeline/__init__.py,sha256=nHdcilMUCQJATXwseGf_ejz-wFKlGSMs0sM7HneaZjQ,61
 e2eqavn/pipeline/e2e_question_answering.py,sha256=vre0tEAI9klK9Musj8VRWf2DTrnQkpIa43ku-lx7GQo,2083
 e2eqavn/pipeline/pipeline.py,sha256=kRQmaA9GDAsU4zfB3TOY92jxVF5r3N-JWg2xqvCu1o4,2090
 e2eqavn/processor/__init__.py,sha256=7Y91pODl0Ba5-z0UHyw4uGkWO8M4r5S4vtzDx30Qlhs,145
 e2eqavn/processor/bm25.py,sha256=hkw_usvZ03MA37X5nlTvFpCkjwHzXBqRI_JKGvPlcrM,4462
@@ -25,16 +25,16 @@
 e2eqavn/processor/retrieval_sampling.py,sha256=RajiTjF8zxXbwTD_ig2XICQcSkToPIHQ7OFFpF1aplA,6506
 e2eqavn/retrieval/__init__.py,sha256=SiDAppt0_X9DRa9dhiCxmle7RpKQOW0dtYpA84y0W18,138
 e2eqavn/retrieval/base.py,sha256=XGkgyS5es4DFJyS5HwZKzfWBWaKekJowk6UvLuqMVhs,644
 e2eqavn/retrieval/bm25_retrieval.py,sha256=EyxojDjg8saKj-xEozm9PhrGyu4iV63P98a-UrDjS48,3322
 e2eqavn/retrieval/sbert_retrieval.py,sha256=KhdDq2CBF_E9nhWpQExyNtcjQOtxvOaZybj1ZAV7idQ,10419
 e2eqavn/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 e2eqavn/utils/calculate.py,sha256=dFQCipyqcBWlQeq_aQgON2Rg06rYjMhfzByYtUSjifQ,12402
-e2eqavn/utils/io.py,sha256=WqUUbnR4R_OHw4PgY1yqWERts_oWL07apJJ88ZQBd2E,585
+e2eqavn/utils/io.py,sha256=StKqF8nRhuApqlI0vPlme8Y50Do4epId8KeW4oJ5uTI,603
 e2eqavn/utils/preprocess.py,sha256=1yhNSGZ5FRzIglW0IRiZyxS1b3Rr5CNzLN78lQsRQjQ,998
 test/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 test/test_chunking.py,sha256=tW3Cg_Ll8mtfYjmPmNy5MgElUSe8eIKhcWAND20SRDA,334
-e2eqavn-2.1.dist-info/METADATA,sha256=BUUbwDwufX13-ByRKa0yFqscH9vS9O6k4mycSI-V4EM,649
-e2eqavn-2.1.dist-info/WHEEL,sha256=bb2Ot9scclHKMOLDEHY6B2sicWOgugjFKaJsT7vwMQo,110
-e2eqavn-2.1.dist-info/entry_points.txt,sha256=pPUmMeBtyeitA4rbQwBo_glm5HRGPLREjN9eeAGu_Bk,52
-e2eqavn-2.1.dist-info/top_level.txt,sha256=Qu5Dlk8CtzRo5i3_O722D1zBatiVaNkiPAbSDCOA2pA,13
-e2eqavn-2.1.dist-info/RECORD,,
+e2eqavn-2.2.dist-info/METADATA,sha256=IgH-zvJu9pQngAZg61ZCqS-7gtS8xSol5-04Unn5ldc,649
+e2eqavn-2.2.dist-info/WHEEL,sha256=bb2Ot9scclHKMOLDEHY6B2sicWOgugjFKaJsT7vwMQo,110
+e2eqavn-2.2.dist-info/entry_points.txt,sha256=pPUmMeBtyeitA4rbQwBo_glm5HRGPLREjN9eeAGu_Bk,52
+e2eqavn-2.2.dist-info/top_level.txt,sha256=Qu5Dlk8CtzRo5i3_O722D1zBatiVaNkiPAbSDCOA2pA,13
+e2eqavn-2.2.dist-info/RECORD,,
```

