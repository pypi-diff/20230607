# Comparing `tmp/synapgrad-0.5.1-py3-none-any.whl.zip` & `tmp/synapgrad-0.6.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,28 +1,30 @@
-Zip file size: 43620 bytes, number of entries: 26
--rw-rw-rw-  2.0 fat      852 b- defN 23-May-27 15:38 synapgrad/__init__.py
+Zip file size: 46700 bytes, number of entries: 28
+-rw-rw-rw-  2.0 fat      877 b- defN 23-Jun-06 16:07 synapgrad/__init__.py
 -rw-rw-rw-  2.0 fat    22932 b- defN 23-May-27 01:01 synapgrad/conv_tools.py
--rw-rw-rw-  2.0 fat    21480 b- defN 23-May-27 13:17 synapgrad/cpu_ops.py
+-rw-rw-rw-  2.0 fat    22055 b- defN 23-Jun-06 16:50 synapgrad/cpu_ops.py
 -rw-rw-rw-  2.0 fat      171 b- defN 23-May-26 20:26 synapgrad/device.py
 -rw-rw-rw-  2.0 fat    36424 b- defN 23-May-27 01:01 synapgrad/functional.py
--rw-rw-rw-  2.0 fat    20437 b- defN 23-May-27 01:01 synapgrad/tensor.py
+-rw-rw-rw-  2.0 fat    21144 b- defN 23-Jun-06 00:29 synapgrad/tensor.py
 -rw-rw-rw-  2.0 fat     1980 b- defN 23-May-27 01:01 synapgrad/utils.py
--rw-rw-rw-  2.0 fat      445 b- defN 23-May-26 20:26 synapgrad/nn/__init__.py
--rw-rw-rw-  2.0 fat     1731 b- defN 23-May-26 20:26 synapgrad/nn/activations.py
--rw-rw-rw-  2.0 fat    35049 b- defN 23-May-27 12:23 synapgrad/nn/functional.py
--rw-rw-rw-  2.0 fat     2907 b- defN 23-May-27 01:01 synapgrad/nn/initializations.py
--rw-rw-rw-  2.0 fat    25851 b- defN 23-May-27 01:01 synapgrad/nn/layers.py
+-rw-rw-rw-  2.0 fat       81 b- defN 23-Jun-02 14:26 synapgrad/c/__init__.py
+-rw-rw-rw-  2.0 fat     1985 b- defN 23-Jun-02 16:51 synapgrad/c/conv_tools.py
+-rw-rw-rw-  2.0 fat      462 b- defN 23-Jun-06 16:16 synapgrad/nn/__init__.py
+-rw-rw-rw-  2.0 fat     2562 b- defN 23-Jun-06 16:46 synapgrad/nn/activations.py
+-rw-rw-rw-  2.0 fat    37187 b- defN 23-Jun-06 16:17 synapgrad/nn/functional.py
+-rw-rw-rw-  2.0 fat     9540 b- defN 23-Jun-06 17:15 synapgrad/nn/init.py
+-rw-rw-rw-  2.0 fat    26350 b- defN 23-Jun-06 15:38 synapgrad/nn/layers.py
 -rw-rw-rw-  2.0 fat     4321 b- defN 23-May-27 01:01 synapgrad/nn/losses.py
--rw-rw-rw-  2.0 fat     4967 b- defN 23-May-26 20:26 synapgrad/nn/modules.py
+-rw-rw-rw-  2.0 fat     5295 b- defN 23-Jun-06 16:56 synapgrad/nn/modules.py
 -rw-rw-rw-  2.0 fat      198 b- defN 23-May-26 20:26 synapgrad/nn/utils/__init__.py
 -rw-rw-rw-  2.0 fat     3207 b- defN 23-May-26 20:26 synapgrad/nn/utils/data.py
 -rw-rw-rw-  2.0 fat    10586 b- defN 23-May-26 20:26 synapgrad/nn/utils/train.py
--rw-rw-rw-  2.0 fat       61 b- defN 23-May-26 20:26 synapgrad/optim/__init__.py
--rw-rw-rw-  2.0 fat     5426 b- defN 23-May-27 01:01 synapgrad/optim/optimizers.py
+-rw-rw-rw-  2.0 fat       68 b- defN 23-Jun-06 19:55 synapgrad/optim/__init__.py
+-rw-rw-rw-  2.0 fat     7784 b- defN 23-Jun-06 21:29 synapgrad/optim/optimizers.py
 -rw-rw-rw-  2.0 fat       34 b- defN 23-May-26 20:26 synapgrad/visual/__init__.py
 -rw-rw-rw-  2.0 fat     1783 b- defN 23-May-27 01:01 synapgrad/visual/graph.py
--rw-rw-rw-  2.0 fat     1088 b- defN 23-May-27 15:56 synapgrad-0.5.1.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     5003 b- defN 23-May-27 15:56 synapgrad-0.5.1.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-May-27 15:56 synapgrad-0.5.1.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       10 b- defN 23-May-27 15:56 synapgrad-0.5.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     2102 b- defN 23-May-27 15:56 synapgrad-0.5.1.dist-info/RECORD
-26 files, 209137 bytes uncompressed, 40264 bytes compressed:  80.7%
+-rw-rw-rw-  2.0 fat     1088 b- defN 23-Jun-06 23:11 synapgrad-0.6.0.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     4988 b- defN 23-Jun-06 23:11 synapgrad-0.6.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Jun-06 23:11 synapgrad-0.6.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       10 b- defN 23-Jun-06 23:11 synapgrad-0.6.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     2251 b- defN 23-Jun-06 23:11 synapgrad-0.6.0.dist-info/RECORD
+28 files, 225455 bytes uncompressed, 43118 bytes compressed:  80.9%
```

## zipnote {}

```diff
@@ -15,24 +15,30 @@
 
 Filename: synapgrad/tensor.py
 Comment: 
 
 Filename: synapgrad/utils.py
 Comment: 
 
+Filename: synapgrad/c/__init__.py
+Comment: 
+
+Filename: synapgrad/c/conv_tools.py
+Comment: 
+
 Filename: synapgrad/nn/__init__.py
 Comment: 
 
 Filename: synapgrad/nn/activations.py
 Comment: 
 
 Filename: synapgrad/nn/functional.py
 Comment: 
 
-Filename: synapgrad/nn/initializations.py
+Filename: synapgrad/nn/init.py
 Comment: 
 
 Filename: synapgrad/nn/layers.py
 Comment: 
 
 Filename: synapgrad/nn/losses.py
 Comment: 
@@ -57,23 +63,23 @@
 
 Filename: synapgrad/visual/__init__.py
 Comment: 
 
 Filename: synapgrad/visual/graph.py
 Comment: 
 
-Filename: synapgrad-0.5.1.dist-info/LICENSE
+Filename: synapgrad-0.6.0.dist-info/LICENSE
 Comment: 
 
-Filename: synapgrad-0.5.1.dist-info/METADATA
+Filename: synapgrad-0.6.0.dist-info/METADATA
 Comment: 
 
-Filename: synapgrad-0.5.1.dist-info/WHEEL
+Filename: synapgrad-0.6.0.dist-info/WHEEL
 Comment: 
 
-Filename: synapgrad-0.5.1.dist-info/top_level.txt
+Filename: synapgrad-0.6.0.dist-info/top_level.txt
 Comment: 
 
-Filename: synapgrad-0.5.1.dist-info/RECORD
+Filename: synapgrad-0.6.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## synapgrad/__init__.py

```diff
@@ -1,23 +1,23 @@
 from synapgrad.tensor import (
-    Tensor, tensor, ones, ones_like, zeros, zeros_like, 
+    Tensor, tensor, empty, ones, ones_like, zeros, zeros_like, 
     arange, rand, randn, normal, randint, eye, no_grad,
     retain_grads, no_grad
 )
 from synapgrad.functional import (
     add, mul, matmul, addmm, pow, rpow, neg, slice,
     concat, stack, unbind,
     clone, exp, log, sqrt, sum, mean, max, min, squeeze, unsqueeze,
     reshape, movedim, transpose, flatten, unfold_dim
 )
 
 from synapgrad.utils import manual_seed
 
 from synapgrad.nn.functional import (
-    relu, tanh, sigmoid, softmax, log_softmax,
+    relu, leaky_relu, selu, tanh, sigmoid, softmax, log_softmax,
     mse_loss, nll_loss, binary_cross_entropy, binary_cross_entropy_with_logits, cross_entropy,
     linear, unfold, fold, max_pool1d, max_pool2d, avg_pool1d, avg_pool2d, conv1d, conv2d, batch_norm
 )
 
 from synapgrad import (
     cpu_ops, conv_tools, device, functional, nn, optim, utils, visual
 )
```

## synapgrad/cpu_ops.py

```diff
@@ -290,14 +290,28 @@
 def relu_forward(a:np.ndarray) -> np.ndarray:
     return np.maximum(0, a)
 
 def relu_backward(grad:np.ndarray, a:np.ndarray) -> np.ndarray:
     return grad * (a > 0)
 
 
+def leaky_relu_forward(a:np.ndarray, neg_slope:float) -> np.ndarray:
+    return np.maximum(neg_slope * a, a)
+
+def leaky_relu_backward(grad:np.ndarray, a:np.ndarray, neg_slope:float) -> np.ndarray:
+    return grad * ((a > 0) + neg_slope * (a <= 0))
+
+
+def selu_forward(a:np.ndarray, alpha:float, scale:float) -> np.ndarray:
+    return scale * (np.maximum(0, a) + np.minimum(0, alpha * (np.exp(a) - 1)))
+
+def selu_backward(grad:np.ndarray, a:np.ndarray, alpha:float, scale:float) -> np.ndarray:
+    return scale * grad *((a > 0) +  alpha * np.exp(a) * (a <= 0))
+
+
 def tanh_forward(a:np.ndarray) -> np.ndarray:
     return np.tanh(a)
 
 def tanh_backward(grad:np.ndarray, tanh_a:np.ndarray) -> np.ndarray:
     return grad * (1 - tanh_a**2)
```

## synapgrad/tensor.py

```diff
@@ -56,30 +56,42 @@
 def tensor(data, requires_grad=False, dtype=None, device=None) -> 'Tensor':
     """
     Creates a Tensor from a numpy array
     """
     data = np.array(data).astype(default_type__)
     return Tensor(data, requires_grad=requires_grad, dtype=dtype, device=device)
 
-def ones(shape, dtype=None, requires_grad=False, name=None, device=None):
+def empty(*shape, dtype=None, requires_grad=False, name=None, device=None):
+    """
+    Creates an empty Tensor
+    """
+    if len(shape) == 1 and isinstance(shape[0], (list, tuple)):
+        shape = shape[0]
+    return Tensor(np.empty(shape).astype(default_type__), dtype=dtype, requires_grad=requires_grad, name=name, device=device)
+
+def ones(*shape, dtype=None, requires_grad=False, name=None, device=None):
     """
     Creates a Tensor filled with ones
     """
+    if len(shape) == 1 and isinstance(shape[0], (list, tuple)):
+        shape = shape[0]
     return Tensor(np.ones(shape).astype(default_type__), dtype=dtype, requires_grad=requires_grad, name=name, device=device)
 
 def ones_like(tensor:'Tensor', dtype=None, requires_grad=False, name=None, device=None):
     """
     Creates a Tensor filled with ones
     """
     return Tensor(np.ones_like(tensor.data), dtype=dtype, requires_grad=requires_grad, name=name, device=device)
 
-def zeros(shape, dtype=None, requires_grad=False, name=None, device=None):
+def zeros(*shape, dtype=None, requires_grad=False, name=None, device=None):
     """
     Creates a Tensor filled with zeros
     """
+    if len(shape) == 1 and isinstance(shape[0], (list, tuple)):
+        shape = shape[0]
     return Tensor(np.zeros(shape).astype(default_type__), dtype=dtype, requires_grad=requires_grad, name=name, device=device)
 
 def zeros_like(tensor:'Tensor', dtype=None, requires_grad=False, name=None, device=None):
     """
     Creates a Tensor filled with zeros
     """
     return Tensor(np.zeros_like(tensor.data), dtype=dtype, requires_grad=requires_grad, name=name, device=device)
@@ -90,33 +102,37 @@
     """
     return Tensor(np.arange(*interval).astype(default_type__), dtype=dtype, requires_grad=requires_grad, name=name, device=device)
 
 def rand(*shape, dtype=None, requires_grad=False, name=None, device=None):
     """
     Creates a Tensor filled with values drawn from the uniform distribution
     """
+    if len(shape) == 1 and isinstance(shape[0], (list, tuple)):
+        shape = shape[0]
     return Tensor(np.random.rand(*shape).astype(default_type__), dtype=dtype, requires_grad=requires_grad, name=name, device=device)
 
 def randn(*shape, dtype=None, requires_grad=False, name=None, device=None):
     """
     Creates a Tensor filled with values drawn from the standard Gaussian distribution
     """
+    if len(shape) == 1 and isinstance(shape[0], (list, tuple)):
+        shape = shape[0]
     return Tensor(np.random.randn(*shape).astype(default_type__), dtype=dtype, requires_grad=requires_grad, name=name, device=device)
 
 def normal(loc, scale, *shape, dtype=None, requires_grad=False, name=None, device=None):
     """
     Creates a Tensor filled with values drawn from a custom Gaussian distribution
     """
     return Tensor(np.random.normal(loc, scale, *shape).astype(default_type__), dtype=dtype, requires_grad=requires_grad, name=name, device=device)
 
-def randint(low, high, *shape,  dtype=None, requires_grad=False, name=None, device=None):
+def randint(low, high, shape:tuple, dtype=None, requires_grad=False, name=None, device=None):
     """
     Creates a Tensor filled with integer values drawn in the range between low and high
     """
-    return Tensor(np.random.randint(low, high, *shape).astype(default_type__), dtype=dtype, requires_grad=requires_grad, name=name, device=device)
+    return Tensor(np.random.randint(low, high, shape).astype(np.int64), dtype=dtype, requires_grad=requires_grad, name=name, device=device)
 
 def eye(dim, dtype=None, requires_grad=False, name=None, device=None):
     """
     Creates a 2-dimensional Tensor (matrix) equal to the identity matrix.
     """
     return Tensor(np.eye(dim).astype(default_type__), dtype=dtype, requires_grad=requires_grad, name=name, device=device)
```

## synapgrad/nn/__init__.py

```diff
@@ -1,10 +1,10 @@
 
 from synapgrad.nn.modules import Module, Sequential, Parameter
-from synapgrad.nn.activations import ReLU, Sigmoid, Tanh, Softmax, LogSoftmax
+from synapgrad.nn.activations import ReLU, LeakyReLU, SELU, Sigmoid, Tanh, Softmax, LogSoftmax
 from synapgrad.nn.losses import (
     Loss, MSELoss, CrossEntropyLoss, NLLLoss, BCELoss, BCEWithLogitsLoss
 )
 from synapgrad.nn.layers import (
     Linear, Neuron, Flatten, Unfold, Fold, Dropout,
     MaxPool1d, MaxPool2d, AvgPool1d, AvgPool2d, Conv1d, Conv2d,
     BatchNorm1d, BatchNorm2d
```

## synapgrad/nn/activations.py

```diff
@@ -12,17 +12,48 @@
     ReLU activation function. 
     
     The ReLU activation function is defined as:
     f(x) = max(0, x)
     """
     
     def forward(self, x:Tensor) -> Tensor:
-        F.relu.__doc__
         return F.relu(x)
-
+    
+    
+class LeakyReLU(nn.Module):
+    """
+    LeakyReLU activation function. 
+    
+    The LeakyReLU activation function is defined as:
+    f(x) = max(0, x) + negative_slope * min(0, x)
+    """
+    
+    def __init__(self, negative_slope:float=0.01) -> None:
+        super().__init__()
+        self.negative_slope = negative_slope
+        
+    def forward(self, x:Tensor) -> Tensor:
+        return F.leaky_relu(x, self.negative_slope)
+    
+    
+class SELU(nn.Module):
+    """
+    SELU activation function.
+    
+    The SELU activation function is defined as:
+    f(x) = scale * (max(0, x) + min(0, alpha*(exp(x)-1))
+    
+    with:
+        - alpha = 1.6732632423543772848170429916717
+        - scale = 1.0507009873554804934193349852946
+    """
+    
+    def forward(self, x:Tensor) -> Tensor:
+        return F.selu(x)
+    
 
 class Tanh(nn.Module):
     """ 
     Tanh activation function
     
     It is defined as np.sinh(x)/np.cosh(x) or -1j * np.tan(1j*x) 
     """
```

## synapgrad/nn/functional.py

```diff
@@ -43,14 +43,85 @@
         if x.requires_grad: x._grad += a_grad 
     
     if out.requires_grad: out.grad_fn = BackwardFunction(backward, out._operation)
         
     return out
 
 
+def leaky_relu(x:Tensor, negative_slope=0.01):
+    """
+    Leaky ReLU activation function.
+
+    Args:
+        x (Tensor): tensor
+
+    Returns:
+        Tensor: result
+    """
+    if not isinstance(x, Tensor):
+        raise TypeError(f"Expected x to be a Tensor but got {type(x)}")
+    
+    if x.device == Device.CPU:
+        out_data = cpu_ops.leaky_relu_forward(x.data, negative_slope)
+    else:
+        raise RuntimeError(f"{x.device} not supported")
+
+    out = Tensor(out_data, device=x.device, children=(x,), requires_grad=x.requires_grad, operation="LeakyRelu")
+    
+    def backward():
+        grad_output = out.grad
+        if grad_output.device == Device.CPU:
+            a_grad = cpu_ops.leaky_relu_backward(grad_output.data, x.data, negative_slope)
+        else:
+            raise RuntimeError(f"{grad_output.device} not supported")
+        
+        if x.requires_grad: x._grad += a_grad 
+    
+    if out.requires_grad: out.grad_fn = BackwardFunction(backward, out._operation)
+        
+    return out
+
+
+def selu(x:Tensor):
+    """
+    SELU activation function.
+
+    Args:
+        x (Tensor): tensor
+
+    Returns:
+        Tensor: result
+    """
+    if not isinstance(x, Tensor):
+        raise TypeError(f"Expected x to be a Tensor but got {type(x)}")
+    
+    alpha = 1.6732632423543772848170429916717
+    scale = 1.0507009873554804934193349852946
+    
+    if x.device == Device.CPU:
+        out_data = cpu_ops.selu_forward(x.data, alpha, scale)
+    else:
+        raise RuntimeError(f"{x.device} not supported")
+
+    out = Tensor(out_data, device=x.device, children=(x,), requires_grad=x.requires_grad, operation="SELU")
+    
+    def backward():
+        grad_output = out.grad
+        if grad_output.device == Device.CPU:
+            a_grad = cpu_ops.selu_backward(grad_output.data, x.data, alpha, scale)
+        else:
+            raise RuntimeError(f"{grad_output.device} not supported")
+        
+        if x.requires_grad: x._grad += a_grad 
+    
+    if out.requires_grad: out.grad_fn = BackwardFunction(backward, out._operation)
+        
+    return out
+
+
 def tanh(x:Tensor):
     """ 
     Tanh activation function.
 
     Args:
         x (Tensor): tensor
```

## synapgrad/nn/layers.py

```diff
@@ -1,89 +1,103 @@
+import math
 import numpy as np
 import synapgrad
 from synapgrad import nn
+from synapgrad.nn.modules import Parameter
 from synapgrad.tensor import Tensor
 from synapgrad.nn import functional as F
-from synapgrad.nn.initializations import init_weights
+from synapgrad.nn import init
 
 
 class Linear(nn.Module):
     
-    def __init__(self, input_size:int, output_size:int, weight_init_method='he_normal', bias=True):
+    def __init__(self, in_features:int, out_features:int, bias=True):
         """ 
         Applies a linear transformation to the incoming data: y = x @ w.T + b. 
         
         This layer is also known as Dense layer.
         
         Reference:
             - https://pytorch.org/docs/stable/generated/torch.nn.Linear.html
         
         Args:
-            input_size (int): The number of features in the input tensor.
-            output_size (int): The number of features in the output tensor.
+            in_features (int): The number of features in the input tensor.
+            out_features (int): The number of features in the output tensor.
             weight_init_method (str): The method to use for initializing the weights.
                 Options are 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform', 'lecun_uniform'.
                 Defaults to 'he_normal'.
             bias: Whether to use a bias or not. Defaults to True.
         
         Returns:
             A tensor of shape (batch_size, output_size)
         
+        Variables:
+            - weight (synapgrad.Tensor) - the learnable weights of the module of shape
+                (out_features, in_features). The values are initialized from `U(-sqrt(k), sqrt(k))`
+                where `k = 1/in_features`.
+            - bias (synapgrad.Tensor) - the learnable bias of the module of shape (out_features).
+                If `bias=True`, the values are initialized from `U(-sqrt(k), sqrt(k))` where
+                `k = 1/in_features`.
+        
         Notes:
-            - The weights are initialized using the method specified in `weight_init_method`.
-            - The bias is initialized to zero.
             - The input tensor is expected to have a shape of (batch_size, input_size).
             - The output tensor is expected to have a shape of (batch_size, output_size).
         
         Example:
             >>> layer = nn.Linear(input_size=3, output_size=4)
             >>> x = synapgrad.ones((2, 3))
             >>> y = layer(x)
         """
         super().__init__()
-        self.input_size = input_size
-        self.output_size = output_size
-        
-        self.weight_init_method = weight_init_method
-        weight_values = init_weights((output_size, input_size), weight_init_method).astype(np.float32)
-        self.weight = nn.Parameter(weight_values, requires_grad=True, name='weight')
+        self.in_features = in_features
+        self.out_features = out_features
+      
+        self.weight = nn.Parameter(synapgrad.empty((out_features, in_features), dtype=np.float32, requires_grad=True, name='weight'))
         if bias: 
-            bias = synapgrad.zeros((output_size,), dtype=np.float32, requires_grad=True, name='bias')
-            self.bias = nn.Parameter(bias)
-        else: self.bias = None
+            self.bias = nn.Parameter(synapgrad.empty((out_features,), dtype=np.float32, requires_grad=True, name='bias'))
+        else:
+            self.bias = None
+        self.reset_parameters()
+        
+    def reset_parameters(self):
+        """ 
+        Resets the parameters of the layer.
+        """
+        fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)
+        std = 1. / math.sqrt(float(fan_in)) if fan_in > 0 else 0
+        init.uniform_(self.weight, -std, std)
+        if self.bias is not None:
+            init.uniform_(self.bias, -std, std)
         
     def forward(self, x:Tensor) -> Tensor:
-        assert x.shape[1] == self.input_size, f"Expected input size '{self.input_size}' but received '{x.shape[1]}'"
+        assert x.shape[1] == self.in_features, f"Expected input size '{self.in_features}' but received '{x.shape[1]}'"
         return F.linear(x, self.weight, self.bias)
 
 
 class Neuron(Linear):
 
-    def __init__(self, input_size: int, weight_init_method='he_normal', bias=True):
+    def __init__(self, in_features: int, bias=True):
         """ 
-        Creates a single Neuron layer. It's just a linear layer with output_size = 1
+        Creates a single Neuron layer. It's just a linear layer with out_features = 1
         
         Reference:
             - https://pytorch.org/docs/stable/generated/torch.nn.Linear.html
         
         Args:
             input_size (int): The number of features in the input tensor.
-            weight_init_method (str): The method to use for initializing the weights.
-                Options are 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform', 'lecun_uniform'.
-                Defaults to 'he_normal'.
             bias: Whether to use a bias or not. Defaults to True.
          
         Returns:
             A tensor of shape (batch_size, 1)
         
         Example:
             >>> layer = nn.Neuron(input_size=3)
             >>> x = synapgrad.ones((2, 3))
         """
-        super().__init__(input_size, 1, weight_init_method=weight_init_method, bias=bias)
+        super().__init__(in_features, 1, bias=bias)
     
 
 class Flatten(nn.Module):
     
     def __init__(self, start_dim=1, end_dim=-1) -> None:
         """ 
         Flattens a tensor over the specified start and end dimensions
@@ -361,15 +375,15 @@
     def forward(self, x: Tensor) -> Tensor: 
         return F.avg_pool2d(x, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding,
                                 dilation=self.dilation)
 
 
 class Conv1d(nn.Module):
     
-    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, bias=True, weight_init_method='he_uniform') -> None:
+    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, bias=True) -> None:
         """
         Applies 1D Convolution to a batch of N images with C channels (N, C, W).
         
         Reference:
             - https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html
 
         Args:
@@ -398,30 +412,38 @@
         self.in_channels = in_channels
         self.out_channels = out_channels
         self.kernel_size = kernel_size
         self.stride = stride
         self.stride = stride
         self.padding = padding
         self.dilation = dilation
-        
-        self.weight_init_method = weight_init_method
-        weight_values = init_weights((out_channels, in_channels, kernel_size), weight_init_method).astype(np.float32)
-        self.weight = nn.Parameter(weight_values, requires_grad=True, name='weight')
+
+        weight = synapgrad.empty((out_channels, in_channels, kernel_size), dtype=np.float32, requires_grad=True, name='weight')
+        self.weight = nn.Parameter(weight)
         if bias: 
-            bias = synapgrad.zeros((out_channels,), dtype=np.float32, requires_grad=True, name='bias')
+            bias = synapgrad.empty((out_channels,), dtype=np.float32, requires_grad=True, name='bias')
             self.bias = nn.Parameter(bias)
-        else: self.bias = None
+        else:
+            self.bias = None
+        self.reset_parameters()
+        
+    def reset_parameters(self) -> None:
+        fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)
+        bound = 1. / math.sqrt(float(fan_in)) if fan_in > 0 else 0
+        nn.init.uniform_(self.weight, -bound, bound)
+        if self.bias is not None:
+            nn.init.uniform_(self.bias, -bound, bound)
     
     def forward(self, x: Tensor) -> Tensor:
         return F.conv1d(x, self.weight, self.bias, self.stride, self.padding, self.dilation)
 
 
 class Conv2d(nn.Module):
     
-    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, bias=True, weight_init_method='he_uniform') -> None:
+    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, bias=True) -> None:
         """
         Applies 2D Convolution to a batch of N images with C channels (N, C, H, W).
         
         Reference:
             - https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html
 
         Args:
@@ -430,14 +452,20 @@
             kernel_size (int or tuple): the size of the sliding blocks
             stride (int or tuple, optional): the stride of the sliding blocks in the input 
                 spatial dimensions. Default: 1
             padding (int or tuple, optional): implicit zero padding to be added on both sides
                 of input. Default: 0
             dilation (int or tuple, optional): controls the spacing between the kernel points. Default: 1
             bias: Whether to use a bias or not. Defaults to True.
+            
+        Variables:
+            - weight (Tensor): the learnable weights of the module of shape (C_out, C_in, kH, kW). The
+                values of these weights are sampled from `U(-sqrt(k), sqrt(k))` where k = 1 / (C_in * kH * kW)
+            - bias (Tensor): the learnable bias of the module of shape (C_out). If bias is True, then the values
+                are initialized sampling from `U(-sqrt(k), sqrt(k))` where k = 1 / (C_in * kH * kW)
         
         Output:
             Tensor of shape (N, C_out, lW, lH).
         """
         super().__init__()
         
         kernel_size = np.broadcast_to(kernel_size, 2)
@@ -456,21 +484,29 @@
         self.out_channels = out_channels
         self.kernel_size = kernel_size
         self.stride = stride
         self.stride = stride
         self.padding = padding
         self.dilation = dilation
         
-        self.weight_init_method = weight_init_method
-        weight_values = init_weights((out_channels, in_channels, *kernel_size), weight_init_method).astype(np.float32)
-        self.weight = nn.Parameter(weight_values, requires_grad=True, name='weight')
+        weight = synapgrad.empty((out_channels, in_channels, kernel_size[0], kernel_size[1]), dtype=np.float32, requires_grad=True, name='weight')
+        self.weight = nn.Parameter(weight)
         if bias:
-            bias = synapgrad.zeros((out_channels,), dtype=np.float32, requires_grad=True, name='bias') 
+            bias = synapgrad.empty((out_channels,), dtype=np.float32, requires_grad=True, name='bias') 
             self.bias = nn.Parameter(bias)
-        else: self.bias = None
+        else: 
+            self.bias = None
+        self.reset_parameters()
+        
+    def reset_parameters(self) -> None:
+        fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)
+        bound = 1. / math.sqrt(float(fan_in)) if fan_in > 0 else 0
+        nn.init.uniform_(self.weight, -bound, bound)
+        if self.bias is not None:
+            nn.init.uniform_(self.bias, -bound, bound)
     
     def forward(self, x: Tensor) -> Tensor:
         return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation)
     
     
 class BatchNorm(nn.Module):
     
@@ -492,22 +528,29 @@
             self.running_var = synapgrad.ones(num_features, dtype=dtype, name='running_var')
         else:
             self.running_mean = None
             self.running_var = None
         
         if affine:
             # gamma
-            gamma = synapgrad.ones(num_features, requires_grad=True, dtype=dtype, name="gamma")
+            gamma = synapgrad.empty(num_features, requires_grad=True, dtype=dtype, name="gamma")
             self.weight = nn.Parameter(gamma)
             # beta
-            beta = synapgrad.zeros(num_features, requires_grad=True, dtype=dtype, name="beta")
+            beta = synapgrad.empty(num_features, requires_grad=True, dtype=dtype, name="beta")
             self.bias = nn.Parameter(beta)
+            # Initialize parameters
+            self.reset_parameters()
         else:
             self.weight = None
             self.bias = None
+            
+    def reset_parameters(self):
+        if self.affine:
+            init.ones_(self.weight)
+            init.zeros_(self.bias)
                 
     def forward(self, x: Tensor) -> Tensor:
         if self.momentum is None:
             exponential_average_factor = 0.0
         else:
             exponential_average_factor = self.momentum
 
@@ -526,38 +569,14 @@
         
         running_mean = self.running_mean if not self.training or self.track_running_stats else None
         running_var = self.running_var if not self.training or self.track_running_stats else None
             
         return F.batch_norm(x, self.weight, self.bias, running_mean, running_var,
                                                 bn_training, exponential_average_factor, self.eps)
 
-        # calculate running estimates
-        if self.training:
-            mean = x.mean(dim=dims)
-            # use biased var in train
-            var_sum = ((x - mean.reshape(view_shape))**2).sum(dim=dims)
-            var = var_sum / n
-            
-            with synapgrad.no_grad():
-                r_mu = (exponential_average_factor * mean.data + (1 - exponential_average_factor) * self.running_mean.data)
-                self.running_mean = synapgrad.tensor(r_mu)
-                
-                unbiased_var = var_sum.data / (n - 1)
-                r_var = (exponential_average_factor * unbiased_var + (1 - exponential_average_factor) * self.running_var.data)
-                self.running_var = synapgrad.tensor(r_var)
-        else:
-            mean = self.running_mean
-            var = self.running_var
-
-        out = (x - mean.reshape(view_shape)) / (var.reshape(view_shape) + self.eps).sqrt()
-        if self.affine:
-            out = out * self.weight.reshape(view_shape) + self.bias.reshape(view_shape)
-
-        return out
-
 
 class BatchNorm1d(BatchNorm):
 
     def __init__(self, num_features:int, eps:float=1e-5, momentum:float=0.1, affine:bool=True,
                  track_running_stats:bool=True, dtype=None) -> None:
         """
         Computes the batchnorm of a 2 or 3 dimensional tensor (N, C) or (N, C, L)
```

## synapgrad/nn/modules.py

```diff
@@ -66,14 +66,25 @@
         self.check_is_initialized()
         
         if not isinstance(parameter, Parameter):
             raise TypeError("All parameters must be of type Parameter")
         
         self._parameters[name] = parameter
         object.__setattr__(self, name, parameter)
+        
+    def apply(self, fn):
+        """ 
+        Applies fn recursively to every submodule as well as self. 
+        Typical use includes initializing the parameters of a model
+        """
+        self.check_is_initialized()
+        
+        fn(self)
+        for m in self.submodules():
+            m.apply(fn)
     
     def __setattr__(self, __name: str, __value: Any) -> None:
         """
         Keeps track of the submodules and parameters added to this module 
         """
         if isinstance(__value, Module):
             self.register_module(__name, __value)
```

## synapgrad/optim/__init__.py

```diff
@@ -1,2 +1,2 @@
 
-from synapgrad.optim.optimizers import Optimizer, SGD, Adam
+from synapgrad.optim.optimizers import Optimizer, SGD, Adam, AdamW
```

## synapgrad/optim/optimizers.py

```diff
@@ -131,8 +131,60 @@
                 # Update biased second raw moment estimate
                 self.m2[i] = self.beta2 * self.m2[i] + (1.0 - self.beta2) * grad**2.0
                 
                 m1_corrected = self.m1[i] / (1.0 - self.beta1**self.t)
                 m2_corrected = self.m2[i] / (1.0 - self.beta2**self.t)
 
                 # Update the parameters using the Adam formula
+                p.data -= (self.lr * m1_corrected) / (np.sqrt(m2_corrected) + self.epsilon)
+                
+                
+class AdamW(Optimizer):
+
+    def __init__(self, parameters: list[Tensor], lr=0.001, betas=(0.9, 0.999), eps=1e-8,
+                    weight_decay=0, maximize=False) -> None:
+        """
+        Implements AdamW algorithm.
+        
+        Reference: 
+            - https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html
+        
+        Args:
+            params (iterable): Iterable of model parameters to optimize.
+            lr (float, optional): Learning rate. Default is 0.001.
+            betas (tuple, optional): Coefficients used for computing running averages of gradient and its square.
+                                    Default is (0.9, 0.999).
+            eps (float, optional): Term added to the denominator to improve numerical stability. Default is 1e-08.
+            weight_decay (float, optional): Weight decay (L2 penalty) factor. Default is 0.
+            maximize (bool, optional): Whether to maximize or minimize the objective function. Default is False.
+        
+        """
+        super().__init__(parameters, lr)
+        self.beta1 = betas[0]
+        self.beta2 = betas[1]
+        self.epsilon = eps
+        self.weight_decay = weight_decay
+        self.maximize = maximize
+        
+        self.m1 = [0 for _ in range(len(parameters))]
+        self.m2 = [0 for _ in range(len(parameters))]
+        
+    def step(self):
+        super().step()
+        with synapgrad.no_grad():
+            for i, p in enumerate(self.parameters):
+                grad = -p._grad if self.maximize else p._grad   
+                
+                # Weight decay
+                p.data -= self.lr*self.weight_decay*p.data
+                    
+                # Update biased first moment estimate
+                self.m1[i] = self.beta1 * self.m1[i] + (1.0 - self.beta1) * grad
+                
+                # Update biased second raw moment estimate
+                self.m2[i] = self.beta2 * self.m2[i] + (1.0 - self.beta2) * grad**2.0
+                
+                m1_corrected = self.m1[i] / (1.0 - self.beta1**self.t)
+                m2_corrected = self.m2[i] / (1.0 - self.beta2**self.t)
+
+                # Update the parameters using the Adam formula
                 p.data -= (self.lr * m1_corrected) / (np.sqrt(m2_corrected) + self.epsilon)
```

## Comparing `synapgrad-0.5.1.dist-info/LICENSE` & `synapgrad-0.6.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `synapgrad-0.5.1.dist-info/METADATA` & `synapgrad-0.6.0.dist-info/METADATA`

 * *Files 4% similar despite different names*

```diff
@@ -1,25 +1,25 @@
 Metadata-Version: 2.1
 Name: synapgrad
-Version: 0.5.1
-Summary: An autograd Tensor-based engine with a deep learning library built on top of it made from scratch
+Version: 0.6.0
+Summary: A lightweight autograd Tensor-based engine with a deep learning library built on top of it made from scratch
 Home-page: https://github.com/pgmesa/synapgrad
 Author: Pablo García Mesa
 Author-email: pgmesa.sm@gmail.com
 Classifier: Programming Language :: Python :: 3.9
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Operating System :: OS Independent
 Requires-Python: >=3.9
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: numpy (==1.23.5)
 
 #  SynapGrad
 
-An autograd Tensor-based engine with a deep learning library built on top of it made from scratch
+A lightweight autograd Tensor-based engine with a deep learning library built on top of it made from scratch
 
 [![Downloads](https://static.pepy.tech/personalized-badge/synapgrad?period=total&units=international_system&left_color=black&right_color=blue&left_text=Downloads)](https://pepy.tech/project/synapgrad)
 
 ## Installation
 ```bash
 pip install synapgrad
 ```
@@ -28,15 +28,15 @@
 This project implements a completely functional engine for tracking operations between Tensors, by dynamically building a Directed Acyclic Graph (DAG), and an automatic gradient calculation and backpropagation algorithm (reverse-mode autodiff) over this DAG.
 
 Built on top of the engine, the deep learning library implements the most common functions, layers, losses and optimizers in order to create AI models able to solve real problems.
 
 This library mimics Pytorch in a simplified way, but with similar functions and behaviour. 
 
 ## Aim of the project
-The objective of this project is to develop a deep learning library entirely from scratch, without relying on any existing frameworks (such as Keras, PyTorch, TensorFlow, scikit-learn, etc.). The primary goal is to gain an in-depth comprehension of the fundamental mechanisms underlying deep learning.
+The objective of this project is to develop a lightweight deep learning library entirely from scratch, without relying on any existing frameworks (such as Keras, PyTorch, TensorFlow, scikit-learn, etc.), using only the `numpy` library.
 
 ## Autograd Engine
 Automatic gradient calculation and backpropagation algorithm
 
 ### Requirements
 ```r
 numpy==1.23.5 # Core
@@ -60,19 +60,20 @@
 z.draw_graph()
 ```
 ![Graph Image](/.github/graph_example.svg)
 
 ## Deep learning library
 Built on top of the engine, synapgrad has a deep learning library that implements the following features:
 
-- `Weight initialization`: Xavier Glorot uniform, Xavier Glorot normal, He Kaiming uniform, He Kaiming normal, LeCun uniform
-- `Activations`: ReLU, Tanh, Sigmoid, Softmax, LogSoftmax
+- `Weight initialization`: Xavier Glorot uniform, Xavier Glorot normal, He Kaiming uniform, He Kaiming normal
+- `Activations`: ReLU, LeakyReLU, SELU, Tanh, Sigmoid, Softmax, LogSoftmax
+- `Layers`: Linear, Unfold, Fold, Flatten, Dropout
 - `Convolutions`: MaxPool1d, MaxPool2d, AvgPool1d, AvgPool2d, Conv1d, Conv2d
-- `Layers`: Linear, Unfold, Fold, BatchNorm1d, BatchNorm2d, Flatten, Dropout
-- `Optimizers`: SGD, Adam
+- `Normalizations`: BatchNorm1d, BatchNorm2d
+- `Optimizers`: SGD, Adam, AdamW
 - `Losses`: MSELoss, NLLLoss, BCELoss, BCEWithLogitsLoss, CrossEntropyLoss
 
 This project includes three Jupyter notebooks (located in `examples/`) that tackle three beginner-level AI problems:
 
 - [x] 1. Basic MLP for binary classification (sklearn 'make_moons' toy dataset)
 - [x] 2. MLP for handwritten digits classification (MNIST dataset) 
 - [x] 3. CNN for handwritten digits classification (MNIST dataset)
```

## Comparing `synapgrad-0.5.1.dist-info/RECORD` & `synapgrad-0.6.0.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,26 +1,28 @@
-synapgrad/__init__.py,sha256=esJPMRiAEBWaDfirVyf-z_ImxUylw3AR0rEoN4tvVkM,852
+synapgrad/__init__.py,sha256=VCwGMfN_dfn5jpgtXVHvqVmPp3k9hOzvPRA3lqpo3Yw,877
 synapgrad/conv_tools.py,sha256=Tx7K_2K6-7LQsuWLkFG7jugfKK78fPb_VlLGIwiFm20,22932
-synapgrad/cpu_ops.py,sha256=kkIIc0MgiMq65yRvgsnjUiQwdt0eCO-umolzejnzl1A,21480
+synapgrad/cpu_ops.py,sha256=FaiCxCzu9uGi0cPbm3MJdhpiBiGeLJ4sZ-46A1gmlao,22055
 synapgrad/device.py,sha256=eNnmGWhQuNmwc5K0TLPuj3KuxRKermqEfBFf9R8ouSY,171
 synapgrad/functional.py,sha256=K-yRA-OkKXjqvkPaNPDIVXybK2eHcBOkGeys3eu2Dkw,36424
-synapgrad/tensor.py,sha256=2MHDR-_PjsJaM29BeK7MbPOImX_CgCglyA_sDwtMNfw,20437
+synapgrad/tensor.py,sha256=B5btceqUhAKHbEp7NO_ganR9qxwP3k4xUHo_OMe_jYc,21144
 synapgrad/utils.py,sha256=mSLyO-miwD0KG0pQ6oJ81qCEa-Obx_Jm1AzEYbIp0jE,1980
-synapgrad/nn/__init__.py,sha256=vyywTJXfyOgZvh-OcPdjoyDd_IOu01aSGsBv8o22Nzw,445
-synapgrad/nn/activations.py,sha256=Hi_2-rvdWUvd_ocbZNoAM14cBC4qWlqqqWiVT3hYppI,1731
-synapgrad/nn/functional.py,sha256=k3agwz8aQrrP67ryS82QUswP6ZkK032M__DfmC8W830,35049
-synapgrad/nn/initializations.py,sha256=h2zqYhADolyShJx8brX-hSiCL-xpqTCvNsR1-1j6FAk,2907
-synapgrad/nn/layers.py,sha256=ZwZM8bFecHAsgK3swq0ZxXGega8HKKt0bXHtneA9Ydo,25851
+synapgrad/c/__init__.py,sha256=MFCsOa-IjzwZxkplWoSGjOp5Q7Zkp6J_47e9yOLa68k,81
+synapgrad/c/conv_tools.py,sha256=Pmds9teTMY64lUjyDSHoD-fAYzGcRyOs8KKpGcCfL90,1985
+synapgrad/nn/__init__.py,sha256=Ho6jS9SGm27v02KVdkNparyGq0qXamwmtD2mwZpeBsE,462
+synapgrad/nn/activations.py,sha256=UKxomXXqdYppn_G544TN3Ml_dP9ONfmH70v-OLLT9wY,2562
+synapgrad/nn/functional.py,sha256=nCSZa-G67UDP7qKJffud6G5dm4HwHa-nRhI0CGLr9SU,37187
+synapgrad/nn/init.py,sha256=LW1xFLrUs1qZP6-gFvmvQ5cSIr-QFHzYPl5GmmXynVw,9540
+synapgrad/nn/layers.py,sha256=F3aIB2bIafaqrSShHA_W-ogIK5WLsNp4a6oqW6WvSsU,26350
 synapgrad/nn/losses.py,sha256=YTITerYGqX2lSoTP5tqI7YrruqG9qiWHEwY2zh2KCag,4321
-synapgrad/nn/modules.py,sha256=SoRuJJNGrCQtor9t_Zx8PFlc0_uEPWyNGs46qtEjGD8,4967
+synapgrad/nn/modules.py,sha256=io0_0Hyqz1qDHtWY7-0QT8rlIg5eWIkhxg358TAs_sE,5295
 synapgrad/nn/utils/__init__.py,sha256=d12ONwt_8qYes7FC4Jk3oWIZvZHZNiMj3aaU79Df2lA,198
 synapgrad/nn/utils/data.py,sha256=6oM7r7uh-Fe7ynUmX49BBD0eVohmFcrAKFEvSDCYjMw,3207
 synapgrad/nn/utils/train.py,sha256=V3j4iqSW9sjSfIz9bski95Ni25KuYzIAkjqD4o3-Tjg,10586
-synapgrad/optim/__init__.py,sha256=Zf5AT4UC9suDpKl_SYHVbVu9bwfUJXkc4k2SuEnXgtA,61
-synapgrad/optim/optimizers.py,sha256=u1n2SPH4kSH_gSuYBt_XlvVC-AcJGQhffAn9c5ZohFg,5426
+synapgrad/optim/__init__.py,sha256=VnIR3Jb34jQy6owZLqxfCtL4wfvIzC63mh-84AC1aq4,68
+synapgrad/optim/optimizers.py,sha256=FwjMpsKnpuA_JIlLL-HaKBLy5ZFyBVrfHP5qbnXGT1s,7784
 synapgrad/visual/__init__.py,sha256=lfiZqPYDvdRY4ollOKTnoonUYJ-Bsg5NvuJ7ycqNhAU,34
 synapgrad/visual/graph.py,sha256=ru57v_ngT9dZzdRaXtZkjHhm9dCbHkLFGcJWzuSW-Lc,1783
-synapgrad-0.5.1.dist-info/LICENSE,sha256=tILw81bLBHb_8uhiXZAlAo5QrlyMSCJH33lb7GYpz6E,1088
-synapgrad-0.5.1.dist-info/METADATA,sha256=fDYm0jFHY-7KDocsCa6TxElJnyI-Nv2vszCRyJeKlX0,5003
-synapgrad-0.5.1.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-synapgrad-0.5.1.dist-info/top_level.txt,sha256=D7_rcC0S5ytl1AFRAo5c73mZggZB-Z_zm_GNIGe0QgM,10
-synapgrad-0.5.1.dist-info/RECORD,,
+synapgrad-0.6.0.dist-info/LICENSE,sha256=tILw81bLBHb_8uhiXZAlAo5QrlyMSCJH33lb7GYpz6E,1088
+synapgrad-0.6.0.dist-info/METADATA,sha256=jigWqBWa5TA6hmtWKFLIb__x9pp7j-hkOx5g_Rt7kyQ,4988
+synapgrad-0.6.0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+synapgrad-0.6.0.dist-info/top_level.txt,sha256=D7_rcC0S5ytl1AFRAo5c73mZggZB-Z_zm_GNIGe0QgM,10
+synapgrad-0.6.0.dist-info/RECORD,,
```

