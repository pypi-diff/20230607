# Comparing `tmp/trulens_eval-0.1.2-py3-none-any.whl.zip` & `tmp/trulens_eval-0.1.2a0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,31 +1,30 @@
-Zip file size: 78717 bytes, number of entries: 29
--rw-r--r--  2.0 unx     4782 b- defN 23-Jun-07 13:21 trulens_eval/Example_TruBot.py
--rw-r--r--  2.0 unx     2890 b- defN 23-Jun-06 18:55 trulens_eval/Leaderboard.py
--rw-r--r--  2.0 unx      749 b- defN 23-Jun-07 13:21 trulens_eval/__init__.py
--rw-r--r--  2.0 unx     5401 b- defN 23-May-31 15:12 trulens_eval/benchmark.py
--rw-r--r--  2.0 unx     3443 b- defN 23-May-31 15:12 trulens_eval/feedback_prompts.py
--rw-r--r--  2.0 unx      985 b- defN 23-Jun-04 15:17 trulens_eval/keys.py
--rw-r--r--  2.0 unx     3376 b- defN 23-Jun-06 18:55 trulens_eval/provider_apis.py
--rw-r--r--  2.0 unx    16001 b- defN 23-Jun-06 18:55 trulens_eval/schema.py
--rw-r--r--  2.0 unx    11263 b- defN 23-Jun-06 18:55 trulens_eval/slackbot.py
--rw-r--r--  2.0 unx    10308 b- defN 23-Jun-07 13:21 trulens_eval/tru.py
--rw-r--r--  2.0 unx    20398 b- defN 23-Jun-07 13:21 trulens_eval/tru_chain.py
--rw-r--r--  2.0 unx    18525 b- defN 23-Jun-07 13:21 trulens_eval/tru_db.py
--rw-r--r--  2.0 unx    28881 b- defN 23-Jun-07 13:21 trulens_eval/tru_feedback.py
--rw-r--r--  2.0 unx    28697 b- defN 23-Jun-06 18:55 trulens_eval/util.py
--rw-r--r--  2.0 unx     4782 b- defN 23-Jun-07 13:21 trulens_eval/examples/App_TruBot.py
--rw-r--r--  2.0 unx    11074 b- defN 23-Jun-07 13:21 trulens_eval/examples/trubot.py
--rw-r--r--  2.0 unx    11975 b- defN 23-Jun-06 18:55 trulens_eval/pages/Evaluations.py
--rw-r--r--  2.0 unx     1518 b- defN 23-Jun-06 18:55 trulens_eval/pages/Progress.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-31 15:12 trulens_eval/pages/__init__.py
--rw-r--r--  2.0 unx     5455 b- defN 23-May-31 15:12 trulens_eval/tests/test_tru_chain.py
--rw-r--r--  2.0 unx     1803 b- defN 23-Jun-06 18:55 trulens_eval/utils/langchain.py
--rw-r--r--  2.0 unx      915 b- defN 23-Jun-04 15:17 trulens_eval/ux/add_logo.py
--rw-r--r--  2.0 unx     1273 b- defN 23-Jun-06 18:55 trulens_eval/ux/components.py
--rw-r--r--  2.0 unx     1213 b- defN 23-Jun-06 18:55 trulens_eval/ux/styles.py
--rw-r--r--  2.0 unx    29567 b- defN 23-May-31 15:12 trulens_eval/ux/trulens_logo.svg
--rw-r--r--  2.0 unx    13424 b- defN 23-Jun-07 13:26 trulens_eval-0.1.2.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-07 13:26 trulens_eval-0.1.2.dist-info/WHEEL
--rw-r--r--  2.0 unx       13 b- defN 23-Jun-07 13:26 trulens_eval-0.1.2.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2434 b- defN 23-Jun-07 13:26 trulens_eval-0.1.2.dist-info/RECORD
-29 files, 241237 bytes uncompressed, 74823 bytes compressed:  69.0%
+Zip file size: 71620 bytes, number of entries: 28
+-rw-rw-r--  2.0 unx     4782 b- defN 23-Jun-02 15:56 trulens_eval/Example_TruBot.py
+-rw-rw-r--  2.0 unx     2422 b- defN 23-Jun-02 15:56 trulens_eval/Leaderboard.py
+-rw-rw-r--  2.0 unx      407 b- defN 23-Jun-02 16:01 trulens_eval/__init__.py
+-rw-rw-r--  2.0 unx     5401 b- defN 23-Jun-02 15:56 trulens_eval/benchmark.py
+-rw-rw-r--  2.0 unx     3443 b- defN 23-Jun-02 15:56 trulens_eval/feedback_prompts.py
+-rw-rw-r--  2.0 unx      949 b- defN 23-Jun-02 15:56 trulens_eval/keys.py
+-rw-rw-r--  2.0 unx     3341 b- defN 23-Jun-02 15:56 trulens_eval/provider_apis.py
+-rw-rw-r--  2.0 unx    11179 b- defN 23-Jun-02 15:56 trulens_eval/slackbot.py
+-rw-rw-r--  2.0 unx     5455 b- defN 23-Jun-02 15:56 trulens_eval/test_tru_chain.py
+-rw-rw-r--  2.0 unx    10449 b- defN 23-Jun-02 15:56 trulens_eval/tru.py
+-rw-rw-r--  2.0 unx    22796 b- defN 23-Jun-02 15:56 trulens_eval/tru_chain.py
+-rw-rw-r--  2.0 unx    29397 b- defN 23-Jun-02 15:56 trulens_eval/tru_db.py
+-rw-rw-r--  2.0 unx    30628 b- defN 23-Jun-02 15:56 trulens_eval/tru_feedback.py
+-rw-rw-r--  2.0 unx     4067 b- defN 23-Jun-02 15:56 trulens_eval/util.py
+-rw-rw-r--  2.0 unx     4782 b- defN 23-Jun-02 15:56 trulens_eval/examples/App_TruBot.py
+-rw-rw-r--  2.0 unx    11074 b- defN 23-Jun-02 15:56 trulens_eval/examples/trubot.py
+-rw-rw-r--  2.0 unx    10278 b- defN 23-Jun-02 15:56 trulens_eval/pages/Evaluations.py
+-rw-rw-r--  2.0 unx     1323 b- defN 23-Jun-02 15:56 trulens_eval/pages/Progress.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-02 15:56 trulens_eval/pages/__init__.py
+-rw-rw-r--  2.0 unx     5455 b- defN 23-Jun-02 15:56 trulens_eval/tests/test_tru_chain.py
+-rw-rw-r--  2.0 unx     1484 b- defN 23-Jun-02 15:56 trulens_eval/utils/langchain.py
+-rw-rw-r--  2.0 unx      915 b- defN 23-Jun-02 15:56 trulens_eval/ux/add_logo.py
+-rw-rw-r--  2.0 unx     1142 b- defN 23-Jun-02 15:56 trulens_eval/ux/components.py
+-rw-rw-r--  2.0 unx    29567 b- defN 23-Jun-02 15:56 trulens_eval/ux/trulens_logo.svg
+-rw-rw-r--  2.0 unx    12734 b- defN 23-Jun-02 16:01 trulens_eval-0.1.2a0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Jun-02 16:01 trulens_eval-0.1.2a0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       13 b- defN 23-Jun-02 16:01 trulens_eval-0.1.2a0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     2366 b- defN 23-Jun-02 16:01 trulens_eval-0.1.2a0.dist-info/RECORD
+28 files, 215941 bytes uncompressed, 67820 bytes compressed:  68.6%
```

## zipnote {}

```diff
@@ -15,18 +15,18 @@
 
 Filename: trulens_eval/keys.py
 Comment: 
 
 Filename: trulens_eval/provider_apis.py
 Comment: 
 
-Filename: trulens_eval/schema.py
+Filename: trulens_eval/slackbot.py
 Comment: 
 
-Filename: trulens_eval/slackbot.py
+Filename: trulens_eval/test_tru_chain.py
 Comment: 
 
 Filename: trulens_eval/tru.py
 Comment: 
 
 Filename: trulens_eval/tru_chain.py
 Comment: 
@@ -63,26 +63,23 @@
 
 Filename: trulens_eval/ux/add_logo.py
 Comment: 
 
 Filename: trulens_eval/ux/components.py
 Comment: 
 
-Filename: trulens_eval/ux/styles.py
-Comment: 
-
 Filename: trulens_eval/ux/trulens_logo.svg
 Comment: 
 
-Filename: trulens_eval-0.1.2.dist-info/METADATA
+Filename: trulens_eval-0.1.2a0.dist-info/METADATA
 Comment: 
 
-Filename: trulens_eval-0.1.2.dist-info/WHEEL
+Filename: trulens_eval-0.1.2a0.dist-info/WHEEL
 Comment: 
 
-Filename: trulens_eval-0.1.2.dist-info/top_level.txt
+Filename: trulens_eval-0.1.2a0.dist-info/top_level.txt
 Comment: 
 
-Filename: trulens_eval-0.1.2.dist-info/RECORD
+Filename: trulens_eval-0.1.2a0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## trulens_eval/Leaderboard.py

```diff
@@ -5,16 +5,14 @@
 import streamlit as st
 from streamlit_extras.switch_page_button import switch_page
 
 st.runtime.legacy_caching.clear_cache()
 
 from trulens_eval import Tru
 from trulens_eval import tru_db
-from trulens_eval.ux import styles
-from trulens_eval.tru_feedback import default_pass_fail_color_threshold
 
 st.set_page_config(page_title="Leaderboard", layout="wide")
 
 from trulens_eval.ux.add_logo import add_logo
 
 add_logo()
 
@@ -61,39 +59,31 @@
                 sum(
                     tokens for tokens in chain_df.total_tokens
                     if tokens is not None
                 ),
                 precision=2
             )
         )
+
         for i, col_name in enumerate(feedback_col_names):
             mean = chain_df[col_name].mean()
 
-            st.write(
-                styles.stmetricdelta_hidearrow,
-                unsafe_allow_html=True,
-            )
+            if i < len(feedback_cols):
+                if math.isnan(mean):
+                    pass
 
-            if math.isnan(mean):
-                pass
+                else:
+                    feedback_cols[i].metric(col_name, round(mean, 2))
 
             else:
-                if mean >= default_pass_fail_color_threshold:
-                    feedback_cols[i].metric(
-                        label=col_name,
-                        value=f'{round(mean, 2)}',
-                        delta='✅ High'
-                    )
+                if math.isnan(mean):
+                    pass
+
                 else:
-                    feedback_cols[i].metric(
-                        label=col_name,
-                        value=f'{round(mean, 2)}',
-                        delta='⚠️ Low ',
-                        delta_color="inverse"
-                    )
+                    feedback_cols[i].metric(col_name, round(mean, 2))
 
         with col99:
             if st.button('Select Chain', key=f"model-selector-{chain}"):
                 st.session_state.chain = chain
                 switch_page('Evaluations')
 
         st.markdown("""---""")
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## trulens_eval/__init__.py

```diff
@@ -1,36 +1,13 @@
 """
-# Imports of most common parts of the library.
-
-    Should include everything to get started.
-
-# Module organization/dependency
-
-    Modules on lower lines should not import modules on same or above lines:
-
-    - __init__.py
-
-    - tru_chain.py
-
-    - tru.py
-
-    - tru_feedback.py
-
-    - tru_db.py
-
-    - provider_apis.py feedback_prompts.py
-
-    - schema.py
-
-    - util.py keys.py
+Imports of most common parts of the library. Should include everything to get started.
 """
 
-__version__ = "0.1.2"
+__version__ = "0.1.2a"
 
 from trulens_eval.tru_chain import TruChain
 from trulens_eval.tru_feedback import Feedback
 from trulens_eval.tru_feedback import OpenAI
 from trulens_eval.tru_feedback import Huggingface
 from trulens_eval.tru import Tru
-from trulens_eval.tru_db import Query
 
-__all__ = ['TruChain', 'Feedback', 'OpenAI', 'Huggingface', 'Tru', 'Query']
+__all__ = ['TruChain', 'Feedback', 'OpenAI', 'Huggingface', 'Tru']
```

## trulens_eval/keys.py

```diff
@@ -16,20 +16,17 @@
 for k, v in config.items():
     print(f"KEY SET: {k}")
     globals()[k] = v
 
     # set them into environment as well
     os.environ[k] = v
 
-
-def set_openai_key():
-    if 'OPENAI_API_KEY' in os.environ:
-        import openai
-        openai.api_key = os.environ["OPENAI_API_KEY"]
-
+if 'OPENAI_API_KEY' in os.environ:
+    import openai
+    openai.api_key = os.environ["OPENAI_API_KEY"]
 
 global cohere_agent
 cohere_agent = None
 
 
 def get_cohere_agent():
     global cohere_agent
```

## trulens_eval/provider_apis.py

```diff
@@ -8,16 +8,14 @@
 import requests
 from tqdm.auto import tqdm
 from trulens_eval.tru_db import JSON
 
 from trulens_eval.util import SingletonPerName
 from trulens_eval.util import TP
 
-logger = logging.getLogger(__name__)
-
 
 class Endpoint(SingletonPerName):
 
     def __init__(
         self, name: str, rpm: float = 60, retries: int = 3, post_headers=None
     ):
         """
@@ -35,15 +33,15 @@
           post.
         """
 
         if hasattr(self, "rpm"):
             # already initialized via the SingletonPerName mechanism
             return
 
-        logger.debug(f"*** Creating {name} endpoint ***")
+        logging.debug(f"*** Creating {name} endpoint ***")
 
         self.rpm = rpm
         self.retries = retries
         self.pace = Queue(
             maxsize=rpm // 6
         )  # 10 second's worth of accumulated api
         self.tqdm = tqdm(desc=f"{name} api", unit="requests")
@@ -72,15 +70,15 @@
         ret = requests.post(url, json=payload, timeout=timeout, **extra)
 
         j = ret.json()
 
         # Huggingface public api sometimes tells us that a model is loading and how long to wait:
         if "estimated_time" in j:
             wait_time = j['estimated_time']
-            logger.error(f"Waiting for {j} ({wait_time}) second(s).")
+            logging.error(f"Waiting for {j} ({wait_time}) second(s).")
             sleep(wait_time + 2)
             return self.post(url, payload)
 
         assert isinstance(
             j, Sequence
         ) and len(j) > 0, f"Post did not return a sequence: {j}"
 
@@ -98,15 +96,15 @@
         while retries > 0:
             try:
                 self.pace_me()
                 ret = thunk()
                 return ret
             except Exception as e:
                 retries -= 1
-                logger.error(
+                logging.error(
                     f"{self.name} request failed {type(e)}={e}. Retries={retries}."
                 )
                 if retries > 0:
                     sleep(retry_delay)
                     retry_delay *= 2
 
         raise RuntimeError(
```

## trulens_eval/slackbot.py

```diff
@@ -20,21 +20,20 @@
 import pinecone
 from pydantic import Field
 from slack_bolt import App
 from slack_sdk import WebClient
 
 from trulens_eval import Tru
 from trulens_eval import tru_feedback
-from trulens_eval.schema import FeedbackMode
 from trulens_eval.tru_chain import TruChain
 from trulens_eval.tru_db import LocalSQLite
-from trulens_eval.tru_db import Query
+from trulens_eval.tru_db import Record
 from trulens_eval.tru_feedback import Feedback
 from trulens_eval.util import TP
-from trulens_eval.utils.langchain import WithFeedbackFilterDocuments
+from trulens_eval.utils.langchain import WithFilterDocuments
 
 os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'
 
 pp = PrettyPrinter()
 
 PORT = 3000
 verb = False
@@ -71,28 +70,29 @@
 # Construct feedback functions.
 
 hugs = tru_feedback.Huggingface()
 openai = tru_feedback.OpenAI()
 
 # Language match between question/answer.
 f_lang_match = Feedback(hugs.language_match).on(
-    text1=Query.RecordInput, text2=Query.RecordOutput
+    text1="prompt", text2="response"
 )
 
 # Question/answer relevance between overall question and answer.
 f_qa_relevance = Feedback(openai.relevance).on(
-    prompt=Query.RecordInput, response=Query.RecordOutput
+    prompt="input", response="output"
 )
 
 # Question/statement relevance between question and each context chunk.
-f_qs_relevance = tru_feedback.Feedback(openai.qs_relevance).on(
-    question=Query.RecordInput,
-    statement=Query.Record.chain.combine_docs_chain._call.args.inputs.
-    input_documents[:].page_content
-).aggregate(np.min)
+f_qs_relevance = Feedback(openai.qs_relevance).on(
+    question="input",
+    statement=Record.chain.combine_docs_chain._call.args.inputs.input_documents
+).on_multiple(
+    multiarg="statement", each_query=Record.page_content, agg=np.min
+)
 
 
 def filter_by_relevance(query, doc):
     return openai.qs_relevance(question=query, statement=doc.page_content) > 0.5
 
 
 def get_or_make_chain(cid: str, selector: int = 0) -> TruChain:
@@ -119,16 +119,16 @@
     docsearch = Pinecone.from_existing_index(
         index_name="llmdemo", embedding=embedding
     )
 
     retriever = docsearch.as_retriever()
 
     if "filtered" in chain_id:
-        retriever = WithFeedbackFilterDocuments.of_retriever(
-            retriever=retriever, feedback=f_qs_relevance, threshold=0.5
+        retriever = WithFilterDocuments.of_retriever(
+            retriever=retriever, filter_func=filter_by_relevance
         )
 
     # LLM for completing prompts, and other tasks.
     llm = OpenAI(temperature=0, max_tokens=128)
 
     # Conversation memory.
     memory = ConversationSummaryBufferMemory(
@@ -187,15 +187,15 @@
         chain.combine_docs_chain.document_prompt.template = "\tContext: {page_content}"
 
     # Trulens instrumentation.
     tc = tru.Chain(
         chain=chain,
         chain_id=chain_id,
         feedbacks=[f_lang_match, f_qa_relevance, f_qs_relevance],
-        feedback_mode=FeedbackMode.DEFERRED
+        feedback_mode="deferred"
     )
 
     convos[cid] = tc
 
     return tc
 
 
@@ -205,15 +205,15 @@
     sources elaboration text.
     """
 
     # Pace our API usage. This is not perfect since the chain makes multiple api calls
     # internally.
     openai.endpoint.pace_me()
 
-    outs = chain(question)
+    outs = chain(dict(question=question))
 
     result = outs['answer']
     sources = outs['source_documents']
 
     result_sources = "Sources:\n"
 
     temp = set()
```

## trulens_eval/tru.py

```diff
@@ -9,16 +9,16 @@
 from time import sleep
 from typing import Iterable, List, Optional, Sequence, Union
 
 import pkg_resources
 
 from trulens_eval.tru_db import JSON
 from trulens_eval.tru_db import LocalSQLite
+from trulens_eval.tru_db import TruDB
 from trulens_eval.tru_feedback import Feedback
-from trulens_eval.schema import FeedbackResult, Model, Record
 from trulens_eval.util import TP, SingletonPerName
 
 
 class Tru(SingletonPerName):
     """
     Tru is the main class that provides an entry points to trulens-eval. Tru lets you:
 
@@ -33,69 +33,100 @@
 
     # Process or Thread of the deferred feedback function evaluator.
     evaluator_proc = None
 
     # Process of the dashboard app.
     dashboard_proc = None
 
-    def Chain(self, **kwargs):
+    def Chain(self, *args, **kwargs):
         """
         Create a TruChain with database managed by self.
         """
 
         from trulens_eval.tru_chain import TruChain
 
-        return TruChain(tru=self, **kwargs)
+        return TruChain(tru=self, *args, **kwargs)
 
     def __init__(self):
         """
         TruLens instrumentation, logging, and feedback functions for chains.
         Creates a local database 'default.sqlite' in current working directory.
         """
 
         if hasattr(self, "db"):
             # Already initialized by SingletonByName mechanism.
             return
 
-        self.db = LocalSQLite(filename=Path(Tru.DEFAULT_DATABASE_FILE))
+        self.db = LocalSQLite(Tru.DEFAULT_DATABASE_FILE)
 
     def reset_database(self):
         """
         Reset the database. Clears all tables.
         """
 
         self.db.reset_database()
 
-    def add_record(self, record: Optional[Record] = None, **kwargs):
+    def add_record(
+        self,
+        prompt: str,
+        response: str,
+        record_json: JSON,
+        tags: Optional[str] = "",
+        ts: Optional[int] = None,
+        total_tokens: Optional[int] = None,
+        total_cost: Optional[float] = None,
+    ):
         """
         Add a record to the database.
 
         Parameters:
-        
-        - record: Record
 
-        - **kwargs: Record fields.
-            
+            prompt (str): Chain input or "prompt".
+
+            response (str): Chain output or "response".
+
+            record_json (JSON): Record as produced by `TruChain.call_with_record`.
+
+            tags (str, optional): Additional metadata to include with the record.
+
+            ts (int, optional): Timestamp of record creation.
+
+            total_tokens (int, optional): The number of tokens generated in
+            producing the response.
+
+            total_cost (float, optional): The cost of producing the response.
+
         Returns:
-            RecordID: Unique record identifier.
+            str: Unique record identifier.
 
         """
+        ts = ts or datetime.now()
+        total_tokens = total_tokens or record_json['_cost']['total_tokens']
+        total_cost = total_cost or record_json['_cost']['total_cost']
+
+        chain_id = record_json['chain_id']
+
+        record_id = self.db.insert_record(
+            chain_id=chain_id,
+            input=prompt,
+            output=response,
+            record_json=record_json,
+            ts=ts,
+            tags=tags,
+            total_tokens=total_tokens,
+            total_cost=total_cost
+        )
 
-        if record is None:
-            record = Record(**kwargs)
-        else:
-            record.update(**kwargs)
-
-        return self.db.insert_record(record=record)
+        return record_id
 
     def run_feedback_functions(
         self,
-        record: Record,
-        feedback_functions: Sequence[Feedback],
-        chain: Optional[Model] = None,
+        record_json: JSON,
+        feedback_functions: Sequence['Feedback'],
+        chain_json: Optional[JSON] = None,
     ) -> Sequence[JSON]:
         """
         Run a collection of feedback functions and report their result.
 
         Parameters:
 
             record_json (JSON): The record on which to evaluate the feedback
@@ -106,106 +137,104 @@
 
             feedback_functions (Sequence[Feedback]): A collection of feedback
             functions to evaluate.
 
         Returns nothing.
         """
 
-        chain_id = record.chain_id
+        chain_id = record_json['chain_id']
 
-        if chain is None:
-            chain = self.db.get_chain(chain_id=chain_id)
-            if chain is None:
+        if chain_json is None:
+            chain_json = self.db.get_chain(chain_id=chain_id)
+            if chain_json is None:
                 raise RuntimeError(
                     "Chain {chain_id} not present in db. "
                     "Either add it with `tru.add_chain` or provide `chain_json` to `tru.run_feedback_functions`."
                 )
 
         else:
-            assert chain_id == chain.chain_id, "Record was produced by a different chain."
+            assert chain_id == chain_json[
+                'chain_id'], "Record was produced by a different chain."
 
-            if self.db.get_chain(chain_id=chain.chain_id) is None:
-                logger.warn(
+            if self.db.get_chain(chain_id=chain_json['chain_id']) is None:
+                logging.warn(
                     "Chain {chain_id} was not present in database. Adding it."
                 )
-                self.add_chain(chain=chain)
+                self.add_chain(chain_json=chain_json)
 
         evals = []
 
         for func in feedback_functions:
             evals.append(
                 TP().promise(
-                    lambda f: f.run_on_record(chain=chain, record=record), func
+                    lambda f: f.run_on_record(
+                        chain_json=chain_json, record_json=record_json
+                    ), func
                 )
             )
 
         evals = map(lambda p: p.get(), evals)
 
         return list(evals)
 
-    def add_chain(self, chain: Model) -> None:
+    def add_chain(
+        self, chain_json: JSON, chain_id: Optional[str] = None
+    ) -> None:
         """
         Add a chain to the database.        
         """
 
-        self.db.insert_chain(chain=chain)
+        self.db.insert_chain(chain_id=chain_id, chain_json=chain_json)
 
-    def add_feedback(self, result: FeedbackResult, **kwargs) -> None:
+    def add_feedback(self, result_json: JSON) -> None:
         """
         Add a single feedback result to the database.
         """
 
-        if result is None:
-            result = FeedbackResult(**kwargs)
-        else:
-            result.update(**kwargs)
+        if 'record_id' not in result_json or result_json['record_id'] is None:
+            raise RuntimeError(
+                "Result does not include record_id. "
+                "To log feedback, log the record first using `tru.add_record`."
+            )
 
-        self.db.insert_feedback(result=result)
+        self.db.insert_feedback(result_json=result_json, status=2)
 
-    def add_feedbacks(self, results: Iterable[FeedbackResult]) -> None:
+    def add_feedbacks(self, result_jsons: Iterable[JSON]) -> None:
         """
         Add multiple feedback results to the database.
         """
 
-        for result in results:
-            self.add_feedback(result=result)
+        for result_json in result_jsons:
+            self.add_feedback(result_json=result_json)
 
-    def get_chain(self, chain_id: Optional[str] = None) -> JSON:
+    def get_chain(self, chain_id: str) -> JSON:
         """
         Look up a chain from the database.
         """
 
-        # TODO: unserialize
         return self.db.get_chain(chain_id)
 
     def get_records_and_feedback(self, chain_ids: List[str]):
         """
         Get records, their feeback results, and feedback names from the database.
         """
 
         df, feedback_columns = self.db.get_records_and_feedback(chain_ids)
 
         return df, feedback_columns
 
-    def start_evaluator(self,
-                        restart=False,
-                        fork=False) -> Union[Process, Thread]:
+    def start_evaluator(self, fork=False) -> Union[Process, Thread]:
         """
         Start a deferred feedback function evaluation thread.
         """
 
         assert not fork, "Fork mode not yet implemented."
 
         if self.evaluator_proc is not None:
-            if restart:
-                self.stop_evaluator()
-            else:
-                raise RuntimeError(
-                    "Evaluator is already running in this process."
-                )
+            raise RuntimeError("Evaluator is already running in this process.")
 
         from trulens_eval.tru_feedback import Feedback
 
         if not fork:
             self.evaluator_stop = threading.Event()
 
         def runloop():
@@ -249,69 +278,41 @@
         elif isinstance(self.evaluator_proc, Thread):
             self.evaluator_stop.set()
             self.evaluator_proc.join()
             self.evaluator_stop = None
 
         self.evaluator_proc = None
 
-    def stop_dashboard(self, force: bool = False) -> None:
+    def stop_dashboard(self) -> None:
         """Stop existing dashboard if running.
 
         Raises:
             ValueError: Dashboard is already running.
         """
         if Tru.dashboard_proc is None:
-            if not force:
-                raise ValueError(
-                    "Dashboard not running in this workspace. "
-                    "You may be able to shut other instances by setting the `force` flag."
-                )
-
-            else:
-                print("Force stopping dashboard ...")
-                import psutil
-                import pwd
-                import os
-                username = pwd.getpwuid(os.getuid())[0]
-                for p in psutil.process_iter():
-                    try:
-                        cmd = " ".join(p.cmdline())
-                        if "streamlit" in cmd and "Leaderboard.py" in cmd and p.username(
-                        ) == username:
-                            print(f"killing {p}")
-                            p.kill()
-                    except Exception as e:
-                        continue
+            raise ValueError("Dashboard not running.")
 
-        else:
-            Tru.dashboard_proc.kill()
-            Tru.dashboard_proc = None
+        Tru.dashboard_proc.kill()
+        Tru.dashboard_proc = None
 
-    def run_dashboard(
-        self, force: bool, _dev: Optional[Path] = None
-    ) -> Process:
+    def run_dashboard(self, _dev: bool = False) -> Process:
         """ Runs a streamlit dashboard to view logged results and chains
 
         Raises:
             ValueError: Dashboard is already running.
 
         Returns:
             Process: Process containing streamlit dashboard.
         """
 
-        if force:
-            self.stop_dashboard(force=force)
-
         if Tru.dashboard_proc is not None:
             raise ValueError(
                 "Dashboard already running. Run tru.stop_dashboard() to stop existing dashboard."
             )
 
-        print("Starting dashboard ...")
-
         # Create .streamlit directory if it doesn't exist
         streamlit_dir = os.path.join(os.getcwd(), '.streamlit')
         os.makedirs(streamlit_dir, exist_ok=True)
 
         # Create config.toml file
         config_path = os.path.join(streamlit_dir, 'config.toml')
         with open(config_path, 'w') as f:
@@ -329,17 +330,17 @@
 
         #run leaderboard with subprocess
         leaderboard_path = pkg_resources.resource_filename(
             'trulens_eval', 'Leaderboard.py'
         )
 
         env_opts = {}
-        if _dev is not None:
+        if _dev:
             env_opts['env'] = os.environ
-            env_opts['env']['PYTHONPATH'] = str(_dev)
+            env_opts['env']['PYTHONPATH'] = str(Path.cwd())
 
         proc = subprocess.Popen(
             ["streamlit", "run", "--server.headless=True", leaderboard_path],
             **env_opts
         )
 
         Tru.dashboard_proc = proc
```

## trulens_eval/tru_chain.py

```diff
@@ -70,118 +70,171 @@
 """
 
 from collections import defaultdict
 from datetime import datetime
 from inspect import BoundArguments
 from inspect import signature
 from inspect import stack
+import inspect
 import logging
 import os
-from pprint import PrettyPrinter
 import threading as th
 from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union
 
 import langchain
 from langchain.callbacks import get_openai_callback
 from langchain.chains.base import Chain
 from pydantic import BaseModel
 from pydantic import Field
 
-from trulens_eval.schema import FeedbackMode, Method, MethodIdent
-from trulens_eval.schema import LangChainModel
-from trulens_eval.schema import Record
-from trulens_eval.schema import RecordChainCall
-from trulens_eval.schema import RecordChainCallMethod
-from trulens_eval.schema import Cost
+from trulens_eval.tru_db import JSON, noserio
+from trulens_eval.tru_db import obj_id_of_obj
 from trulens_eval.tru_db import Query
 from trulens_eval.tru_db import TruDB
 from trulens_eval.tru_feedback import Feedback
-from trulens_eval.tru import Tru
-from trulens_eval.schema import FeedbackResult
-from trulens_eval.util import get_local_in_call_stack
-from trulens_eval.util import TP, JSONPath, jsonify, noserio
+from trulens_eval.util import TP
 
-logger = logging.getLogger(__name__)
+langchain.verbose = False
 
-pp = PrettyPrinter()
+# Addresses of chains or their contents. This is used to refer chains/parameters
+# even in cases where the live object is not in memory (i.e. on some remote
+# app).
+Path = Tuple[Union[str, int], ...]
+
+# Records of a chain run are dictionaries with these keys:
+#
+# - 'args': Dict[str, Any] -- chain __call__ input args.
+# - 'rets': Dict[str, Any] -- chain __call__ return dict for calls that succeed.
+# - 'error': str -- exception text if not successful.
+# - 'start_time': datetime
+# - 'end_time': datetime -- runtime info.
+# - 'pid': int -- process id for debugging multiprocessing.
+# - 'tid': int -- thread id for debuggin threading.
+# - 'chain_stack': List[Path] -- call stack of chain runs. Elements address
+#   chains.
 
 
-class TruChain(LangChainModel):
+class TruChain(Chain):
     """
     Wrap a langchain Chain to capture its configuration and evaluation steps. 
     """
 
-    class Config:
-        arbitrary_types_allowed = True
+    # The wrapped/instrumented chain.
+    chain: Chain = None
 
-    # See LangChainModel for serializable fields.
+    # Chain name/id. Will be a hash of chain definition/configuration if not provided.
+    chain_id: Optional[str] = None
+
+    # Flag of whether the chain is currently recording records. This is set
+    # automatically but is imperfect in threaded situations. The second check
+    # for recording is based on the call stack, see _call.
+    recording: Optional[bool] = Field(exclude=True)
 
     # Feedback functions to evaluate on each record.
-    feedbacks: Sequence[Feedback] = Field(exclude=True)
+    feedbacks: Optional[Sequence[Feedback]] = Field(exclude=True)
+
+    # Feedback evaluation mode.
+    # - "withchain" - Try to run feedback functions immediately and before chain
+    #   returns a record.
+    # - "withchainthread" - Try to run feedback functions in the same process as
+    #   the chain but after it produces a record.
+    # - "deferred" - Evaluate later via the process started by
+    #   `tru.start_deferred_feedback_evaluator`.
+    # - None - No evaluation will happen even if feedback functions are specified.
+
+    # NOTE: Custom feedback functions cannot be run deferred and will be run as
+    # if "withchainthread" was set.
+    feedback_mode: Optional[str] = "withchainthread"
 
     # Database interfaces for models/records/feedbacks.
-    # NOTE: Maybe move to schema.Model .
-    tru: Optional[Tru] = Field(exclude=True)
+    tru: Optional['Tru'] = Field(exclude=True)
 
     # Database interfaces for models/records/feedbacks.
-    # NOTE: Maybe mobe to schema.Model .
     db: Optional[TruDB] = Field(exclude=True)
 
     def __init__(
         self,
-        tru: Optional[Tru] = None,
+        chain: Chain,
+        chain_id: Optional[str] = None,
+        verbose: bool = False,
         feedbacks: Optional[Sequence[Feedback]] = None,
-        feedback_mode: FeedbackMode = FeedbackMode.WITH_CHAIN_THREAD,
-        **kwargs
+        feedback_mode: Optional[str] = "withchainthread",
+        tru: Optional['Tru'] = None
     ):
         """
         Wrap a chain for monitoring.
 
         Arguments:
         
         - chain: Chain -- the chain to wrap.
         - chain_id: Optional[str] -- chain name or id. If not given, the
           name is constructed from wrapped chain parameters.
         """
 
+        Chain.__init__(self, verbose=verbose)
+
+        self.chain = chain
+
+        self._instrument_object(obj=self.chain, query=Query().chain)
+        self.recording = False
+
+        chain_def = self.json
+
+        # Track chain id. This will produce a name if not provided.
+        self.chain_id = chain_id or obj_id_of_obj(obj=chain_def, prefix="chain")
+
         if feedbacks is not None and tru is None:
             raise ValueError("Feedback logging requires `tru` to be specified.")
-        feedbacks = feedbacks or []
+
+        self.feedbacks = feedbacks or []
+
+        assert feedback_mode in [
+            'withchain', 'withchainthread', 'deferred', None
+        ], "`feedback_mode` must be one of 'withchain', 'withchainthread', 'deferred', or None."
+        self.feedback_mode = feedback_mode
 
         if tru is not None:
-            kwargs['db'] = tru.db
+            self.db = tru.db
 
-            if feedback_mode == FeedbackMode.NONE:
-                logger.warn(
-                    "`tru` is specified but `feedback_mode` is FeedbackMode.NONE. "
+            if feedback_mode is None:
+                logging.warn(
+                    "`tru` is specified but `feedback_mode` is None. "
                     "No feedback evaluation and logging will occur."
                 )
         else:
-
-            if feedback_mode != FeedbackMode.NONE:
-                logger.warn(
-                    f"`feedback_mode` is {feedback_mode} but `tru` was not specified. Reverting to FeedbackMode.NONE ."
+            if feedback_mode is not None:
+                logging.warn(
+                    f"`feedback_mode` is {feedback_mode} but `tru` was not specified. Reverting to None."
                 )
-                feedback_mode = FeedbackMode.NONE
+                self.feedback_mode = None
+                feedback_mode = None
+                # Import here to avoid circular imports.
+                # from trulens_eval import Tru
+                # tru = Tru()
 
-        kwargs['tru'] = tru
-        kwargs['feedbacks'] = feedbacks
-        kwargs['feedback_mode'] = feedback_mode
+        self.tru = tru
 
-        super().__init__(**kwargs)
-
-        if tru is not None and feedback_mode != FeedbackMode.NONE:
-            logger.debug(
+        if tru is not None and feedback_mode is not None:
+            logging.debug(
                 "Inserting chain and feedback function definitions to db."
             )
-            self.db.insert_chain(chain=self)
+            self.db.insert_chain(chain_id=self.chain_id, chain_json=self.json)
             for f in self.feedbacks:
-                self.db.insert_feedback_definition(f)
+                self.db.insert_feedback_def(f.json)
+
+    @property
+    def json(self):
+        temp = TruDB.jsonify(self)  # not using self.dict()
+        # Need these to run feedback functions when they don't specify their
+        # inputs exactly.
+
+        temp['input_keys'] = self.input_keys
+        temp['output_keys'] = self.output_keys
 
-        self._instrument_object(obj=self.chain, query=Query.Query().chain)
+        return temp
 
     # Chain requirement
     @property
     def _chain_type(self):
         return "TruChain"
 
     # Chain requirement
@@ -190,166 +243,192 @@
         return self.chain.input_keys
 
     # Chain requirement
     @property
     def output_keys(self) -> List[str]:
         return self.chain.output_keys
 
-    # NOTE: Input signature compatible with langchain.chains.base.Chain.__call__
-    def call_with_record(self, inputs: Union[Dict[str, Any], Any], **kwargs):
+    def call_with_record(self, *args, **kwargs):
         """ Run the chain and also return a record metadata object.
 
+    
         Returns:
             Any: chain output
             dict: record metadata
         """
         # Mark us as recording calls. Should be sufficient for non-threaded
         # cases.
         self.recording = True
 
         # Wrapped calls will look this up by traversing the call stack. This
         # should work with threads.
-        record: Sequence[RecordChainCall] = []
+        record = defaultdict(list)
 
         ret = None
         error = None
 
         total_tokens = None
         total_cost = None
 
         try:
             # TODO: do this only if there is an openai model inside the chain:
             with get_openai_callback() as cb:
-                ret = self.chain.__call__(inputs=inputs, **kwargs)
+                ret = self.chain.__call__(*args, **kwargs)
                 total_tokens = cb.total_tokens
                 total_cost = cb.total_cost
 
         except BaseException as e:
             error = e
-            logger.error(f"Chain raised an exception: {e}")
+            logging.error(f"Chain raised an exception: {e}")
 
         self.recording = False
 
         assert len(record) > 0, "No information recorded in call."
 
-        ret_record_args = dict()
+        ret_record = dict()
+        chain_json = self.json
 
-        inputs = self.chain.prep_inputs(inputs)
+        for path, calls in record.items():
+            obj = TruDB._project(path=path, obj=chain_json)
 
-        # Figure out the content of the "inputs" arg that __call__ constructs
-        # for _call so we can lookup main input and output.
-        input_key = self.input_keys[0]
-        output_key = self.output_keys[0]
+            if obj is None:
+                logging.warn(f"Cannot locate {path} in chain.")
 
-        ret_record_args['main_input'] = inputs[input_key]
-        ret_record_args['main_output'] = ret[output_key]
-
-        ret_record_args['main_error'] = str(error)
-        ret_record_args['calls'] = record
-        ret_record_args['cost'] = Cost(n_tokens=total_tokens, cost=total_cost)
-        ret_record_args['chain_id'] = self.chain_id
+            ret_record = TruDB._set_in_json(
+                path=path, in_json=ret_record, val={"_call": calls}
+            )
 
-        ret_record = Record(**ret_record_args)
+        ret_record['_cost'] = dict(
+            total_tokens=total_tokens, total_cost=total_cost
+        )
+        ret_record['chain_id'] = self.chain_id
 
         if error is not None:
-            if self.feedback_mode == FeedbackMode.WITH_CHAIN:
-                self._handle_error(record=ret_record, error=error)
 
-            elif self.feedback_mode in [FeedbackMode.DEFERRED,
-                                        FeedbackMode.WITH_CHAIN_THREAD]:
+            if self.feedback_mode == "withchain":
+                self._handle_error(record_json=ret_record, error=error)
+
+            elif self.feedback_mode in ["deferred", "withchainthread"]:
                 TP().runlater(
-                    self._handle_error, record=ret_record, error=error
+                    self._handle_error, record_json=ret_record, error=error
                 )
 
             raise error
 
-        if self.feedback_mode == FeedbackMode.WITH_CHAIN:
-            self._handle_record(record=ret_record)
+        if self.feedback_mode == "withchain":
+            self._handle_record(record_json=ret_record)
 
-        elif self.feedback_mode in [FeedbackMode.DEFERRED,
-                                    FeedbackMode.WITH_CHAIN_THREAD]:
-            TP().runlater(self._handle_record, record=ret_record)
+        elif self.feedback_mode in ["deferred", "withchainthread"]:
+            TP().runlater(self._handle_record, record_json=ret_record)
 
         return ret, ret_record
 
     # langchain.chains.base.py:Chain
     def __call__(self, *args, **kwargs) -> Dict[str, Any]:
         """
         Wrapped call to self.chain.__call__ with instrumentation. If you need to
         get the record, use `call_with_record` instead. 
         """
 
-        ret, _ = self.call_with_record(*args, **kwargs)
+        ret, record = self.call_with_record(*args, **kwargs)
 
         return ret
 
-    def _handle_record(self, record: Record):
+    def _handle_record(self, record_json: JSON):
         """
         Write out record-related info to database if set.
         """
 
         if self.tru is None or self.feedback_mode is None:
             return
 
-        record_id = self.tru.add_record(record=record)
+        main_input = record_json['chain']['_call']['args']['inputs'][
+            self.input_keys[0]]
+        main_output = record_json['chain']['_call']['rets'][self.output_keys[0]]
+
+        record_id = self.tru.add_record(
+            prompt=main_input,
+            response=main_output,
+            record_json=record_json,
+            tags='dev',  # TODO: generalize
+            total_tokens=record_json['_cost']['total_tokens'],
+            total_cost=record_json['_cost']['total_cost']
+        )
 
         if len(self.feedbacks) == 0:
             return
 
         # Add empty (to run) feedback to db.
-        if self.feedback_mode == FeedbackMode.DEFERRED:
+        if self.feedback_mode == "deferred":
             for f in self.feedbacks:
-                self.db.insert_feedback(
-                    FeedbackResult(
-                        name=f.name,
-                        chain_id=self.chain_id,
-                        record_id=record_id,
-                        feedback_definition_id=f.feedback_definition_id
-                    )
-                )
+                feedback_id = f.feedback_id
+                self.db.insert_feedback(record_id, feedback_id)
 
-        elif self.feedback_mode in [FeedbackMode.WITH_CHAIN,
-                                    FeedbackMode.WITH_CHAIN_THREAD]:
+        elif self.feedback_mode in ["withchain", "withchainthread"]:
 
             results = self.tru.run_feedback_functions(
-                record=record, feedback_functions=self.feedbacks, chain=self
+                record_json=record_json,
+                feedback_functions=self.feedbacks,
+                chain_json=self.json
             )
 
-            for result in results:
-                self.tru.add_feedback(result)
+            for result_json in results:
+                self.tru.add_feedback(result_json)
 
-    def _handle_error(self, record: Record, error: Exception):
+    def _handle_error(self, record, error):
         if self.db is None:
             return
 
+        pass
+
     # Chain requirement
     # TODO(piotrm): figure out whether the combination of _call and __call__ is working right.
     def _call(self, *args, **kwargs) -> Any:
         return self.chain._call(*args, **kwargs)
 
+    def _get_local_in_call_stack(
+        self, key: str, func: Callable, offset: int = 1
+    ) -> Optional[Any]:
+        """
+        Get the value of the local variable named `key` in the stack at the
+        nearest frame executing `func`. Returns None if `func` is not in call
+        stack. Raises RuntimeError if `func` is in call stack but does not have
+        `key` in its locals.
+        """
+
+        for fi in stack()[offset + 1:]:  # + 1 to skip this method itself
+            if id(fi.frame.f_code) == id(func.__code__):
+                locs = fi.frame.f_locals
+                if key in locs:
+                    return locs[key]
+                else:
+                    raise RuntimeError(f"No local named {key} in {func} found.")
+
+        return None
+
     def _instrument_dict(self, cls, obj: Any):
         """
         Replacement for langchain's dict method to one that does not fail under
         non-serialization situations.
         """
 
         if hasattr(obj, "memory"):
             if obj.memory is not None:
-                # logger.warn(
+                # logging.warn(
                 #     f"Will not be able to serialize object of type {cls} because it has memory."
                 # )
                 pass
 
         def safe_dict(s, json: bool = True, **kwargs: Any) -> Dict:
             """
             Return dictionary representation `s`. If `json` is set, will make
             sure output can be serialized.
             """
 
-            # if s.memory is not None:
+            #if s.memory is not None:
             # continue anyway
             # raise ValueError("Saving of memory is not yet supported.")
 
             sup = super(cls, s)
             if hasattr(sup, "dict"):
                 _dict = super(cls, s).dict(**kwargs)
             else:
@@ -389,25 +468,27 @@
 
         safe_type._instrumented = prop
         new_prop = property(fget=safe_type)
 
         return new_prop
 
     def _instrument_tracked_method(
-        self, query: Query, func: Callable, method_name: str, cls: type,
-        obj: object
+        self, query: Query, func: Callable, method_name: str, class_name: str,
+        module_name: str
     ):
         """
         Instrument a method to capture its inputs/outputs/errors.
         """
 
-        logger.debug(f"instrumenting {method_name}={func} in {query}")
+        if self.verbose:
+            print(f"instrumenting {method_name}={func} in {query._path}")
 
         if hasattr(func, "_instrumented"):
-            logger.debug(f"{func} is already instrumented")
+            if self.verbose:
+                print(f"{func} is already instrumented")
 
             # Already instrumented. Note that this may happen under expected
             # operation when the same chain is used multiple times as part of a
             # larger chain.
 
             # TODO: How to consistently address calls to chains that appear more
             # than once in the wrapped chain or are called more than once.
@@ -420,174 +501,169 @@
             # any recording. This check is not perfect in threaded situations so
             # the next call stack-based lookup handles the rarer cases.
 
             # NOTE(piotrm): Disabling this for now as it is not thread safe.
             #if not self.recording:
             #    return func(*args, **kwargs)
 
-            logger.debug(f"Calling instrumented method {func} on {query}")
-
-            def find_call_with_record(f):
-                return id(f) == id(TruChain.call_with_record.__code__)
-
             # Look up whether TruChain._call was called earlier in the stack and
             # "record" variable was defined there. Will use that for recording
             # the wrapped call.
-            record = get_local_in_call_stack(
-                key="record", func=find_call_with_record
+            record = self._get_local_in_call_stack(
+                key="record", func=TruChain.call_with_record
             )
 
             if record is None:
-                logger.debug("No record found, not recording.")
                 return func(*args, **kwargs)
 
             # Otherwise keep track of inputs and outputs (or exception).
 
             error = None
-            rets = None
+            ret = None
 
             start_time = datetime.now()
 
-            def find_instrumented(f):
-                return id(f) == id(wrapper.__code__)
-                # return hasattr(f, "_instrumented")
-
-            # If a wrapped method was called in this call stack, get the prior
-            # calls from this variable. Otherwise create a new chain stack.
-            chain_stack = get_local_in_call_stack(
-                key="chain_stack", func=find_instrumented, offset=1
+            chain_stack = self._get_local_in_call_stack(
+                key="chain_stack", func=wrapper, offset=1
             ) or ()
-            frame_ident = RecordChainCallMethod(
-                path=query, method=MethodIdent.of_method(func, obj=obj)
+            frame_ident = dict(
+                path=tuple(query._path),
+                method_name=method_name,
+                class_name=class_name,
+                module_name=module_name,
             )
             chain_stack = chain_stack + (frame_ident,)
 
             try:
                 # Using sig bind here so we can produce a list of key-value
                 # pairs even if positional arguments were provided.
                 bindings: BoundArguments = sig.bind(*args, **kwargs)
-                rets = func(*bindings.args, **bindings.kwargs)
+                ret = func(*bindings.args, **bindings.kwargs)
 
             except BaseException as e:
                 error = e
-                error_str = str(e)
 
             end_time = datetime.now()
 
             # Don't include self in the recorded arguments.
             nonself = {
-                k: jsonify(v)
+                k: TruDB.jsonify(v)
                 for k, v in bindings.arguments.items()
                 if k != "self"
             }
 
             row_args = dict(
                 args=nonself,
-                start_time=start_time,
-                end_time=end_time,
+                start_time=str(start_time),
+                end_time=str(end_time),
                 pid=os.getpid(),
                 tid=th.get_native_id(),
-                chain_stack=chain_stack,
-                rets=rets,
-                error=error_str if error is not None else None
+                chain_stack=chain_stack
             )
 
-            row = RecordChainCall(**row_args)
-            record.append(row)
+            if error is not None:
+                row_args['error'] = error
+            else:
+                row_args['rets'] = ret
+
+            # If there already is a call recorded at the same path, turn the
+            # calls into a list.
+            if query._path in record:
+                existing_call = record[query._path]
+                if isinstance(existing_call, dict):
+                    record[query._path] = [existing_call, row_args]
+                else:
+                    record[query._path].append(row_args)
+            else:
+                # Otherwise record just the one call not inside a list.
+                record[query._path] = row_args
 
             if error is not None:
                 raise error
 
-            return rets
+            return ret
 
         wrapper._instrumented = func
 
         # Put the address of the instrumented chain in the wrapper so that we
         # don't pollute its list of fields. Note that this address may be
         # deceptive if the same subchain appears multiple times in the wrapped
         # chain.
         wrapper._query = query
 
         return wrapper
 
     def _instrument_object(self, obj, query: Query):
 
+        # cls = inspect.getattr_static(obj, "__class__").__get__()
         cls = type(obj)
 
-        logger.debug(
-            f"instrumenting {query} {cls.__name__}, bases={cls.__bases__}"
-        )
+        if self.verbose:
+            pass
+            #print(
+            #    f"instrumenting {query._path} {cls.__name__}, bases={cls.__bases__}"
+            #)
 
         # NOTE: We cannot instrument chain directly and have to instead
         # instrument its class. The pydantic BaseModel does not allow instance
         # attributes that are not fields:
         # https://github.com/pydantic/pydantic/blob/11079e7e9c458c610860a5776dc398a4764d538d/pydantic/main.py#LL370C13-L370C13
         # .
 
-        # Instrument only methods with these names and of these classes.
-        methods_to_instrument = {
-            "_call": lambda o: isinstance(o, langchain.chains.base.Chain),
-            "get_relevant_documents": lambda o: True,  # VectorStoreRetriever
-            "__call__":
-                lambda o: isinstance(o, Feedback)  # Feedback
-        }
+        methods_to_instrument = {"_call", "get_relevant_documents"}
 
         for base in [cls] + cls.mro():
             # All of mro() may need instrumentation here if some subchains call
             # superchains, and we want to capture the intermediate steps.
 
             if not base.__module__.startswith(
                     "langchain.") and not base.__module__.startswith("trulens"):
                 continue
 
             for method_name in methods_to_instrument:
                 if hasattr(base, method_name):
-                    check_class = methods_to_instrument[method_name]
-                    if not check_class(obj):
-                        continue
-
                     original_fun = getattr(base, method_name)
 
-                    logger.debug(f"instrumenting {base}.{method_name}")
+                    if self.verbose:
+                        print(f"instrumenting {base}.{method_name}")
 
                     setattr(
                         base, method_name,
                         self._instrument_tracked_method(
                             query=query,
                             func=original_fun,
                             method_name=method_name,
-                            cls=base,
-                            obj=obj
+                            class_name=base.__name__,
+                            module_name=base.__module__
                         )
                     )
 
-            # Instrument special langchain methods that may cause serialization
-            # failures.
             if hasattr(base, "_chain_type"):
-                logger.debug(f"instrumenting {base}._chain_type")
+                if self.verbose:
+                    print(f"instrumenting {base}._chain_type")
 
                 prop = getattr(base, "_chain_type")
                 setattr(
                     base, "_chain_type",
                     self._instrument_type_method(obj=obj, prop=prop)
                 )
 
             if hasattr(base, "_prompt_type"):
-                logger.debug(f"instrumenting {base}._chain_prompt")
+                if self.verbose:
+                    print(f"instrumenting {base}._chain_prompt")
 
                 prop = getattr(base, "_prompt_type")
                 setattr(
                     base, "_prompt_type",
                     self._instrument_type_method(obj=obj, prop=prop)
                 )
 
-            # Instrument a pydantic.BaseModel method that may cause
-            # serialization failures.
             if hasattr(base, "dict"):
-                logger.debug(f"instrumenting {base}.dict")
+                if self.verbose:
+                    print(f"instrumenting {base}.dict")
 
                 setattr(base, "dict", self._instrument_dict(cls=base, obj=obj))
 
         # Not using chain.dict() here as that recursively converts subchains to
         # dicts but we want to traverse the instantiations here.
         if isinstance(obj, BaseModel):
 
@@ -600,15 +676,15 @@
 
                 elif type(v).__module__.startswith("langchain.") or type(
                         v).__module__.startswith("trulens"):
                     self._instrument_object(obj=v, query=query[k])
 
                 elif isinstance(v, Sequence):
                     for i, sv in enumerate(v):
-                        if isinstance(sv, langchain.chains.base.Chain):
+                        if isinstance(sv, Chain):
                             self._instrument_object(obj=sv, query=query[k][i])
 
                 # TODO: check if we want to instrument anything not accessible through __fields__ .
         else:
-            logger.debug(
+            logging.debug(
                 f"Do not know how to instrument object {str(obj)[:32]} of type {cls}."
             )
```

## trulens_eval/tru_db.py

```diff
@@ -1,97 +1,136 @@
 import abc
-from datetime import datetime
 import json
 import logging
 from pathlib import Path
-from pprint import PrettyPrinter
 import sqlite3
-from typing import (Dict, Iterable, List, Optional, Sequence, Tuple)
-
-import numpy as np
+from typing import (
+    Any, Callable, Dict, Iterable, List, Optional, Sequence, Set, Tuple, Union
+)
 
 from frozendict import frozendict
 from merkle_json import MerkleJson
 import pandas as pd
+import pydantic
+from tinydb import Query as TinyQuery
+from tinydb.queries import QueryInstance as TinyQueryInstance
 
-from trulens_eval.schema import ChainID
-from trulens_eval.schema import FeedbackDefinition
-from trulens_eval.schema import FeedbackDefinitionID
-from trulens_eval.schema import FeedbackResult
-from trulens_eval.schema import FeedbackResultID
-from trulens_eval.schema import JSONPath
-from trulens_eval.schema import Model
-from trulens_eval.schema import Record
-from trulens_eval.schema import RecordChainCall
-from trulens_eval.schema import RecordID
-from trulens_eval.schema import Cost
-from trulens_eval.schema import FeedbackResultStatus
-from trulens_eval.util import GetItemOrAttribute
-from trulens_eval.util import all_queries
-from trulens_eval.util import JSON
-from trulens_eval.util import json_str_of_obj
-from trulens_eval.util import JSONPath
-from trulens_eval.util import SerialModel
 from trulens_eval.util import UNCIODE_YIELD
 from trulens_eval.util import UNICODE_CHECK
 
 mj = MerkleJson()
 NoneType = type(None)
 
-pp = PrettyPrinter()
+JSON_BASES = (str, int, float, NoneType)
+JSON_BASES_T = Union[str, int, float, NoneType]
+# JSON = (List, Dict) + JSON_BASES
+# JSON_T = Union[JSON_BASES_T, List, Dict]
+JSON = Dict
+
+
+def is_empty(obj):
+    try:
+        return len(obj) == 0
+    except Exception:
+        return False
+
+
+def is_noserio(obj):
+    """
+    Determines whether the given json object represents some non-serializable
+    object. See `noserio`.
+    """
+    return isinstance(obj, dict) and "_NON_SERIALIZED_OBJECT" in obj
+
+
+def noserio(obj, **extra: Dict) -> dict:
+    """
+    Create a json structure to represent a non-serializable object. Any
+    additional keyword arguments are included.
+    """
+
+    inner = {
+        "id": id(obj),
+        "class": obj.__class__.__name__,
+        "module": obj.__class__.__module__,
+        "bases": list(map(lambda b: b.__name__, obj.__class__.__bases__))
+    }
+    inner.update(extra)
+
+    return {'_NON_SERIALIZED_OBJECT': inner}
+
+
+def obj_id_of_obj(obj: dict, prefix="obj"):
+    """
+    Create an id from a json-able structure/definition. Should produce the same
+    name if definition stays the same.
+    """
+
+    return f"{prefix}_hash_{mj.hash(obj)}"
+
+
+def json_str_of_obj(obj: Any) -> str:
+    """
+    Encode the given json object as a string.
+    """
+    return json.dumps(obj, default=json_default)
+
+
+def json_default(obj: Any) -> str:
+    """
+    Produce a representation of an object which cannot be json-serialized.
+    """
+
+    if isinstance(obj, pydantic.BaseModel):
+        try:
+            return json.dumps(obj.dict())
+        except Exception as e:
+            return noserio(obj, exception=e)
 
-logger = logging.getLogger(__name__)
+    # Intentionally not including much in this indicator to make sure the model
+    # hashing procedure does not get randomized due to something here.
 
+    return noserio(obj)
 
-class Query:
 
-    # Typing for type hints.
-    Query = JSONPath
+# Typing for type hints.
+Query = TinyQuery
 
-    # Instance for constructing queries for record json like `Record.chain.llm`.
-    Record = Query().__record__
+# Instance for constructing queries for record json like `Record.chain.llm`.
+Record = Query()._record
 
-    # Instance for constructing queries for chain json.
-    Chain = Query().__chain__
+# Instance for constructing queries for chain json.
+Chain = Query()._chain
 
-    # A Chain's main input and main output.
-    # TODO: Chain input/output generalization.
-    RecordInput = Record.main_input
-    RecordOutput = Record.main_output
+# Type of conditions, constructed from query/record like `Record.chain != None`.
+Condition = TinyQueryInstance
 
 
-def get_calls(record: Record) -> Iterable[RecordChainCall]:
+def get_calls(record_json: JSON) -> Iterable[JSON]:
     """
     Iterate over the call parts of the record.
     """
 
-    for q in all_queries(record):
-        print("consider query", q)
-        if len(q.path) > 0 and q.path[-1] == GetItemOrAttribute(
-                item_or_attribute="_call"):
+    for q in TruDB.all_queries(record_json):
+        if q._path[-1] == "_call":
             yield q
 
 
-def get_calls_by_stack(
-    record: Record
-) -> Dict[Tuple[str, ...], RecordChainCall]:
+def get_calls_by_stack(record_json: JSON) -> Dict[Tuple[str, ...], JSON]:
     """
     Get a dictionary mapping chain call stack to the call information.
     """
 
     def frozen_frame(frame):
         frame['path'] = tuple(frame['path'])
         return frozendict(frame)
 
     ret = dict()
-
-    for c in get_calls(record):
-        print("call", c)
-
-        obj = TruDB.project(c, record_json=record, chain_json=None)
+    for c in get_calls(record_json):
+        obj = TruDB.project(c, record_json=record_json, chain_json=None)
         if isinstance(obj, Sequence):
             for o in obj:
                 call_stack = tuple(map(frozen_frame, o['chain_stack']))
                 if call_stack not in ret:
                     ret[call_stack] = []
                 ret[call_stack].append(o)
         else:
@@ -99,442 +138,785 @@
             if call_stack not in ret:
                 ret[call_stack] = []
             ret[call_stack].append(obj)
 
     return ret
 
 
-class TruDB(SerialModel, abc.ABC):
+def query_of_path(path: List[Union[str, int]]) -> Query:
+    """
+    Convert the given path to a query object.
+    """
+
+    if path[0] == "_record":
+        ret = Record
+        path = path[1:]
+    elif path[0] == "_chain":
+        ret = Chain
+        path = path[1:]
+    else:
+        ret = Query()
+
+    for attr in path:
+        ret = getattr(ret, attr)
+
+    return ret
+
+
+def path_of_query(query: Query) -> List[Union[str, int]]:
+    return query._path
+
+
+class TruDB(abc.ABC):
+
+    # Use TinyDB queries for looking up parts of records/models and/or filtering
+    # on those parts.
 
     @abc.abstractmethod
     def reset_database(self):
+        """Delete all data."""
+
+        raise NotImplementedError()
+
+    @abc.abstractmethod
+    def select(
+        self,
+        *query: Tuple[Query],
+        where: Optional[Condition] = None
+    ) -> pd.DataFrame:
         """
-        Delete all data.
+        Select `query` fields from the records database, filtering documents
+        that do not match the `where` condition.
         """
 
         raise NotImplementedError()
 
     @abc.abstractmethod
     def insert_record(
-        self,
-        record: Record,
-    ) -> RecordID:
+        self, chain_id: str, input: str, output: str, record_json: JSON,
+        ts: int, tags: str, total_tokens: int, total_cost: float
+    ) -> int:
         """
         Insert a new `record` into db, indicating its `model` as well. Return
         record id.
-
-        Args:
-        - record: Record
         """
 
         raise NotImplementedError()
 
     @abc.abstractmethod
-    def insert_chain(self, chain: Model) -> ChainID:
+    def insert_chain(
+        self, chain_json: JSON, chain_id: Optional[str] = None
+    ) -> str:
         """
-        Insert a new `chain` into db under the given `chain_id`. 
-
-        Args:
-        - chain: Chain - Chain definition. 
+        Insert a new `chain` into db under the given `chain_id`. If name not
+        provided, generate a name from chain definition. Return the name.
         """
 
         raise NotImplementedError()
 
     @abc.abstractmethod
-    def insert_feedback_definition(
-        self, feedback_definition: FeedbackDefinition
-    ) -> FeedbackDefinitionID:
-        """
-        Insert a feedback definition into the db.
-        """
-
+    def insert_feedback_def(self, feedback_json: dict):
         raise NotImplementedError()
 
     @abc.abstractmethod
     def insert_feedback(
         self,
-        feedback_result: FeedbackResult,
-    ) -> FeedbackResultID:
-        """
-        Insert a feedback record into the db.
-
-        Args:
-
-        - feedback_result: FeedbackResult
-        """
-
+        record_id: str,
+        feedback_id: str,
+        last_ts: Optional[int] = None,  # "last timestamp"
+        status: Optional[int] = None,
+        result_json: Optional[JSON] = None,
+        total_cost: Optional[float] = None,
+        total_tokens: Optional[int] = None,
+    ) -> str:
         raise NotImplementedError()
 
     @abc.abstractmethod
     def get_records_and_feedback(
         self, chain_ids: List[str]
-    ) -> Tuple[pd.DataFrame, Sequence[str]]:
+    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
+        raise NotImplementedError()
+
+    @staticmethod
+    def jsonify(obj: Any, dicted=None) -> JSON:
         """
-        Get the records logged for the given set of `chain_ids` (otherwise all)
-        alongside the names of the feedback function columns listed the
-        dataframe.
+        Convert the given object into types that can be serialized in json.
         """
-        raise NotImplementedError()
 
+        dicted = dicted or dict()
 
-class LocalSQLite(TruDB):
-    filename: Path
+        if isinstance(obj, JSON_BASES):
+            return obj
 
-    TABLE_RECORDS = "records"
-    TABLE_FEEDBACKS = "feedbacks"
-    TABLE_FEEDBACK_DEFS = "feedback_defs"
-    TABLE_CHAINS = "chains"
+        if id(obj) in dicted:
+            return {'_CIRCULAR_REFERENCE': id(obj)}
 
-    TYPE_TIMESTAMP = "FLOAT"
-    TYPE_ENUM = "TEXT"
+        new_dicted = {k: v for k, v in dicted.items()}
 
-    TABLES = [TABLE_RECORDS, TABLE_FEEDBACKS, TABLE_FEEDBACK_DEFS, TABLE_CHAINS]
+        if isinstance(obj, Dict):
+            temp = {}
+            new_dicted[id(obj)] = temp
+            temp.update(
+                {
+                    k: TruDB.jsonify(v, dicted=new_dicted)
+                    for k, v in obj.items()
+                }
+            )
+            return temp
 
-    def __init__(self, filename: Path):
+        elif isinstance(obj, Sequence):
+            temp = []
+            new_dicted[id(obj)] = temp
+            for x in (TruDB.jsonify(v, dicted=new_dicted) for v in obj):
+                temp.append(x)
+            return temp
+
+        elif isinstance(obj, Set):
+            temp = []
+            new_dicted[id(obj)] = temp
+            for x in (TruDB.jsonify(v, dicted=new_dicted) for v in obj):
+                temp.append(x)
+            return temp
+
+        elif isinstance(obj, pydantic.BaseModel):
+            temp = {}
+            new_dicted[id(obj)] = temp
+            temp.update(
+                {
+                    k: TruDB.jsonify(getattr(obj, k), dicted=new_dicted)
+                    for k in obj.__fields__
+                }
+            )
+            return temp
+
+        else:
+            logging.debug(
+                f"Don't know how to jsonify an object '{str(obj)[0:32]}' of type '{type(obj)}'."
+            )
+            return noserio(obj)
+
+    @staticmethod
+    def leaf_queries(obj_json: JSON, query: Query = None) -> Iterable[Query]:
+        """
+        Get all queries for the given object that select all of its leaf values.
         """
-        Database locally hosted using SQLite.
 
-        Args
-        
-        - filename: Optional[Path] -- location of sqlite database dump
-          file. It will be created if it does not exist.
+        query = query or Record
+
+        if isinstance(obj_json, (str, int, float, NoneType)):
+            yield query
 
+        elif isinstance(obj_json, Dict):
+            for k, v in obj_json.items():
+                sub_query = query[k]
+                for res in TruDB.leaf_queries(obj_json[k], sub_query):
+                    yield res
+
+        elif isinstance(obj_json, Sequence):
+            for i, v in enumerate(obj_json):
+                sub_query = query[i]
+                for res in TruDB.leaf_queries(obj_json[i], sub_query):
+                    yield res
+
+        else:
+            yield query
+
+    @staticmethod
+    def all_queries(obj: Any, query: Query = None) -> Iterable[Query]:
+        """
+        Get all queries for the given object.
         """
-        super().__init__(filename=filename)
 
-        self._build_tables()
+        query = query or Record
 
-    def __str__(self) -> str:
-        return f"SQLite({self.filename})"
+        if isinstance(obj, (str, int, float, NoneType)):
+            yield query
 
-    # TruDB requirement
-    def reset_database(self) -> None:
-        self._drop_tables()
-        self._build_tables()
+        elif isinstance(obj, pydantic.BaseModel):
+            yield query
 
-    def _clear_tables(self) -> None:
-        conn, c = self._connect()
+            for k in obj.__fields__:
+                v = getattr(obj, k)
+                sub_query = query[k]
+                for res in TruDB.all_queries(v, sub_query):
+                    yield res
 
-        for table in self.TABLES:
-            c.execute(f'''DELETE FROM {table}''')
+        elif isinstance(obj, Dict):
+            yield query
 
-        self._close(conn)
+            for k, v in obj.items():
+                sub_query = query[k]
+                for res in TruDB.all_queries(obj[k], sub_query):
+                    yield res
 
-    def _drop_tables(self) -> None:
-        conn, c = self._connect()
+        elif isinstance(obj, Sequence):
+            yield query
+
+            for i, v in enumerate(obj):
+                sub_query = query[i]
+                for res in TruDB.all_queries(obj[i], sub_query):
+                    yield res
+
+        else:
+            yield query
+
+    @staticmethod
+    def all_objects(obj: Any,
+                    query: Query = None) -> Iterable[Tuple[Query, Any]]:
+        """
+        Get all queries for the given object.
+        """
+
+        query = query or Record
 
-        for table in self.TABLES:
-            c.execute(f'''DROP TABLE IF EXISTS {table}''')
+        if isinstance(obj, (str, int, float, NoneType)):
+            yield (query, obj)
+
+        elif isinstance(obj, pydantic.BaseModel):
+            yield (query, obj)
+
+            for k in obj.__fields__:
+                v = getattr(obj, k)
+                sub_query = query[k]
+                for res in TruDB.all_objects(v, sub_query):
+                    yield res
+
+        elif isinstance(obj, Dict):
+            yield (query, obj)
+
+            for k, v in obj.items():
+                sub_query = query[k]
+                for res in TruDB.all_objects(obj[k], sub_query):
+                    yield res
+
+        elif isinstance(obj, Sequence):
+            yield (query, obj)
+
+            for i, v in enumerate(obj):
+                sub_query = query[i]
+                for res in TruDB.all_objects(obj[i], sub_query):
+                    yield res
+
+        else:
+            yield (query, obj)
+
+    @staticmethod
+    def leafs(obj: Any) -> Iterable[Tuple[str, Any]]:
+        for q in TruDB.leaf_queries(obj):
+            path_str = TruDB._query_str(q)
+            val = TruDB._project(q._path, obj)
+            yield (path_str, val)
+
+    @staticmethod
+    def matching_queries(obj: Any, match: Callable) -> Iterable[Query]:
+        for q in TruDB.all_queries(obj):
+            val = TruDB._project(q._path, obj)
+            if match(q, val):
+                yield q
+
+    @staticmethod
+    def matching_objects(obj: Any,
+                         match: Callable) -> Iterable[Tuple[Query, Any]]:
+        for q, val in TruDB.all_objects(obj):
+            if match(q, val):
+                yield (q, val)
+
+    @staticmethod
+    def _query_str(query: Query) -> str:
+
+        def render(ks):
+            if len(ks) == 0:
+                return ""
+
+            first = ks[0]
+            if len(ks) > 1:
+                rest = ks[1:]
+            else:
+                rest = ()
+
+            if isinstance(first, str):
+                return f".{first}{render(rest)}"
+            elif isinstance(first, int):
+                return f"[{first}]{render(rest)}"
+            else:
+                RuntimeError(
+                    f"Don't know how to render path element {first} of type {type(first)}."
+                )
 
+        return "Record" + render(query._path)
+
+    @staticmethod
+    def set_in_json(query: Query, in_json: JSON, val: JSON) -> JSON:
+        return TruDB._set_in_json(query._path, in_json=in_json, val=val)
+
+    @staticmethod
+    def _set_in_json(path, in_json: JSON, val: JSON) -> JSON:
+        if len(path) == 0:
+            if isinstance(in_json, Dict):
+                assert isinstance(val, Dict)
+                in_json = {k: v for k, v in in_json.items()}
+                in_json.update(val)
+                return in_json
+
+            assert in_json is None, f"Cannot set non-None json object: {in_json}"
+
+            return val
+
+        if len(path) == 1:
+            first = path[0]
+            rest = []
+        else:
+            first = path[0]
+            rest = path[1:]
+
+        if isinstance(first, str):
+            if isinstance(in_json, Dict):
+                in_json = {k: v for k, v in in_json.items()}
+                if not first in in_json:
+                    in_json[first] = None
+            elif in_json is None:
+                in_json = {first: None}
+            else:
+                raise RuntimeError(
+                    f"Do not know how to set path {path} in {in_json}."
+                )
+
+            in_json[first] = TruDB._set_in_json(
+                path=rest, in_json=in_json[first], val=val
+            )
+            return in_json
+
+        elif isinstance(first, int):
+            if isinstance(in_json, Sequence):
+                # In case it is some immutable sequence. Also copy.
+                in_json = list(in_json)
+            elif in_json is None:
+                in_json = []
+            else:
+                raise RuntimeError(
+                    f"Do not know how to set path {path} in {in_json}."
+                )
+
+            while len(in_json) <= first:
+                in_json.append(None)
+
+            in_json[first] = TruDB._set_in_json(
+                path=rest, in_json=in_json[first], val=val
+            )
+            return in_json
+
+        else:
+            raise RuntimeError(
+                f"Do not know how to set path {path} in {in_json}."
+            )
+
+    @staticmethod
+    def project(
+        query: Query,
+        record_json: JSON,
+        chain_json: JSON,
+        obj: Optional[JSON] = None
+    ):
+        path = query._path
+        if path[0] == "_record":
+            if len(path) == 1:
+                return record_json
+            return TruDB._project(path=path[1:], obj=record_json)
+        elif path[0] == "_chain":
+            if len(path) == 1:
+                return chain_json
+            return TruDB._project(path=path[1:], obj=chain_json)
+        else:
+            return TruDB._project(path=path, obj=obj)
+
+    @staticmethod
+    def _project(path: List, obj: Any):
+        if len(path) == 0:
+            return obj
+
+        first = path[0]
+        if len(path) > 1:
+            rest = path[1:]
+        else:
+            rest = ()
+
+        if isinstance(first, str):
+            if isinstance(obj, pydantic.BaseModel):
+                if not hasattr(obj, first):
+                    logging.warn(
+                        f"Cannot project {str(obj)[0:32]} with path {path} because {first} is not an attribute here."
+                    )
+                    return None
+                return TruDB._project(path=rest, obj=getattr(obj, first))
+
+            elif isinstance(obj, Dict):
+                if first not in obj:
+                    logging.warn(
+                        f"Cannot project {str(obj)[0:32]} with path {path} because {first} is not a key here."
+                    )
+                    return None
+                return TruDB._project(path=rest, obj=obj[first])
+
+            else:
+                logging.warn(
+                    f"Cannot project {str(obj)[0:32]} with path {path} because object is not a dict or model."
+                )
+                return None
+
+        elif isinstance(first, int):
+            if not isinstance(obj, Sequence) or first >= len(obj):
+                logging.warn(
+                    f"Cannot project {str(obj)[0:32]} with path {path}."
+                )
+                return None
+
+            return TruDB._project(path=rest, obj=obj[first])
+        else:
+            raise RuntimeError(
+                f"Don't know how to locate element with key of type {first}"
+            )
+
+
+class LocalSQLite(TruDB):
+
+    TABLE_RECORDS = "records"
+    TABLE_FEEDBACKS = "feedbacks"
+    TABLE_FEEDBACK_DEFS = "feedback_defs"
+    TABLE_CHAINS = "chains"
+
+    def __str__(self):
+        return f"SQLite({self.filename})"
+
+    def reset_database(self):
+        self._clear_tables()
+        self._build_tables()
+
+    def _clear_tables(self):
+        conn, c = self._connect()
+
+        # Create table if it does not exist
+        c.execute(f'''DELETE FROM {self.TABLE_RECORDS}''')
+        c.execute(f'''DELETE FROM {self.TABLE_FEEDBACKS}''')
+        c.execute(f'''DELETE FROM {self.TABLE_FEEDBACK_DEFS}''')
+        c.execute(f'''DELETE FROM {self.TABLE_CHAINS}''')
         self._close(conn)
 
     def _build_tables(self):
         conn, c = self._connect()
 
-        # Create table if it does not exist. Note that the record_json column
-        # also encodes inside it all other columns.
+        # Create table if it does not exist
         c.execute(
             f'''CREATE TABLE IF NOT EXISTS {self.TABLE_RECORDS} (
-                record_id TEXT NOT NULL PRIMARY KEY,
-                chain_id TEXT NOT NULL,
+                record_id TEXT,
+                chain_id TEXT,
                 input TEXT,
                 output TEXT,
-                record_json TEXT NOT NULL,
-                tags TEXT NOT NULL,
-                ts {self.TYPE_TIMESTAMP} NOT NULL,
-                cost_json TEXT NOT NULL
+                record_json TEXT,
+                tags TEXT,
+                ts INTEGER,
+                total_tokens INTEGER,
+                total_cost REAL,
+                PRIMARY KEY (record_id, chain_id)
             )'''
         )
         c.execute(
             f'''CREATE TABLE IF NOT EXISTS {self.TABLE_FEEDBACKS} (
-                feedback_result_id TEXT NOT NULL PRIMARY KEY,
-                record_id TEXT NOT NULL,
-                chain_id TEXT NOT NULL,
-                feedback_definition_id TEXT,
-                last_ts {self.TYPE_TIMESTAMP} NOT NULL,
-                status {self.TYPE_ENUM} NOT NULL,
-                error TEXT,
-                calls_json TEXT NOT NULL,
-                result FLOAT,
-                name TEXT NOT NULL,
-                cost_json TEXT NOT NULL
+                record_id TEXT,
+                feedback_id TEXT,
+                last_ts INTEGER,
+                status INTEGER,
+                result_json TEXT,
+                total_tokens INTEGER,
+                total_cost REAL,
+                PRIMARY KEY (record_id, feedback_id)
             )'''
         )
         c.execute(
             f'''CREATE TABLE IF NOT EXISTS {self.TABLE_FEEDBACK_DEFS} (
-                feedback_definition_id TEXT NOT NULL PRIMARY KEY,
-                feedback_json TEXT NOT NULL
+                feedback_id TEXT PRIMARY KEY,
+                feedback_json TEXT
             )'''
         )
         c.execute(
             f'''CREATE TABLE IF NOT EXISTS {self.TABLE_CHAINS} (
-                chain_id TEXT NOT NULL PRIMARY KEY,
-                chain_json TEXT NOT NULL
+                chain_id TEXT PRIMARY KEY,
+                chain_json TEXT
             )'''
         )
         self._close(conn)
 
-    def _connect(self) -> Tuple[sqlite3.Connection, sqlite3.Cursor]:
+    def __init__(self, filename: Optional[Path] = 'default.sqlite'):
+        self.filename = filename
+        self._build_tables()
+
+    def _connect(self):
         conn = sqlite3.connect(self.filename)
         c = conn.cursor()
         return conn, c
 
-    def _close(self, conn: sqlite3.Connection) -> None:
+    def _close(self, conn):
         conn.commit()
         conn.close()
 
     # TruDB requirement
     def insert_record(
-        self,
-        record: Record,
-    ) -> RecordID:
-        # NOTE: Oddness here in that the entire record is put into the
-        # record_json column while some parts of that records are also put in
-        # other columns. Might want to keep this so we can query on the columns
-        # within sqlite.
-
-        record_json_str = json_str_of_obj(record)
-        cost_json_str = json_str_of_obj(record.cost)
-        vals = (
-            record.record_id, record.chain_id, record.main_input,
-            record.main_output, record_json_str, record.tags, record.ts,
-            cost_json_str
-        )
+        self, chain_id: str, input: str, output: str, record_json: dict,
+        ts: int, tags: str, total_tokens: int, total_cost: float
+    ) -> int:
+        assert isinstance(
+            record_json, Dict
+        ), f"Attempting to add a record that is not a dict, is {type(record_json)} instead."
+
+        conn, c = self._connect()
 
-        self._insert_or_replace_vals(table=self.TABLE_RECORDS, vals=vals)
+        record_id = obj_id_of_obj(obj=record_json, prefix="record")
+        record_json['record_id'] = record_id
+        record_str = json_str_of_obj(record_json)
+
+        c.execute(
+            f"INSERT INTO {self.TABLE_RECORDS} VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)",
+            (
+                record_id, chain_id, input, output, record_str, tags, ts,
+                total_tokens, total_cost
+            )
+        )
+        self._close(conn)
 
         print(
-            f"{UNICODE_CHECK} record {record.record_id} from {record.chain_id} -> {self.filename}"
+            f"{UNICODE_CHECK} record {record_id} from {chain_id} -> {self.filename}"
         )
 
-        return record.record_id
+        return record_id
 
     # TruDB requirement
-    def insert_chain(self, chain: Model) -> ChainID:
-        chain_id = chain.chain_id
-        chain_str = chain.json()
+    def insert_chain(
+        self, chain_json: dict, chain_id: Optional[str] = None
+    ) -> str:
+        chain_id = chain_id or chain_json['chain_id'] or obj_id_of_obj(
+            obj=chain_json, prefix="chain"
+        )
+        chain_str = json_str_of_obj(chain_json)
 
-        vals = (chain_id, chain_str)
-        self._insert_or_replace_vals(table=self.TABLE_CHAINS, vals=vals)
+        conn, c = self._connect()
+        c.execute(
+            f"INSERT OR IGNORE INTO {self.TABLE_CHAINS} VALUES (?, ?)",
+            (chain_id, chain_str)
+        )
+        self._close(conn)
 
         print(f"{UNICODE_CHECK} chain {chain_id} -> {self.filename}")
 
         return chain_id
 
-    def insert_feedback_definition(
-        self, feedback: FeedbackDefinition
-    ) -> FeedbackDefinitionID:
+    def insert_feedback_def(self, feedback_json: dict):
         """
         Insert a feedback definition into the database.
         """
 
-        feedback_definition_id = feedback.feedback_definition_id
-        feedback_str = feedback.json()
-        vals = (feedback_definition_id, feedback_str)
-
-        self._insert_or_replace_vals(table=self.TABLE_FEEDBACK_DEFS, vals=vals)
+        feedback_id = feedback_json['feedback_id']
+        feedback_str = json_str_of_obj(feedback_json)
 
-        print(
-            f"{UNICODE_CHECK} feedback def. {feedback_definition_id} -> {self.filename}"
+        conn, c = self._connect()
+        c.execute(
+            f"INSERT OR REPLACE INTO {self.TABLE_FEEDBACK_DEFS} VALUES (?, ?)",
+            (feedback_id, feedback_str)
         )
+        self._close(conn)
 
-        return feedback_definition_id
-
-    def get_feedback_defs(
-        self, feedback_definition_id: Optional[str] = None
-    ) -> pd.DataFrame:
+        print(f"{UNICODE_CHECK} feedback def. {feedback_id} -> {self.filename}")
 
+    def get_feedback_defs(self, feedback_id: Optional[str] = None):
         clause = ""
         args = ()
-        if feedback_definition_id is not None:
+        if feedback_id is not None:
             clause = "WHERE feedback_id=?"
-            args = (feedback_definition_id,)
+            args = (feedback_id,)
 
         query = f"""
             SELECT
-                feedback_definition_id, feedback_json
+                feedback_id, feedback_json
             FROM {self.TABLE_FEEDBACK_DEFS}
             {clause}
         """
 
         conn, c = self._connect()
         c.execute(query, args)
         rows = c.fetchall()
         self._close(conn)
 
-        df = pd.DataFrame(
-            rows, columns=[description[0] for description in c.description]
-        )
+        from trulens_eval.tru_feedback import Feedback
 
-        return df
+        df_rows = []
 
-    def _insert_or_replace_vals(self, table, vals):
-        conn, c = self._connect()
-        c.execute(
-            f"""INSERT OR REPLACE INTO {table}
-                VALUES ({','.join('?' for _ in vals)})""", vals
-        )
-        self._close(conn)
+        for row in rows:
+            row = list(row)
+            row[1] = Feedback.of_json(json.loads(row[1]))
+            df_rows.append(row)
+
+        return pd.DataFrame(rows, columns=['feedback_id', 'feedback'])
 
     def insert_feedback(
-        self, feedback_result: FeedbackResult
-    ) -> FeedbackResultID:
+        self,
+        record_id: Optional[str] = None,
+        feedback_id: Optional[str] = None,
+        last_ts: Optional[int] = None,  # "last timestamp"
+        status: Optional[int] = None,
+        result_json: Optional[dict] = None,
+        total_cost: Optional[float] = None,
+        total_tokens: Optional[int] = None,
+    ):
         """
         Insert a record-feedback link to db or update an existing one.
         """
 
-        vals = (
-            feedback_result.feedback_result_id,
-            feedback_result.record_id,
-            feedback_result.chain_id,
-            feedback_result.feedback_definition_id,
-            feedback_result.last_ts.timestamp(),
-            feedback_result.status.value,
-            feedback_result.error,
-            json_str_of_obj(dict(calls=feedback_result.calls)
-                           ),  # extra dict is needed json's root must be a dict
-            feedback_result.result,
-            feedback_result.name,
-            json_str_of_obj(feedback_result.cost)
-        )
+        if record_id is None or feedback_id is None:
+            assert result_json is not None, "`result_json` needs to be given if `record_id` or `feedback_id` are not provided."
+            record_id = result_json['record_id']
+            feedback_id = result_json['feedback_id']
+
+        last_ts = last_ts or 0
+        status = status or 0
+        result_json = result_json or dict()
+        total_cost = total_cost = 0.0
+        total_tokens = total_tokens or 0
+        result_str = json_str_of_obj(result_json)
 
-        self._insert_or_replace_vals(table=self.TABLE_FEEDBACKS, vals=vals)
+        conn, c = self._connect()
+        c.execute(
+            f"INSERT OR REPLACE INTO {self.TABLE_FEEDBACKS} VALUES (?, ?, ?, ?, ?, ?, ?)",
+            (
+                record_id, feedback_id, last_ts, status, result_str,
+                total_tokens, total_cost
+            )
+        )
+        self._close(conn)
 
-        if feedback_result.status == 2:
+        if status == 2:
             print(
-                f"{UNICODE_CHECK} feedback {feedback_result.feedback_result_id} on {feedback_result.record_id} -> {self.filename}"
+                f"{UNICODE_CHECK} feedback {feedback_id} on {record_id} -> {self.filename}"
             )
         else:
             print(
-                f"{UNCIODE_YIELD} feedback {feedback_result.feedback_result_id} on {feedback_result.record_id} -> {self.filename}"
+                f"{UNCIODE_YIELD} feedback {feedback_id} on {record_id} -> {self.filename}"
             )
 
     def get_feedback(
         self,
-        record_id: Optional[RecordID] = None,
-        feedback_result_id: Optional[FeedbackResultID] = None,
-        feedback_definition_id: Optional[FeedbackDefinitionID] = None,
-        status: Optional[FeedbackResultStatus] = None,
-        last_ts_before: Optional[datetime] = None
-    ) -> pd.DataFrame:
+        record_id: Optional[str] = None,
+        feedback_id: Optional[str] = None,
+        status: Optional[int] = None,
+        last_ts_before: Optional[int] = None
+    ):
 
         clauses = []
         vars = []
-
         if record_id is not None:
             clauses.append("record_id=?")
             vars.append(record_id)
-
-        if feedback_result_id is not None:
-            clauses.append("f.feedback_result_id=?")
-            vars.append(feedback_result_id)
-
-        if feedback_definition_id is not None:
-            clauses.append("f.feedback_definition_id=?")
-            vars.append(feedback_definition_id)
-
+        if feedback_id is not None:
+            clauses.append("feedback_id=?")
+            vars.append(feedback_id)
         if status is not None:
             if isinstance(status, Sequence):
                 clauses.append(
-                    "f.status in (" + (",".join(["?"] * len(status))) + ")"
+                    "status in (" + (",".join(["?"] * len(status))) + ")"
                 )
                 for v in status:
-                    vars.append(v.value)
+                    vars.append(v)
             else:
-                clauses.append("f.status=?")
+                clauses.append("status=?")
                 vars.append(status)
-
         if last_ts_before is not None:
-            clauses.append("f.last_ts<=?")
-            vars.append(last_ts_before.timestamp())
+            clauses.append("last_ts<=?")
+            vars.append(last_ts_before)
 
         where_clause = " AND ".join(clauses)
         if len(where_clause) > 0:
             where_clause = " AND " + where_clause
 
         query = f"""
             SELECT
-                f.record_id, f.feedback_result_id, f.feedback_definition_id, 
-                f.last_ts,
-                f.status,
-                f.error,
-                f.name,
-                f.result, 
-                f.cost_json,
-                f.calls_json,
-                fd.feedback_json, 
-                r.record_json, 
-                c.chain_json
-            FROM {self.TABLE_RECORDS} r
-                JOIN {self.TABLE_FEEDBACKS} f 
+                f.record_id, f.feedback_id, f.last_ts, f.status,
+                f.result_json, f.total_cost, f.total_tokens,
+                fd.feedback_json, r.record_json, c.chain_json
+            FROM {self.TABLE_FEEDBACKS} f 
                 JOIN {self.TABLE_FEEDBACK_DEFS} fd
+                JOIN {self.TABLE_RECORDS} r
                 JOIN {self.TABLE_CHAINS} c
-            WHERE f.feedback_definition_id=fd.feedback_definition_id
+            WHERE f.feedback_id=fd.feedback_id
                 AND r.record_id=f.record_id
                 AND r.chain_id=c.chain_id
                 {where_clause}
         """
 
         conn, c = self._connect()
         c.execute(query, vars)
         rows = c.fetchall()
         self._close(conn)
 
-        df = pd.DataFrame(
-            rows, columns=[description[0] for description in c.description]
-        )
+        from trulens_eval.tru_feedback import Feedback
 
-        def map_row(row):
-            # NOTE: pandas dataframe will take in the various classes below but the
-            # agg table used in UI will not like it. Sending it JSON/dicts instead.
-
-            row.calls_json = json.loads(
-                row.calls_json
-            )['calls']  # calls_json (sequence of FeedbackCall)
-            row.cost_json = json.loads(row.cost_json)  # cost_json (Cost)
-            row.feedback_json = json.loads(
-                row.feedback_json
-            )  # feedback_json (FeedbackDefinition)
-            row.record_json = json.loads(
-                row.record_json
-            )  # record_json (Record)
-            row.chain_json = json.loads(row.chain_json)  # chain_json (Model)
-
-            row.status = FeedbackResultStatus(row.status)
+        df_rows = []
+        for row in rows:
+            row = list(row)
+            row[4] = json.loads(row[4])  # result_json
+            row[7] = json.loads(row[7])  # feedback_json
+            row[8] = json.loads(row[8])  # record_json
+            row[9] = json.loads(row[9])  # chain_json
+
+            df_rows.append(row)
+
+        return pd.DataFrame(
+            df_rows,
+            columns=[
+                'record_id', 'feedback_id', 'last_ts', 'status', 'result_json',
+                'total_cost', 'total_tokens', 'feedback_json', 'record_json',
+                'chain_json'
+            ]
+        )
 
-            row['total_tokens'] = row.cost_json['n_tokens']
-            row['total_cost'] = row.cost_json['cost']
+    # TO REMOVE:
+    # TruDB requirement
+    def select(
+        self,
+        *query: Tuple[Query],
+        where: Optional[Condition] = None
+    ) -> pd.DataFrame:
+        raise NotImplementedError
+        """
+        # get the record json dumps from sql
+        record_strs = ...  # TODO(shayak)
 
-            return row
+        records: Sequence[Dict] = map(json.loads, record_strs)
 
-        df = df.apply(map_row, axis=1)
+        db = LocalTinyDB()  # in-memory db if filename not provided
+        for record in records:
+            db.insert_record(chain_id=record['chain_id'], record=record)
 
-        return pd.DataFrame(df)
+        return db.select(*query, where)
+        """
 
     def get_chain(self, chain_id: str) -> JSON:
         conn, c = self._connect()
         c.execute(
             f"SELECT chain_json FROM {self.TABLE_CHAINS} WHERE chain_id=?",
             (chain_id,)
         )
         result = c.fetchone()[0]
         conn.close()
 
         return json.loads(result)
 
     def get_records_and_feedback(
-        self,
-        chain_ids: Optional[List[str]] = None
+        self, chain_ids: List[str]
     ) -> Tuple[pd.DataFrame, Sequence[str]]:
         # This returns all models if the list of chain_ids is empty.
-        chain_ids = chain_ids or []
-
         conn, c = self._connect()
         query = f"""
-            SELECT r.record_id, f.calls_json, f.result, f.name
+            SELECT r.record_id, f.result_json
             FROM {self.TABLE_RECORDS} r 
             LEFT JOIN {self.TABLE_FEEDBACKS} f
                 ON r.record_id = f.record_id
             """
         if len(chain_ids) > 0:
             chain_id_list = ', '.join('?' * len(chain_ids))
             query = query + f" WHERE r.chain_id IN ({chain_id_list})"
@@ -565,48 +947,42 @@
         rows = c.fetchall()
         conn.close()
 
         df_records = pd.DataFrame(
             rows, columns=[description[0] for description in c.description]
         )
 
-        cost = df_records['cost_json'].map(Cost.parse_raw)
-        df_records['total_tokens'] = cost.map(lambda v: v.n_tokens)
-        df_records['total_cost'] = cost.map(lambda v: v.cost)
-
         if len(df_records) == 0:
             return df_records, []
 
-        result_cols = set()
+        # Apply the function to the 'data' column to convert it into separate columns
+        df_results['result_json'] = df_results['result_json'].apply(
+            lambda d: {} if d is None else json.loads(d)
+        )
 
-        def expand_results(row):
-            result_cols.add(row['name'])
-            row[row['name']] = row.result
-            row[row['name'] + "_calls"] = json.loads(
-                row.calls_json
-            )['calls']  # extra step to keep json root a dict
-            return pd.Series(row)
-
-        df_results = df_results.apply(expand_results, axis=1)
-        df_results = df_results.drop(columns=["name", "result", "calls_json"])
-
-        def nonempty(val):
-            if isinstance(val, np.float):
-                return not np.isnan(val)
-            return True
-
-        def merge_feedbacks(vals):
-            ress = list(filter(nonempty, vals))
-            if len(ress) > 0:
-                return ress[0]
-            else:
-                return np.nan
+        if "record_id" not in df_results.columns:
+            return df_results, []
 
-        df_results = df_results.groupby("record_id").agg(merge_feedbacks
-                                                        ).reset_index()
+        df_results = df_results.groupby("record_id").agg(
+            lambda dicts: {key: val for d in dicts for key, val in d.items()}
+        ).reset_index()
+
+        df_results = df_results['result_json'].apply(pd.Series)
+
+        result_cols = [
+            col for col in df_results.columns
+            if col not in ['feedback_id', 'record_id', '_success', "_error"]
+        ]
+
+        if len(df_results) == 0 or len(result_cols) == 0:
+            return df_records, []
 
         assert "record_id" in df_results.columns
         assert "record_id" in df_records.columns
 
         combined_df = df_records.merge(df_results, on=['record_id'])
+        combined_df = combined_df.drop(
+            columns=set(["_success", "_error"]
+                       ).intersection(set(combined_df.columns))
+        )
 
-        return combined_df, list(result_cols)
+        return combined_df, result_cols
```

## trulens_eval/tru_feedback.py

```diff
@@ -33,435 +33,499 @@
 ```
 
 """
 
 from datetime import datetime
 from inspect import Signature
 from inspect import signature
-import itertools
 import logging
 from multiprocessing.pool import AsyncResult
 import re
-from typing import (
-    Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple, Type, Union
-)
 from time import sleep
-from typing import (
-    Any, Callable, Dict, List, Optional, Sequence, Tuple, Type, Union
-)
+from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Type, Union
 
-from langchain.callbacks import get_openai_callback
 import numpy as np
 import openai
-import pydantic
+import requests
 from tqdm.auto import tqdm
 
 from trulens_eval import feedback_prompts
 from trulens_eval.keys import *
 from trulens_eval.provider_apis import Endpoint
-from trulens_eval.schema import Cost
-from trulens_eval.schema import FeedbackDefinition
-from trulens_eval.schema import FeedbackResult
-from trulens_eval.schema import FeedbackResultID
-from trulens_eval.schema import FeedbackResultStatus
-from trulens_eval.schema import FunctionOrMethod
-from trulens_eval.schema import Model
-from trulens_eval.tru_db import JSON
-from trulens_eval.tru_db import Query
-from trulens_eval.tru_db import JSON
-from trulens_eval.tru_db import Query
-
+from trulens_eval.tru_db import JSON, Query, obj_id_of_obj, query_of_path
 from trulens_eval.tru_db import Record
-from trulens_eval.schema import FeedbackCall
-from trulens_eval.util import jsonify
-from trulens_eval.util import SerialModel
+from trulens_eval.tru_db import TruDB
 from trulens_eval.util import TP
 
-PROVIDER_CLASS_NAMES = ['OpenAI', 'Huggingface', 'Cohere']
+# openai
 
-default_pass_fail_color_threshold = 0.5
+# (external) feedback-
+# provider
+# model
+
+# feedback_collator:
+# - record, feedback_imp, selector -> dict (input, output, other)
+
+# (external) feedback:
+# - *args, **kwargs -> real
+# - dict -> real
+# - (record, selectors) -> real
+# - str, List[str] -> real
+#    agg(relevance(str, str[0]),
+#        relevance(str, str[1])
+#    ...)
+
+# (internal) feedback_imp:
+# - Option 1 : input, output -> real
+# - Option 2: dict (input, output, other) -> real
+
+Selection = Union[Query, str]
+# "prompt" or "input" mean overall chain input text
+# "response" or "output"mean overall chain output text
+# Otherwise a Query is a path into a record structure.
+
+PROVIDER_CLASS_NAMES = ['OpenAI', 'Huggingface', 'Cohere']
 
 
 def check_provider(cls_or_name: Union[Type, str]) -> None:
     if isinstance(cls_or_name, str):
         cls_name = cls_or_name
     else:
         cls_name = cls_or_name.__name__
 
     assert cls_name in PROVIDER_CLASS_NAMES, f"Unsupported provider class {cls_name}"
 
 
-class Feedback(FeedbackDefinition):
-    # Implementation, not serializable, note that FeedbackDefinition contains
-    # `implementation` mean to serialize the below.
-    imp: Optional[Callable] = pydantic.Field(exclude=True)
-
-    # Aggregator method for feedback functions that produce more than one
-    # result.
-    agg: Optional[Callable] = pydantic.Field(exclude=True)
+class Feedback():
 
     def __init__(
         self,
         imp: Optional[Callable] = None,
-        agg: Optional[Callable] = None,
-        **kwargs
+        selectors: Optional[Dict[str, Selection]] = None,
+        feedback_id: Optional[str] = None
     ):
         """
         A Feedback function container.
 
         Parameters:
         
         - imp: Optional[Callable] -- implementation of the feedback function.
+        - selectors: Optional[Dict[str, Selection]] -- mapping of implementation
+          argument names to where to get them from a record.
         """
 
-        agg = agg or np.mean
-
-        if imp is not None:
-            # These are for serialization to/from json and for db storage.
-            kwargs['implementation'] = FunctionOrMethod.of_callable(imp)
-        else:
-            if "implementation" in kwargs:
-                imp: Callable = FunctionOrMethod.pick(
-                    **(kwargs['implementation'])
-                ).load()
-
-        if agg is not None:
-            kwargs['aggregator'] = FunctionOrMethod.of_callable(agg)
-        else:
-            if 'arrgregator' in kwargs:
-                agg: Callable = FunctionOrMethod.pick(**(kwargs['aggregator'])
-                                                     ).load()
-
-        super().__init__(**kwargs)
-
-        self.imp = imp
-        self.agg = agg
-
         # Verify that `imp` expects the arguments specified in `selectors`:
-        if self.imp is not None and self.selectors is not None:
-            sig: Signature = signature(self.imp)
-            for argname in self.selectors.keys():
+        if imp is not None and selectors is not None:
+            sig: Signature = signature(imp)
+            for argname in selectors.keys():
                 assert argname in sig.parameters, (
-                    f"{argname} is not an argument to {self.imp.__name__}. "
+                    f"{argname} is not an argument to {imp.__name__}. "
                     f"Its arguments are {list(sig.parameters.keys())}."
                 )
 
+        self.imp = imp
+        self.selectors = selectors
+
+        if feedback_id is not None:
+            self._feedback_id = feedback_id
+
+        if imp is not None and selectors is not None:
+            # These are for serialization to/from json and for db storage.
+
+            assert hasattr(
+                imp, "__self__"
+            ), "Feedback implementation is not a method (it may be a function)."
+            self.provider = imp.__self__
+            check_provider(self.provider.__class__.__name__)
+            self.imp_method_name = imp.__name__
+            self._json = self.to_json()
+            self._feedback_id = feedback_id or obj_id_of_obj(
+                self._json, prefix="feedback"
+            )
+            self._json['feedback_id'] = self._feedback_id
+
     @staticmethod
     def evaluate_deferred(tru: 'Tru'):
         db = tru.db
 
         def prepare_feedback(row):
             record_json = row.record_json
-            record = Record(**record_json)
 
-            chain_json = row.chain_json
-
-            feedback = Feedback(**row.feedback_json)
-            feedback.run_and_log(
-                record=record,
-                chain=chain_json,
-                tru=tru,
-                feedback_result_id=row.feedback_result_id
-            )
+            feedback = Feedback.of_json(row.feedback_json)
+            feedback.run_and_log(record_json=record_json, tru=tru)
 
         feedbacks = db.get_feedback()
 
         for i, row in feedbacks.iterrows():
-            if row.status == FeedbackResultStatus.NONE:
+            if row.status == 0:
                 tqdm.write(f"Starting run for row {i}.")
 
                 TP().runlater(prepare_feedback, row)
-
-            elif row.status in [FeedbackResultStatus.RUNNING]:
+            elif row.status in [1]:
                 now = datetime.now().timestamp()
                 if now - row.last_ts > 30:
                     tqdm.write(
                         f"Incomplete row {i} last made progress over 30 seconds ago. Retrying."
                     )
                     TP().runlater(prepare_feedback, row)
-
                 else:
                     tqdm.write(
                         f"Incomplete row {i} last made progress less than 30 seconds ago. Giving it more time."
                     )
 
-            elif row.status in [FeedbackResultStatus.FAILED]:
+            elif row.status in [-1]:
                 now = datetime.now().timestamp()
                 if now - row.last_ts > 60 * 5:
                     tqdm.write(
                         f"Failed row {i} last made progress over 5 minutes ago. Retrying."
                     )
                     TP().runlater(prepare_feedback, row)
-
                 else:
                     tqdm.write(
                         f"Failed row {i} last made progress less than 5 minutes ago. Not touching it for now."
                     )
 
-            elif row.status == FeedbackResultStatus.DONE:
+            elif row.status == 2:
                 pass
 
-    def __call__(self, *args, **kwargs) -> Any:
-        assert self.imp is not None, "Feedback definition needs an implementation to call."
-        return self.imp(*args, **kwargs)
+        # TP().finish()
+        # TP().runrepeatedly(runner)
+
+    @property
+    def json(self):
+        assert hasattr(
+            self, "_json"
+        ), "Cannot json-size partially defined feedback function."
+        return self._json
+
+    @property
+    def feedback_id(self):
+        assert hasattr(
+            self, "_feedback_id"
+        ), "Cannot get id of partially defined feedback function."
+        return self._feedback_id
+
+    @staticmethod
+    def selection_to_json(select: Selection) -> dict:
+        if isinstance(select, str):
+            return select
+        elif isinstance(select, Query):
+            return select._path
+        else:
+            raise ValueError(f"Unknown selection type {type(select)}.")
+
+    @staticmethod
+    def selection_of_json(obj: Union[List, str]) -> Selection:
+        if isinstance(obj, str):
+            return obj
+        elif isinstance(obj, (List, Tuple)):
+            return query_of_path(obj)  # TODO
+        else:
+            raise ValueError(f"Unknown selection encoding of type {type(obj)}.")
 
-    def aggregate(self, func: Callable) -> 'Feedback':
-        return Feedback(imp=self.imp, selectors=self.selectors, agg=func)
+    def to_json(self) -> dict:
+        selectors_json = {
+            k: Feedback.selection_to_json(v) for k, v in self.selectors.items()
+        }
+        return {
+            'selectors': selectors_json,
+            'imp_method_name': self.imp_method_name,
+            'provider': self.provider.to_json()
+        }
 
     @staticmethod
-    def of_feedback_definition(f: FeedbackDefinition):
-        implementation = f.implementation
-        aggregator = f.aggregator
+    def of_json(obj) -> 'Feedback':
+        assert "selectors" in obj, "Feedback encoding has no 'selectors' field."
+        assert "imp_method_name" in obj, "Feedback encoding has no 'imp_method_name' field."
+        assert "provider" in obj, "Feedback encoding has no 'provider' field."
+
+        imp_method_name = obj['imp_method_name']
+        selectors = {
+            k: Feedback.selection_of_json(v)
+            for k, v in obj['selectors'].items()
+        }
+        provider = Provider.of_json(obj['provider'])
+
+        assert hasattr(
+            provider, imp_method_name
+        ), f"Provider {provider.__name__} has no feedback function {imp_method_name}."
+        imp = getattr(provider, imp_method_name)
+
+        return Feedback(imp, selectors=selectors)
+
+    def on_multiple(
+        self,
+        multiarg: str,
+        each_query: Optional[Query] = None,
+        agg: Callable = np.mean
+    ) -> 'Feedback':
+        """
+        Create a variant of `self` whose implementation will accept multiple
+        values for argument `multiarg`, aggregating feedback results for each.
+        Optionally each input element is further projected with `each_query`.
+
+        Parameters:
+
+        - multiarg: str -- implementation argument that expects multiple values.
+        - each_query: Optional[Query] -- a query providing the path from each
+          input to `multiarg` to some inner value which will be sent to `self.imp`.
+        """
+
+        def wrapped_imp(**kwargs):
+            assert multiarg in kwargs, f"Feedback function expected {multiarg} keyword argument."
+
+            multi = kwargs[multiarg]
+
+            assert isinstance(
+                multi, Sequence
+            ), f"Feedback function expected a sequence on {multiarg} argument."
+
+            rets: List[AsyncResult[float]] = []
+
+            for aval in multi:
+
+                if each_query is not None:
+                    aval = TruDB.project(
+                        query=each_query, record_json=aval, chain_json=None
+                    )
 
-        imp_func = implementation.load()
-        agg_func = aggregator.load()
+                kwargs[multiarg] = aval
 
-        return Feedback(imp=imp_func, agg=agg_func, **f.dict())
+                rets.append(TP().promise(self.imp, **kwargs))
+
+            rets: List[float] = list(map(lambda r: r.get(), rets))
+
+            rets = np.array(rets)
+
+            return agg(rets)
+
+        wrapped_imp.__name__ = self.imp.__name__
+
+        wrapped_imp.__self__ = self.imp.__self__  # needed for serialization
+
+        # Copy over signature from wrapped function. Otherwise signature of the
+        # wrapped method will include just kwargs which is insufficient for
+        # verify arguments (see Feedback.__init__).
+        wrapped_imp.__signature__ = signature(self.imp)
+
+        return Feedback(imp=wrapped_imp, selectors=self.selectors)
 
     def on_prompt(self, arg: str = "text"):
         """
         Create a variant of `self` that will take in the main chain input or
         "prompt" as input, sending it as an argument `arg` to implementation.
         """
 
-        return Feedback(
-            imp=self.imp, selectors={arg: Query.RecordInput}, agg=self.agg
-        )
+        return Feedback(imp=self.imp, selectors={arg: "prompt"})
 
     on_input = on_prompt
 
     def on_response(self, arg: str = "text"):
         """
         Create a variant of `self` that will take in the main chain output or
         "response" as input, sending it as an argument `arg` to implementation.
         """
 
-        return Feedback(
-            imp=self.imp, selectors={arg: Query.RecordOutput}, agg=self.agg
-        )
+        return Feedback(imp=self.imp, selectors={arg: "response"})
 
     on_output = on_response
 
     def on(self, **selectors):
         """
         Create a variant of `self` with the same implementation but the given `selectors`.
         """
 
-        return Feedback(imp=self.imp, selectors=selectors, agg=self.agg)
+        return Feedback(imp=self.imp, selectors=selectors)
 
-    def run(self, chain: Union[Model, JSON], record: Record) -> FeedbackResult:
+    def run_on_record(self, chain_json: JSON, record_json: JSON) -> Any:
         """
         Run the feedback function on the given `record`. The `chain` that
         produced the record is also required to determine input/output argument
         names.
-
-        Might not have a Chain here but only the serialized chain_json .
         """
 
-        if isinstance(chain, Model):
-            chain_json = jsonify(chain)
-        else:
-            chain_json = chain
-
-        result_vals = []
-
-        feedback_calls = []
-
-        feedback_result = FeedbackResult(
-            feedback_definition_id=self.feedback_definition_id,
-            record_id=record.record_id,
-            chain_id=chain_json['chain_id'],
-            name=self.name
-        )
+        if 'record_id' not in record_json:
+            record_json['record_id'] = None
 
         try:
-            total_tokens = 0
-            total_cost = 0.0
-
-            for ins in self.extract_selection(chain=chain_json, record=record):
-
-                # TODO: Do this only if there is an openai model inside the chain:
-                # NODE: This only works for langchain uses of openai.
-                with get_openai_callback() as cb:
-                    result_val = self.imp(**ins)
-                    result_vals.append(result_val)
-
-                    feedback_call = FeedbackCall(args=ins, ret=result_val)
-                    feedback_calls.append(feedback_call)
-
-                    total_tokens += cb.total_tokens
-                    total_cost += cb.total_cost
-
-            result_vals = np.array(result_vals)
-            result = self.agg(result_vals)
-
-            feedback_result.update(
-                result=result,
-                status=FeedbackResultStatus.DONE,
-                cost=Cost(n_tokens=total_tokens, cost=total_cost),
-                calls=feedback_calls
+            ins = self.extract_selection(
+                chain_json=chain_json, record_json=record_json
             )
+            ret = self.imp(**ins)
 
-            return feedback_result
+            return {
+                '_success': True,
+                'feedback_id': self.feedback_id,
+                'record_id': record_json['record_id'],
+                self.name: ret
+            }
 
         except Exception as e:
-            raise e
+            return {
+                '_success': False,
+                'feedback_id': self.feedback_id,
+                'record_id': record_json['record_id'],
+                '_error': str(e)
+            }
+
+    def run_and_log(self, record_json: JSON, tru: 'Tru') -> None:
+        record_id = record_json['record_id']
+        chain_id = record_json['chain_id']
 
-    def run_and_log(
-        self,
-        record: Record,
-        tru: 'Tru',
-        chain: Union[Model, JSON] = None,
-        feedback_result_id: Optional[FeedbackResultID] = None
-    ) -> FeedbackResult:
-        record_id = record.record_id
-        chain_id = record.chain_id
+        ts_now = datetime.now().timestamp()
 
         db = tru.db
 
-        # Placeholder result to indicate a run.
-        feedback_result = FeedbackResult(
-            feedback_definition_id=self.feedback_definition_id,
-            feedback_result_id=feedback_result_id,
-            record_id=record_id,
-            chain_id=chain_id,
-            name=self.name
-        )
-
-        if feedback_result_id is None:
-            feedback_result_id = feedback_result.feedback_result_id
-
         try:
             db.insert_feedback(
-                feedback_result.update(
-                    status=FeedbackResultStatus.RUNNING  # in progress
-                )
+                record_id=record_id,
+                feedback_id=self.feedback_id,
+                last_ts=ts_now,
+                status=1  # in progress
             )
 
-            feedback_result = self.run(
-                chain=chain, record=record
-            ).update(feedback_result_id=feedback_result_id)
+            chain_json = db.get_chain(chain_id=chain_id)
 
-        except Exception as e:
-            db.insert_feedback(
-                feedback_result.update(
-                    error=str(e), status=FeedbackResultStatus.FAILED
-                )
+            res = self.run_on_record(
+                chain_json=chain_json, record_json=record_json
             )
-            return
 
-        # Otherwise update based on what Feedback.run produced (could be success or failure).
-        db.insert_feedback(feedback_result)
+        except Exception as e:
+            print(e)
+            res = {
+                '_success': False,
+                'feedback_id': self.feedback_id,
+                'record_id': record_json['record_id'],
+                '_error': str(e)
+            }
 
-        return feedback_result
+        ts_now = datetime.now().timestamp()
+
+        if res['_success']:
+            db.insert_feedback(
+                record_id=record_id,
+                feedback_id=self.feedback_id,
+                last_ts=ts_now,
+                status=2,  # done and good
+                result_json=res,
+                total_cost=-1.0,  # todo
+                total_tokens=-1  # todo
+            )
+        else:
+            # TODO: indicate failure better
+            db.insert_feedback(
+                record_id=record_id,
+                feedback_id=self.feedback_id,
+                last_ts=ts_now,
+                status=-1,  # failure
+                result_json=res,
+                total_cost=-1.0,  # todo
+                total_tokens=-1  # todo
+            )
 
     @property
     def name(self):
         """
         Name of the feedback function. Presently derived from the name of the
         function implementing it.
         """
 
         return self.imp.__name__
 
-    def extract_selection(self, chain: Union[Model, JSON],
-                          record: Record) -> Iterable[Dict[str, Any]]:
+    def extract_selection(self, chain_json: Dict,
+                          record_json: Dict) -> Dict[str, Any]:
         """
         Given the `chain` that produced the given `record`, extract from
         `record` the values that will be sent as arguments to the implementation
         as specified by `self.selectors`.
         """
 
-        arg_vals = {}
+        ret = {}
 
         for k, v in self.selectors.items():
-            if isinstance(v, Query.Query):
+            if isinstance(v, Query):
                 q = v
 
-            else:
-                raise RuntimeError(f"Unhandled selection type {type(v)}.")
+            elif v == "prompt" or v == "input":
+                if len(chain_json['input_keys']) > 1:
+                    #logging.warn(
+                    #    f"Chain has more than one input, guessing the first one is prompt."
+                    #)
+                    pass
+
+                input_key = chain_json['input_keys'][0]
+
+                q = Record.chain._call.args.inputs[input_key]
+
+            elif v == "response" or v == "output":
+                if len(chain_json['output_keys']) > 1:
+                    #logging.warn(
+                    #    "Chain has more than one ouput, guessing the first one is response."
+                    #)
+                    pass
 
-            if q.path[0] == Query.Record.path[0]:
-                o = record.layout_calls_as_chain()
-            elif q.path[0] == Query.Chain.path[0]:
-                o = chain
-            else:
-                raise ValueError(
-                    f"Query {q} does not indicate whether it is about a record or about a chain."
-                )
+                output_key = chain_json['output_keys'][0]
 
-            q_within_o = Query.Query(path=q.path[1:])
-            arg_vals[k] = list(q_within_o(o))
+                q = Record.chain._call.rets[output_key]
 
-        keys = arg_vals.keys()
-        vals = arg_vals.values()
+            else:
+                raise RuntimeError(f"Unhandled selection type {type(v)}.")
 
-        assignments = itertools.product(*vals)
+            val = TruDB.project(
+                query=q, record_json=record_json, chain_json=chain_json
+            )
+            ret[k] = val
 
-        for assignment in assignments:
-            yield {k: v for k, v in zip(keys, assignment)}
+        return ret
 
 
 pat_1_10 = re.compile(r"\s*([1-9][0-9]*)\s*")
 
 
 def _re_1_10_rating(str_val):
     matches = pat_1_10.fullmatch(str_val)
     if not matches:
         # Try soft match
         matches = re.search('[1-9][0-9]*', str_val)
         if not matches:
-            logger.warn(f"1-10 rating regex failed to match on: '{str_val}'")
+            logging.warn(f"1-10 rating regex failed to match on: '{str_val}'")
             return -10  # so this will be reported as -1 after division by 10
 
     return int(matches.group())
 
 
-class Provider(SerialModel):
-    endpoint: Any = pydantic.Field(exclude=True)
+class Provider():
 
     @staticmethod
     def of_json(obj: Dict) -> 'Provider':
-        cls_name = obj['class_name']
-        mod_name = obj['module_name']  # ignored for now
+        cls_name = obj['class']
         check_provider(cls_name)
 
         cls = eval(cls_name)
-        kwargs = {
-            k: v
-            for k, v in obj.items()
-            if k not in ['class_name', 'module_name']
-        }
+        kwargs = {k: v for k, v in obj.items() if k != "class"}
 
         return cls(**kwargs)
 
     def to_json(self: 'Provider', **extras) -> Dict:
-        obj = {
-            'class_name': self.__class__.__name__,
-            'module_name': self.__class__.__module__
-        }
+        obj = {'class': self.__class__.__name__}
         obj.update(**extras)
         return obj
 
 
 class OpenAI(Provider):
-    model_engine: str = "gpt-3.5-turbo"
 
     def __init__(self, model_engine: str = "gpt-3.5-turbo"):
         """
         A set of OpenAI Feedback Functions.
 
         Parameters:
 
         - model_engine (str, optional): The specific model version. Defaults to
           "gpt-3.5-turbo".
         """
-        super().__init__()  # need to include pydantic.BaseModel.__init__
-
-        set_openai_key()
         self.model_engine = model_engine
         self.endpoint = Endpoint(name="openai")
 
     def to_json(self) -> Dict:
         return Provider.to_json(self, model_engine=self.model_engine)
 
     def _moderation(self, text: str):
@@ -752,31 +816,24 @@
                 }
             ]
         )["choices"][0]["message"]["content"]
     )
     return oai_chat_response
 
 
-# Cannot put these inside Huggingface since it interferes with pydantic.BaseModel.
-HUGS_SENTIMENT_API_URL = "https://api-inference.huggingface.co/models/cardiffnlp/twitter-roberta-base-sentiment"
-HUGS_TOXIC_API_URL = "https://api-inference.huggingface.co/models/martin-ha/toxic-comment-model"
-HUGS_CHAT_API_URL = "https://api-inference.huggingface.co/models/facebook/blenderbot-3B"
-HUGS_LANGUAGE_API_URL = "https://api-inference.huggingface.co/models/papluca/xlm-roberta-base-language-detection"
-
-
 class Huggingface(Provider):
 
+    SENTIMENT_API_URL = "https://api-inference.huggingface.co/models/cardiffnlp/twitter-roberta-base-sentiment"
+    TOXIC_API_URL = "https://api-inference.huggingface.co/models/martin-ha/toxic-comment-model"
+    CHAT_API_URL = "https://api-inference.huggingface.co/models/facebook/blenderbot-3B"
+    LANGUAGE_API_URL = "https://api-inference.huggingface.co/models/papluca/xlm-roberta-base-language-detection"
+
     def __init__(self):
+        """A set of Huggingface Feedback Functions. Utilizes huggingface api-inference
         """
-        A set of Huggingface Feedback Functions. Utilizes huggingface
-        api-inference.
-        """
-
-        super().__init__()  # need to include pydantic.BaseModel.__init__
-
         self.endpoint = Endpoint(
             name="huggingface", post_headers=get_huggingface_headers()
         )
 
     def language_match(self, text1: str, text2: str) -> float:
         """
         Uses Huggingface's papluca/xlm-roberta-base-language-detection model. A
@@ -796,15 +853,15 @@
             float: A value between 0 and 1. 0 being "different languages" and 1
             being "same languages".
         """
 
         def get_scores(text):
             payload = {"inputs": text}
             hf_response = self.endpoint.post(
-                url=HUGS_LANGUAGE_API_URL, payload=payload, timeout=30
+                url=Huggingface.LANGUAGE_API_URL, payload=payload, timeout=30
             )
             return {r['label']: r['score'] for r in hf_response}
 
         max_length = 500
         scores1: AsyncResult[Dict] = TP().promise(
             get_scores, text=text1[:max_length]
         )
@@ -837,15 +894,15 @@
             being "positive sentiment".
         """
         max_length = 500
         truncated_text = text[:max_length]
         payload = {"inputs": truncated_text}
 
         hf_response = self.endpoint.post(
-            url=HUGS_SENTIMENT_API_URL, payload=payload
+            url=Huggingface.SENTIMENT_API_URL, payload=payload
         )
 
         for label in hf_response:
             if label['label'] == 'LABEL_2':
                 return label['score']
 
     def not_toxic(self, text: str) -> float:
@@ -860,29 +917,26 @@
             float: A value between 0 and 1. 0 being "toxic" and 1 being "not
             toxic".
         """
         max_length = 500
         truncated_text = text[:max_length]
         payload = {"inputs": truncated_text}
         hf_response = self.endpoint.post(
-            url=HUGS_TOXIC_API_URL, payload=payload
+            url=Huggingface.TOXIC_API_URL, payload=payload
         )
 
         for label in hf_response:
             if label['label'] == 'toxic':
                 return label['score']
 
 
 # cohere
 class Cohere(Provider):
-    model_engine: str = "large"
 
     def __init__(self, model_engine='large'):
-        super().__init__()  # need to include pydantic.BaseModel.__init__
-
         Cohere().endpoint = Endpoint(name="cohere")
         self.model_engine = model_engine
 
     def to_json(self) -> Dict:
         return Provider.to_json(self, model_engine=self.model_engine)
 
     def sentiment(
```

## trulens_eval/util.py

```diff
@@ -1,897 +1,75 @@
 """
 Utilities.
 
 Do not import anything from trulens_eval here.
 """
 
-from __future__ import annotations
-
-import abc
-from enum import Enum
-from inspect import stack
-import itertools
-import json
 import logging
-from multiprocessing.context import TimeoutError
 from multiprocessing.pool import AsyncResult
 from multiprocessing.pool import ThreadPool
-from pathlib import Path
 from queue import Queue
-from threading import Thread
 from time import sleep
-from typing import (
-    Any, Callable, Dict, Hashable, Iterable, Iterator, List, Optional, Sequence,
-    Set, Tuple, TypeVar, Union
-)
+from typing import Any, Callable, Dict, Hashable, List, Optional, Sequence, TypeVar, Union
 
-from merkle_json import MerkleJson
-from munch import Munch as Bunch
-import pandas as pd
-import pydantic
+from multiprocessing.context import TimeoutError
+from dataclasses import dataclass
 
-logger = logging.getLogger(__name__)
+import pandas as pd
+from tqdm.auto import tqdm
 
 T = TypeVar("T")
 
 UNICODE_CHECK = "✅"
 UNCIODE_YIELD = "⚡"
 
-# Collection utilities
-
 
 def first(seq: Sequence[T]) -> T:
     return seq[0]
 
 
 def second(seq: Sequence[T]) -> T:
     return seq[1]
 
 
 def third(seq: Sequence[T]) -> T:
     return seq[2]
 
 
-# Generator utils
-
-
-def iterable_peek(it: Iterable[T]) -> Tuple[T, Iterable[T]]:
-    iterator = iter(it)
-    item = next(iterator)
-    return item, itertools.chain([item], iterator)
-
-
-# JSON utilities
-
-JSON_BASES = (str, int, float, type(None))
-JSON_BASES_T = Union[str, int, float, type(None)]
-# JSON = (List, Dict) + JSON_BASES
-# JSON_T = Union[JSON_BASES_T, List, Dict]
-JSON = Union[JSON_BASES_T, Dict[str, Any]]
-# want: Union[JSON_BASES_T, Dict[str, JSON]] but this will result in loop at some point
-
-mj = MerkleJson()
-
-
-def is_empty(obj):
-    try:
-        return len(obj) == 0
-    except Exception:
-        return False
-
-
-def is_noserio(obj):
-    """
-    Determines whether the given json object represents some non-serializable
-    object. See `noserio`.
-    """
-    return isinstance(obj, dict) and "_NON_SERIALIZED_OBJECT" in obj
-
-
-def noserio(obj, **extra: Dict) -> dict:
-    """
-    Create a json structure to represent a non-serializable object. Any
-    additional keyword arguments are included.
-    """
-
-    inner = {
-        "id": id(obj),
-        "class": obj.__class__.__name__,
-        "module": obj.__class__.__module__,
-        "bases": list(map(lambda b: b.__name__, obj.__class__.__bases__))
-    }
-    inner.update(extra)
-
-    return {'_NON_SERIALIZED_OBJECT': inner}
-
-
-def obj_id_of_obj(obj: dict, prefix="obj"):
-    """
-    Create an id from a json-able structure/definition. Should produce the same
-    name if definition stays the same.
-    """
-
-    return f"{prefix}_hash_{mj.hash(obj)}"
-
-
-def json_str_of_obj(obj: Any, *args, **kwargs) -> str:
-    """
-    Encode the given json object as a string.
-    """
-
-    if isinstance(obj, pydantic.BaseModel):
-        kwargs['encoder'] = json_default
-        return obj.json(*args, **kwargs)
-
-    return json.dumps(obj, default=json_default)
-
-
-def json_default(obj: Any) -> str:
-    """
-    Produce a representation of an object which cannot be json-serialized.
-    """
-
-    # obj = jsonify(obj)
+class JLens(object):
+    # TODO(piotrm): more appropriate version of tinydb.Query
 
-    # Try the encoders included with pydantic first (should handle things like
-    # Datetime):
-    try:
-        return pydantic.json.pydantic_encoder(obj)
-    except:
+    class Step():
         pass
 
-    #if isinstance(obj, Enum):
-    #    return obj.name
-
-    #if isinstance(obj, dict):
-    #    return
-
-    #if isinstance(obj, pydantic.BaseModel):
-    #    try:
-    #        return json.dumps(obj.json())
-    #    except Exception as e:
-    #        return noserio(obj, exception=e)
-
-    # Intentionally not including much in this indicator to make sure the model
-    # hashing procedure does not get randomized due to something here.
-
-    return noserio(obj)
-
-
-def jsonify(obj: Any, dicted=None) -> JSON:
-    """
-    Convert the given object into types that can be serialized in json.
-    """
-
-    dicted = dicted or dict()
-
-    if isinstance(obj, JSON_BASES):
-        return obj
-
-    if isinstance(obj, Path):
-        return str(obj)
-
-    if type(obj) in pydantic.json.ENCODERS_BY_TYPE:
-        return obj
-    # if isinstance(obj, Enum):
-    #    return str(obj)
-
-    if id(obj) in dicted:
-        return {'_CIRCULAR_REFERENCE': id(obj)}
-
-    new_dicted = {k: v for k, v in dicted.items()}
-
-    if isinstance(obj, Enum):
-        return obj.name
-
-    if isinstance(obj, Dict):
-        temp = {}
-        new_dicted[id(obj)] = temp
-        temp.update({k: jsonify(v, dicted=new_dicted) for k, v in obj.items()})
-        return temp
-
-    elif isinstance(obj, Sequence):
-        temp = []
-        new_dicted[id(obj)] = temp
-        for x in (jsonify(v, dicted=new_dicted) for v in obj):
-            temp.append(x)
-        return temp
-
-    elif isinstance(obj, Set):
-        temp = []
-        new_dicted[id(obj)] = temp
-        for x in (jsonify(v, dicted=new_dicted) for v in obj):
-            temp.append(x)
-        return temp
-
-    elif isinstance(obj, pydantic.BaseModel):
-        temp = {}
-        new_dicted[id(obj)] = temp
-        temp.update(
-            {
-                k: jsonify(getattr(obj, k), dicted=new_dicted)
-                for k in obj.__fields__
-            }
-        )
-        return temp
-
-    else:
-        logger.debug(
-            f"Don't know how to jsonify an object '{str(obj)[0:32]}' of type '{type(obj)}'."
-        )
-
-        return noserio(obj)
-
-
-def leaf_queries(obj_json: JSON, query: JSONPath = None) -> Iterable[JSONPath]:
-    """
-    Get all queries for the given object that select all of its leaf values.
-    """
-
-    query = query or JSONPath()
-
-    if isinstance(obj_json, JSON_BASES):
-        yield query
-
-    elif isinstance(obj_json, Dict):
-        for k, v in obj_json.items():
-            sub_query = query[k]
-            for res in leaf_queries(obj_json[k], sub_query):
-                yield res
-
-    elif isinstance(obj_json, Sequence):
-        for i, v in enumerate(obj_json):
-            sub_query = query[i]
-            for res in leaf_queries(obj_json[i], sub_query):
-                yield res
-
-    else:
-        yield query
-
-
-def all_queries(obj: Any, query: JSONPath = None) -> Iterable[JSONPath]:
-    """
-    Get all queries for the given object.
-    """
-
-    query = query or JSONPath()
-
-    if isinstance(obj, JSON_BASES):
-        yield query
-
-    elif isinstance(obj, pydantic.BaseModel):
-        yield query
-
-        for k in obj.__fields__:
-            v = getattr(obj, k)
-            sub_query = query[k]
-            for res in all_queries(v, sub_query):
-                yield res
-
-    elif isinstance(obj, Dict):
-        yield query
-
-        for k, v in obj.items():
-            sub_query = query[k]
-            for res in all_queries(obj[k], sub_query):
-                yield res
-
-    elif isinstance(obj, Sequence):
-        yield query
-
-        for i, v in enumerate(obj):
-            sub_query = query[i]
-            for res in all_queries(obj[i], sub_query):
-                yield res
-
-    else:
-        yield query
-
-
-def all_objects(obj: Any,
-                query: JSONPath = None) -> Iterable[Tuple[JSONPath, Any]]:
-    """
-    Get all queries for the given object.
-    """
-
-    query = query or JSONPath()
-
-    if isinstance(obj, JSON_BASES):
-        yield (query, obj)
-
-    elif isinstance(obj, pydantic.BaseModel):
-        yield (query, obj)
-
-        for k in obj.__fields__:
-            v = getattr(obj, k)
-            sub_query = query[k]
-            for res in all_objects(v, sub_query):
-                yield res
-
-    elif isinstance(obj, Dict):
-        yield (query, obj)
-
-        for k, v in obj.items():
-            sub_query = query[k]
-            for res in all_objects(obj[k], sub_query):
-                yield res
-
-    elif isinstance(obj, Sequence):
-        yield (query, obj)
-
-        for i, v in enumerate(obj):
-            sub_query = query[i]
-            for res in all_objects(obj[i], sub_query):
-                yield res
-
-    else:
-        yield (query, obj)
-
-
-def leafs(obj: Any) -> Iterable[Tuple[str, Any]]:
-    for q in leaf_queries(obj):
-        path_str = _query_str(q)
-        val = _project(q._path, obj)
-        yield (path_str, val)
-
-
-def matching_queries(obj: Any, match: Callable) -> Iterable[JSONPath]:
-    for q in all_queries(obj):
-        val = _project(q._path, obj)
-        if match(q, val):
-            yield q
-
-
-def matching_objects(obj: Any,
-                     match: Callable) -> Iterable[Tuple[JSONPath, Any]]:
-    for q, val in all_objects(obj):
-        if match(q, val):
-            yield (q, val)
-
-
-def _query_str(query: JSONPath) -> str:
-
-    def render(ks):
-        if len(ks) == 0:
-            return ""
-
-        first = ks[0]
-        if len(ks) > 1:
-            rest = ks[1:]
-        else:
-            rest = ()
-
-        if isinstance(first, str):
-            return f".{first}{render(rest)}"
-        elif isinstance(first, int):
-            return f"[{first}]{render(rest)}"
-        else:
-            RuntimeError(
-                f"Don't know how to render path element {first} of type {type(first)}."
-            )
-
-    return "Record" + render(query._path)
-
-
-@staticmethod
-def set_in_json(query: JSONPath, in_json: JSON, val: JSON) -> JSON:
-    return _set_in_json(query._path, in_json=in_json, val=val)
-
-
-@staticmethod
-def _set_in_json(path, in_json: JSON, val: JSON) -> JSON:
-    if len(path) == 0:
-        if isinstance(in_json, Dict):
-            assert isinstance(val, Dict)
-            in_json = {k: v for k, v in in_json.items()}
-            in_json.update(val)
-            return in_json
-
-        assert in_json is None, f"Cannot set non-None json object: {in_json}"
-
-        return val
-
-    if len(path) == 1:
-        first = path[0]
-        rest = []
-    else:
-        first = path[0]
-        rest = path[1:]
-
-    if isinstance(first, str):
-        if isinstance(in_json, Dict):
-            in_json = {k: v for k, v in in_json.items()}
-            if not first in in_json:
-                in_json[first] = None
-        elif in_json is None:
-            in_json = {first: None}
-        else:
-            raise RuntimeError(
-                f"Do not know how to set path {path} in {in_json}."
-            )
-
-        in_json[first] = _set_in_json(
-            path=rest, in_json=in_json[first], val=val
-        )
-        return in_json
-
-    elif isinstance(first, int):
-        if isinstance(in_json, Sequence):
-            # In case it is some immutable sequence. Also copy.
-            in_json = list(in_json)
-        elif in_json is None:
-            in_json = []
-        else:
-            raise RuntimeError(
-                f"Do not know how to set path {path} in {in_json}."
-            )
-
-        while len(in_json) <= first:
-            in_json.append(None)
-
-        in_json[first] = _set_in_json(
-            path=rest, in_json=in_json[first], val=val
-        )
-        return in_json
-
-    else:
-        raise RuntimeError(f"Do not know how to set path {path} in {in_json}.")
-
-
-# TODO: remove
-def _project(path: List, obj: Any):
-    if len(path) == 0:
-        return obj
-
-    first = path[0]
-    if len(path) > 1:
-        rest = path[1:]
-    else:
-        rest = ()
-
-    if isinstance(first, str):
-        if isinstance(obj, pydantic.BaseModel):
-            if not hasattr(obj, first):
-                logger.warn(
-                    f"Cannot project {str(obj)[0:32]} with path {path} because {first} is not an attribute here."
-                )
-                return None
-            return _project(path=rest, obj=getattr(obj, first))
-
-        elif isinstance(obj, Dict):
-            if first not in obj:
-                logger.warn(
-                    f"Cannot project {str(obj)[0:32]} with path {path} because {first} is not a key here."
-                )
-                return None
-            return _project(path=rest, obj=obj[first])
-
-        else:
-            logger.warn(
-                f"Cannot project {str(obj)[0:32]} with path {path} because object is not a dict or model."
-            )
-            return None
-
-    elif isinstance(first, int):
-        if not isinstance(obj, Sequence) or first >= len(obj):
-            logger.warn(f"Cannot project {str(obj)[0:32]} with path {path}.")
-            return None
-
-        return _project(path=rest, obj=obj[first])
-    else:
-        raise RuntimeError(
-            f"Don't know how to locate element with key of type {first}"
-        )
-
-
-class SerialModel(pydantic.BaseModel):
-    """
-    Trulens-specific additions on top of pydantic models. Includes utilities to help serialization mostly.
-    """
-
-    def update(self, **d):
-        for k, v in d.items():
-            setattr(self, k, v)
-
-        return self
-
-
-# JSONPath, a container for selector/accessors/setters of data stored in a json
-# structure. Cannot make abstract since pydantic will try to initialize it.
-class Step(SerialModel):  #, abc.ABC):
-    """
-    A step in a selection path.
-    """
-
-    @classmethod
-    def __get_validator__(cls):
-        yield cls.validate
-
-    @classmethod
-    def validate(cls, d):
-        if not isinstance(d, Dict):
-            return d
-
-        ATTRIBUTE_TYPE_MAP = {
-            'item': GetItem,
-            'index': GetIndex,
-            'attribute': GetAttribute,
-            'item_or_attribute': GetItemOrAttribute,
-            'start': GetSlice,
-            'stop': GetSlice,
-            'step': GetSlice,
-            'items': GetItems,
-            'indices': GetIndices
-        }
-
-        a = next(iter(d.keys()))
-        if a in ATTRIBUTE_TYPE_MAP:
-            return ATTRIBUTE_TYPE_MAP[a](**d)
-        else:
-            raise RuntimeError(f"Don't know how to deserialize Step with {d}.")
-
-    # @abc.abstractmethod
-    def __call__(self, obj: Any) -> Iterable[Any]:
-        """
-        Get the element of `obj`, indexed by `self`.
-        """
-        raise NotImplementedError()
-
-    # @abc.abstractmethod
-    def set(self, obj: Any, val: Any) -> Any:
-        """
-        Set the value(s) indicated by self in `obj` to value `val`.
-        """
-        raise NotImplementedError()
-
-
-class GetAttribute(Step):
-    attribute: str
-
-    def __hash__(self):
-        return hash(self.attribute)
-
-    def __call__(self, obj: Any) -> Iterable[Any]:
-        if hasattr(obj, self.attribute):
-            yield getattr(obj, self.attribute)
-        else:
-            raise ValueError(
-                f"Object {obj} does not have attribute: {self.attribute}"
-            )
-
-    def set(self, obj: Any, val: Any) -> Any:
-        if obj is None:
-            obj = Bunch()
-
-        if hasattr(obj, self.attribute):
-            setattr(obj, self.attribute, val)
-            return obj
-        else:
-            # might fail
-            setattr(obj, self.attribute, val)
-            return obj
-
-    def __repr__(self):
-        return f".{self.attribute}"
-
-
-class GetIndex(Step):
-    index: int
-
-    def __hash__(self):
-        return hash(self.index)
-
-    def __call__(self, obj: Sequence[T]) -> Iterable[T]:
-        if isinstance(obj, Sequence):
-            if len(obj) > self.index:
-                yield obj[self.index]
-            else:
-                raise IndexError(f"Index out of bounds: {self.index}")
-        else:
-            raise ValueError(f"Object {obj} is not a sequence.")
-
-    def set(self, obj: Any, val: Any) -> Any:
-        if obj is None:
-            obj = []
-
-        assert isinstance(obj, Sequence), "Sequence expected."
-
-        if self.index >= 0:
-            while len(obj) <= self.index:
-                obj.append(None)
-
-        obj[self.index] = val
-        return obj
-
-    def __repr__(self):
-        return f"[{self.index}]"
-
-
-class GetItem(Step):
-    item: str
-
-    def __hash__(self):
-        return hash(self.item)
-
-    def __call__(self, obj: Dict[str, T]) -> Iterable[T]:
-        if isinstance(obj, Dict):
-            if self.item in obj:
-                yield obj[self.item]
-            else:
-                raise KeyError(f"Key not in dictionary: {self.item}")
-        else:
-            raise ValueError(f"Object {obj} is not a dictionary.")
-
-    def set(self, obj: Any, val: Any) -> Any:
-        if obj is None:
-            obj = dict()
-
-        assert isinstance(obj, Dict), "Dictionary expected."
-
-        obj[self.item] = val
-        return obj
-
-    def __repr__(self):
-        return f"[{repr(self.item)}]"
-
-
-class GetItemOrAttribute(Step):
-    # For item/attribute agnostic addressing.
-
-    item_or_attribute: str  # distinct from "item" for deserialization
-
-    def __hash__(self):
-        return hash(self.item)
-
-    def __call__(self, obj: Dict[str, T]) -> Iterable[T]:
-        if isinstance(obj, Dict):
-            if self.item_or_attribute in obj:
-                yield obj[self.item_or_attribute]
-            else:
-                raise KeyError(
-                    f"Key not in dictionary: {self.item_or_attribute}"
-                )
-        else:
-            if hasattr(obj, self.item_or_attribute):
-                yield getattr(obj, self.item_or_attribute)
-            else:
-                raise ValueError(
-                    f"Object {obj} does not have item or attribute {self.item_or_attribute}."
-                )
-
-    def set(self, obj: Any, val: Any) -> Any:
-        if obj is None:
-            obj = dict()
-
-        if isinstance(obj, Dict):
-            obj[self.item_or_attribute] = val
-        else:
-            setattr(obj, self.item_or_attribute)
-
-        return obj
-
-    def __repr__(self):
-        return f".{self.item_or_attribute}"
-
-
-class GetSlice(Step):
-    start: Optional[int]
-    stop: Optional[int]
-    step: Optional[int]
-
-    def __hash__(self):
-        return hash((self.start, self.stop, self.step))
-
-    def __call__(self, obj: Sequence[T]) -> Iterable[T]:
-        if isinstance(obj, Sequence):
-            lower, upper, step = slice(self.start, self.stop,
-                                       self.step).indices(len(obj))
-            for i in range(lower, upper, step):
-                yield obj[i]
-        else:
-            raise ValueError("Object is not a sequence.")
-
-    def set(self, obj: Any, val: Any) -> Any:
-        if obj is None:
-            obj = []
-
-        assert isinstance(obj, Sequence), "Sequence expected."
-
-        lower, upper, step = slice(self.start, self.stop,
-                                   self.step).indices(len(obj))
-
-        for i in range(lower, upper, step):
-            obj[i] = val
-
-        return obj
-
-    def __repr__(self):
-        pieces = ":".join(
-            [
-                "" if p is None else str(p)
-                for p in (self.start, self.stop, self.step)
-            ]
-        )
-        if pieces == "::":
-            pieces = ":"
-
-        return f"[{pieces}]"
-
-
-class GetIndices(Step):
-    indices: Sequence[int]
-
-    def __hash__(self):
-        return hash(tuple(self.indices))
-
-    def __call__(self, obj: Sequence[T]) -> Iterable[T]:
-        if isinstance(obj, Sequence):
-            for i in self.indices:
-                yield obj[i]
-        else:
-            raise ValueError("Object is not a sequence.")
-
-    def set(self, obj: Any, val: Any) -> Any:
-        if obj is None:
-            obj = []
-
-        assert isinstance(obj, Sequence), "Sequence expected."
-
-        for i in self.indices:
-            if i >= 0:
-                while len(obj) <= i:
-                    obj.append(None)
-
-            obj[i] = val
-
-        return obj
-
-    def __repr__(self):
-        return f"[{','.join(map(str, self.indices))}]"
-
+    @dataclass
+    class GetAttribute(Step):
+        attribute: str
+
+    @dataclass
+    class GetItem(Step):
+        item: Union[str, int]
 
-class GetItems(Step):
-    items: Sequence[str]
-
-    def __hash__(self):
-        return hash(tuple(self.items))
-
-    def __call__(self, obj: Dict[str, T]) -> Iterable[T]:
-        if isinstance(obj, Dict):
-            for i in self.items:
-                yield obj[i]
-        else:
-            raise ValueError("Object is not a dictionary.")
-
-    def set(self, obj: Any, val: Any) -> Any:
-        if obj is None:
-            obj = dict()
-
-        assert isinstance(obj, Dict), "Dictionary expected."
-
-        for i in self.items:
-            obj[i] = val
-
-        return obj
-
-    def __repr__(self):
-        return f"[{','.join(self.indices)}]"
-
-
-class JSONPath(SerialModel):
-    """
-    Utilitiy class for building JSONPaths.
-
-    Usage:
-    
-    ```python
-
-        JSONPath().record[5]['somekey]
-    ```
-    """
-
-    path: Tuple[Step, ...]
-
-    def __init__(self, path: Optional[Tuple[Step, ...]] = None):
-
-        super().__init__(path=path or ())
-
-    def __str__(self):
-        return "*" + ("".join(map(repr, self.path)))
-
-    def __repr__(self):
-        return "JSONPath()" + ("".join(map(repr, self.path)))
-
-    def __hash__(self):
-        return hash(self.path)
-
-    def __len__(self):
-        return len(self.path)
-
-    #@staticmethod
-    #def parse_obj(d):
-    #    path = tuple(map(Step.parse_obj, d['path']))
-    #    return JSONPath(path=path)
-
-    def set(self, obj: Any, val: Any) -> Any:
-        if len(self.path) == 0:
-            return val
-
-        first = self.path[0]
-        rest = JSONPath(path=self.path[1:])
-
-        try:
-            firsts = first(obj)
-            first_obj, firsts = iterable_peek(firsts)
-
-        except (ValueError, IndexError, KeyError, AttributeError):
-
-            # `first` points to an element that does not exist, use `set` to create a spot for it.
-            obj = first.set(obj, None)  # will create a spot for `first`
-            firsts = first(obj)
-
-        for first_obj in firsts:
-            obj = first.set(
-                obj,
-                rest.set(first_obj, val),
-            )
-
-        return obj
-
-    def get_sole_item(self, obj: Any) -> Any:
-        return next(self.__call__(obj))
-
-    def __call__(self, obj: Any) -> Iterable[Any]:
-        if len(self.path) == 0:
-            yield obj
-            return
-
-        first = self.path[0]
-        if len(self.path) == 1:
-            rest = JSONPath(path=())
-        else:
-            rest = JSONPath(path=self.path[1:])
+    class Aggregate(Step):
+        pass
 
-        for first_selection in first.__call__(obj):
-            for rest_selection in rest.__call__(first_selection):
-                yield rest_selection
-
-    def _append(self, step: Step) -> JSONPath:
-        return JSONPath(path=self.path + (step,))
-
-    def __getitem__(
-        self, item: int | str | slice | Sequence[int] | Sequence[str]
-    ) -> JSONPath:
-        if isinstance(item, int):
-            return self._append(GetIndex(index=item))
-        if isinstance(item, str):
-            return self._append(GetItemOrAttribute(item_or_attribute=item))
-        if isinstance(item, slice):
-            return self._append(
-                GetSlice(start=item.start, stop=item.stop, step=item.step)
-            )
-        if isinstance(item, Sequence):
-            item = tuple(item)
-            if all(isinstance(i, int) for i in item):
-                return self._append(GetIndices(indices=item))
-            elif all(isinstance(i, str) for i in item):
-                return self._append(GetItems(items=item))
-            else:
-                raise TypeError(
-                    f"Unhandled sequence item types: {list(map(type, item))}. "
-                    f"Note mixing int and str is not allowed."
-                )
+    def __init__(self):
+        self._path = []
 
-        raise TypeError(f"Unhandled item type {type(item)}.")
+    def __call__(self, json: Dict) -> Union[Any, Sequence[Any], Dict[Any, Any]]:
+        pass
 
-    def __getattr__(self, attr: str) -> JSONPath:
-        return self._append(GetItemOrAttribute(item_or_attribute=attr))
+    def _agg(self, aggregator: Callable) -> Any:
+        pass
 
+    def __getitem__(self, index: int) -> 'JLens':
+        pass
 
-# Python utilities
+    def __getattribute__(self, name: str) -> 'JLens':
+        pass
 
 
 class SingletonPerName():
     """
     Class for creating singleton instances except there being one instance max,
     there is one max per different `name` argument. If `name` is never given,
     reverts to normal singleton behaviour.
@@ -904,31 +82,24 @@
         """
         Create the singleton instance if it doesn't already exist and return it.
         """
 
         key = cls.__name__, name
 
         if key not in cls.instances:
-            logger.debug(
+            logging.debug(
                 f"*** Creating new {cls.__name__} singleton instance for name = {name} ***"
             )
             SingletonPerName.instances[key] = super().__new__(cls)
 
         return SingletonPerName.instances[key]
 
 
-# Threading utilities
-
-
 class TP(SingletonPerName):  # "thread processing"
 
-    # Store here stacks of calls to various thread starting methods so that we can retrieve
-    # the trace of calls that caused a thread to start.
-    # pre_run_stacks = dict()
-
     def __init__(self):
         if hasattr(self, "thread_pool"):
             # Already initialized as per SingletonPerName mechanism.
             return
 
         # TODO(piotrm): if more tasks than `processes` get added, future ones
         # will block and earlier ones may never start executing.
@@ -941,126 +112,48 @@
         def runner():
             while True:
                 func(*args, **kwargs)
                 sleep(60 / rpm)
 
         self.runlater(runner)
 
-    @staticmethod
-    def _thread_target_wrapper(stack, func, *args, **kwargs):
-        """
-        Wrapper for a function that is started by threads. This is needed to
-        record the call stack prior to thread creation as in python threads do
-        not inherit the stack. Our instrumentation, however, relies on walking
-        the stack and need to do this to the frames prior to thread starts.
-        """
-
-        # Keep this for looking up via get_local_in_call_stack .
-        pre_start_stack = stack
-
-        return func(*args, **kwargs)
-
-    def _thread_starter(self, func, args, kwargs):
-        present_stack = stack()
-
-        prom = self.thread_pool.apply_async(
-            self._thread_target_wrapper,
-            args=(present_stack, func) + args,
-            kwds=kwargs
-        )
-        return prom
-
-        # thread = Thread(target=func, args=args, kwargs=kwargs)
-        # thread.start()
-        # self.pre_run_stacks[thread_id] = stack()
-
     def runlater(self, func: Callable, *args, **kwargs) -> None:
-        # prom = self.thread_pool.apply_async(func, args=args, kwds=kwargs)
-        prom = self._thread_starter(func, args, kwargs)
+        prom = self.thread_pool.apply_async(func, args=args, kwds=kwargs)
         self.promises.put(prom)
 
     def promise(self, func: Callable[..., T], *args, **kwargs) -> AsyncResult:
-        # prom = self.thread_pool.apply_async(func, args=args, kwds=kwargs)
-        prom = self._thread_starter(func, args, kwargs)
+        prom = self.thread_pool.apply_async(func, args=args, kwds=kwargs)
         self.promises.put(prom)
 
         return prom
 
     def finish(self, timeout: Optional[float] = None) -> int:
-        logger.debug(f"Finishing {self.promises.qsize()} task(s).")
+        print(f"Finishing {self.promises.qsize()} task(s) ", end='')
 
         timeouts = []
 
         while not self.promises.empty():
             prom = self.promises.get()
             try:
                 prom.get(timeout=timeout)
+                print(".", end="")
             except TimeoutError:
+                print("!", end="")
                 timeouts.append(prom)
 
         for prom in timeouts:
             self.promises.put(prom)
 
         if len(timeouts) == 0:
-            logger.debug("Done.")
+            print("done.")
         else:
-            logger.debug("Some tasks timed out.")
+            print("some tasks timed out.")
 
         return len(timeouts)
 
     def _status(self) -> List[str]:
         rows = []
 
         for p in self.thread_pool._pool:
             rows.append([p.is_alive(), str(p)])
 
         return pd.DataFrame(rows, columns=["alive", "thread"])
-
-
-def get_local_in_call_stack(
-    key: str,
-    func: Callable[[Callable], bool],
-    offset: int = 1
-) -> Optional[Any]:
-    """
-    Get the value of the local variable named `key` in the stack at the nearest
-    frame executing a function which `func` recognizes (returns True on).
-    Returns None if `func` does not recognize the correct function. Raises
-    RuntimeError if a function is recognized but does not have `key` in its
-    locals.
-
-    This method works across threads as long as they are started using the TP
-    class above.
-
-    """
-
-    frames = stack()[offset + 1:]  # + 1 to skip this method itself
-
-    # Using queue for frames as additional frames may be added due to handling threads.
-    q = Queue()
-    for f in frames:
-        q.put(f)
-
-    while not q.empty():
-        fi = q.get()
-
-        if id(fi.frame.f_code) == id(TP()._thread_target_wrapper.__code__):
-            logger.debug(
-                "Found thread starter frame. "
-                "Will walk over frames in prior to thread start."
-            )
-            locs = fi.frame.f_locals
-            assert "pre_start_stack" in locs, "Pre thread start stack expected but not found."
-            for f in locs['pre_start_stack']:
-                q.put(f)
-            continue
-
-        if func(fi.frame.f_code):
-            logger.debug(f"looking {func.__name__} found: {fi}")
-            locs = fi.frame.f_locals
-            if key in locs:
-                return locs[key]
-            else:
-                raise RuntimeError(f"No local named {key} in {func} found.")
-        logger.debug(f"looking {func.__name__}, pass : {fi}")
-
-    return None
```

## trulens_eval/pages/Evaluations.py

```diff
@@ -1,32 +1,25 @@
 import json
 from typing import Dict, List
 
 import matplotlib.pyplot as plt
-import numpy as np
 import pandas as pd
 from st_aggrid import AgGrid
 from st_aggrid.grid_options_builder import GridOptionsBuilder
 from st_aggrid.shared import GridUpdateMode
 from st_aggrid.shared import JsCode
 import streamlit as st
-from trulens_eval.schema import Record
-from trulens_eval.util import GetItemOrAttribute
 from ux.add_logo import add_logo
 
-import streamlit.components.v1 as components
-
 from trulens_eval import Tru
 from trulens_eval import tru_db
-from trulens_eval.util import is_empty, matching_objects
-from trulens_eval.util import is_noserio
+from trulens_eval.tru_db import is_empty
+from trulens_eval.tru_db import is_noserio
 from trulens_eval.tru_db import TruDB
-from trulens_eval.ux.components import draw_calls
-from trulens_eval.ux.styles import cellstyle_jscode
-from trulens_eval.tru_feedback import default_pass_fail_color_threshold
+from trulens_eval.ux.components import render_calls
 
 st.set_page_config(page_title="Evaluations", layout="wide")
 
 st.title("Evaluations")
 
 st.runtime.legacy_caching.clear_cache()
 
@@ -43,65 +36,80 @@
 else:
     chains = list(df_results.chain_id.unique())
     if 'chain' in st.session_state:
         chain = st.session_state.chain
     else:
         chain = chains
 
-    options = st.multiselect('Filter Applications', chains, default=chain)
+    options = st.multiselect('Filter Chains', chains, default=chain)
 
     if (len(options) == 0):
-        st.header("All Applications")
+        st.header("All Chains")
         chain_df = df_results
 
     elif (len(options) == 1):
         st.header(options[0])
 
         chain_df = df_results[df_results.chain_id.isin(options)]
 
     else:
-        st.header("Multiple Applications Selected")
+        st.header("Multiple Chains Selected")
 
         chain_df = df_results[df_results.chain_id.isin(options)]
 
     tab1, tab2 = st.tabs(["Records", "Feedback Functions"])
 
     with tab1:
         gridOptions = {'alwaysShowHorizontalScroll': True}
         evaluations_df = chain_df
         gb = GridOptionsBuilder.from_dataframe(evaluations_df)
 
-        cellstyle_jscode = JsCode(cellstyle_jscode)
+        cellstyle_jscode = JsCode(
+            """
+        function(params) {
+            if (parseFloat(params.value) < 0.5) {
+                return {
+                    'color': 'black',
+                    'backgroundColor': '#FCE6E6'
+                }
+            } else if (parseFloat(params.value) >= 0.5) {
+                return {
+                    'color': 'black',
+                    'backgroundColor': '#4CAF50'
+                }
+            } else {
+                return {
+                    'color': 'black',
+                    'backgroundColor': 'white'
+                }
+            }
+        };
+        """
+        )
 
         gb.configure_column('record_json', header_name='Record JSON', hide=True)
         gb.configure_column('chain_json', header_name='Chain JSON', hide=True)
-        gb.configure_column('cost_json', header_name='Cost JSON', hide=True)
 
         gb.configure_column('record_id', header_name='Record ID', hide=True)
         gb.configure_column('chain_id', header_name='Chain ID')
         gb.configure_column('feedback_id', header_name='Feedback ID', hide=True)
         gb.configure_column('input', header_name='User Input')
         gb.configure_column(
             'output',
             header_name='Response',
         )
-        gb.configure_column('total_tokens', header_name='Total Tokens (#)')
-        gb.configure_column('total_cost', header_name='Total Cost (USD)')
+        gb.configure_column('total_tokens', header_name='Total Tokens')
+        gb.configure_column('total_cost', header_name='Total Cost')
         gb.configure_column('tags', header_name='Tags')
         gb.configure_column('ts', header_name='Time Stamp')
 
         for feedback_col in evaluations_df.columns.drop(['chain_id', 'ts',
                                                          'total_tokens',
                                                          'total_cost']):
-            gb.configure_column(
-                feedback_col,
-                cellStyle=cellstyle_jscode,
-                hide=feedback_col.endswith("_calls")
-            )
-
+            gb.configure_column(feedback_col, cellStyle=cellstyle_jscode)
         gb.configure_pagination()
         gb.configure_side_bar()
         gb.configure_selection(selection_mode="single", use_checkbox=False)
 
         #gb.configure_default_column(groupable=True, value=True, enableRowGroup=True, aggFunc="sum", editable=True)
         gridOptions = gb.build()
         data = AgGrid(
@@ -114,103 +122,59 @@
         selected_rows = data['selected_rows']
         selected_rows = pd.DataFrame(selected_rows)
 
         if len(selected_rows) == 0:
             st.write("Hint: select a row to display chain metadata")
 
         else:
-            st.header(
-                f"Selected LLM Application: {selected_rows['chain_id'][0]}"
-            )
+            st.header(f"Selected Chain ID: {selected_rows['chain_id'][0]}")
             st.text(f"Selected Record ID: {selected_rows['record_id'][0]}")
-
             prompt = selected_rows['input'][0]
             response = selected_rows['output'][0]
-
             with st.expander("Input Prompt", expanded=True):
                 st.write(prompt)
 
             with st.expander("Response", expanded=True):
                 st.write(response)
 
-            row = selected_rows.head().iloc[0]
-
-            st.header("Feedback")
-            for fcol in feedback_cols:
-                feedback_name = fcol
-                feedback_result = row[fcol]
-                feedback_calls = row[f"{fcol}_calls"]
-
-                def display_feedback_call(call):
-
-                    def highlight(s):
-                        return ['background-color: #4CAF50'] * len(
-                            s
-                        ) if s.result >= default_pass_fail_color_threshold else [
-                            'background-color: #FCE6E6'
-                        ] * len(s)
-
-                    if (len(call) > 0):
-                        df = pd.DataFrame.from_records(
-                            [call[i]["args"] for i in range(len(call))]
-                        )
-                        df["result"] = pd.DataFrame(
-                            [float(call[i]["ret"]) for i in range(len(call))]
-                        )
-                        st.dataframe(
-                            df.style.apply(highlight, axis=1
-                                          ).format("{:.2}", subset=["result"])
-                        )
-                    else:
-                        st.text("No feedback details.")
-
-                with st.expander(f"{feedback_name} = {feedback_result}",
-                                 expanded=True):
-                    display_feedback_call(feedback_calls)
-
             record_str = selected_rows['record_json'][0]
             record_json = json.loads(record_str)
-            record = Record(**record_json)
+
+            st.header("Call Trace")
+            render_calls(record_json)
 
             details = selected_rows['chain_json'][0]
-            chain_json = json.loads(
-                details
-            )  # chains may not be deserializable, don't try to, keep it json.
-
-            step_llm = GetItemOrAttribute(item_or_attribute="llm")
-            step_prompt = GetItemOrAttribute(item_or_attribute="prompt")
-            step_call = GetItemOrAttribute(item_or_attribute="_call")
+            details_json = json.loads(details)
+            #json.loads(details))  # ???
+
+            chain_json = details_json['chain']
 
             llm_queries = list(
-                matching_objects(
-                    chain_json,
-                    match=lambda q, o: len(q.path) > 0 and step_llm == q.path[-1
-                                                                             ]
+                TruDB.matching_objects(
+                    details_json,
+                    match=lambda q, o: len(q._path) > 0 and "llm" == q._path[-1]
                 )
             )
 
             prompt_queries = list(
-                matching_objects(
-                    chain_json,
-                    match=lambda q, o: len(q.path) > 0 and step_prompt == q.
-                    path[-1] and step_call not in q._path
+                TruDB.matching_objects(
+                    details_json,
+                    match=lambda q, o: len(q._path) > 0 and "prompt" == q._path[
+                        -1] and "_call" not in q._path
                 )
             )
 
             max_len = max(len(llm_queries), len(prompt_queries))
 
-            for i in range(max_len + 1):
-                st.header(f"Component {i+1}")
-                draw_calls(record, index=i + 1)
-
+            for i in range(max_len):
                 if i < len(llm_queries):
                     query, llm_details_json = llm_queries[i]
+                    path_str = TruDB._query_str(query)
+                    st.header(f"Chain Step {i}: {path_str.replace('.llm', '')}")
                     st.subheader(f"LLM Details:")
-                    path_str = str(query)
-                    st.text(path_str[:-4])
 
                     llm_kv = {
                         k: v
                         for k, v in llm_details_json.items()
                         if (v is not None) and not is_empty(v) and
                         not is_noserio(v)
                     }
@@ -250,17 +214,16 @@
                             )
                     # Inject CSS with Markdown
                     st.markdown(hide_table_row_index, unsafe_allow_html=True)
                     st.table(df)
 
                 if i < len(prompt_queries):
                     query, prompt_details_json = prompt_queries[i]
-                    path_str = str(query)
+                    path_str = TruDB._query_str(query)
                     st.subheader(f"Prompt Details:")
-                    st.text(path_str)
 
                     prompt_types = {
                         k: v
                         for k, v in prompt_details_json.items()
                         if (v is not None) and not is_empty(v) and
                         not is_noserio(v)
                     }
@@ -270,19 +233,18 @@
                             if isinstance(value, (Dict, List)):
                                 st.write(value)
                             else:
                                 if isinstance(value, str) and len(value) > 32:
                                     st.text(value)
                                 else:
                                     st.write(value)
-
             st.header("More options:")
             if st.button("Display full chain json"):
 
-                st.write(chain_json)
+                st.write(details_json)
 
             if st.button("Display full record json"):
 
                 st.write(record_json)
 
     with tab2:
         feedback = feedback_cols
```

## trulens_eval/pages/Progress.py

```diff
@@ -1,29 +1,29 @@
 from datetime import datetime
 import json
 from typing import Dict, List
 
+from trulens_eval.keys import *
+
 import pandas as pd
 from st_aggrid import AgGrid
 from st_aggrid.grid_options_builder import GridOptionsBuilder
 from st_aggrid.shared import GridUpdateMode
 from st_aggrid.shared import JsCode
 import streamlit as st
-from trulens_eval.schema import FeedbackResultStatus
+from trulens_eval.tru_feedback import Feedback
+from trulens_eval.util import TP
 from ux.add_logo import add_logo
+from trulens_eval.tru_db import is_empty
 
-from trulens_eval import Tru
-from trulens_eval import tru_db
-from trulens_eval.keys import *
+from trulens_eval import tru_db, Tru
 from trulens_eval.provider_apis import Endpoint
+
+from trulens_eval.tru_db import is_noserio
 from trulens_eval.tru_db import TruDB
-from trulens_eval.tru_feedback import Feedback
-from trulens_eval.util import is_empty
-from trulens_eval.util import is_noserio
-from trulens_eval.util import TP
 
 st.set_page_config(page_title="Feedback Progress", layout="wide")
 
 st.title("Feedback Progress")
 
 st.runtime.legacy_caching.clear_cache()
 
@@ -37,20 +37,15 @@
 e_cohere = Endpoint("cohere")
 
 endpoints = [e_openai, e_hugs, e_cohere]
 
 tab1, tab2, tab3 = st.tabs(["Progress", "Endpoints", "Feedback Functions"])
 
 with tab1:
-    feedbacks = lms.get_feedback(
-        status=[
-            FeedbackResultStatus.NONE, FeedbackResultStatus.RUNNING,
-            FeedbackResultStatus.FAILED
-        ]
-    )
+    feedbacks = lms.get_feedback(status=[-1, 0, 1])
     st.write(feedbacks)
 
 with tab2:
     for e in endpoints:
         st.header(e.name.upper())
         st.metric("RPM", e.rpm)
         st.write(e.tqdm)
```

## trulens_eval/utils/langchain.py

```diff
@@ -1,54 +1,43 @@
 from typing import Callable, List
 
 from langchain.schema import Document
 from langchain.vectorstores.base import VectorStoreRetriever
 from pydantic import Field
-from trulens_eval.tru_feedback import Feedback
 
 from trulens_eval.util import TP, first, second
 
 
-class WithFeedbackFilterDocuments(VectorStoreRetriever):
-    feedback: Feedback
-    threshold: float
+class WithFilterDocuments(VectorStoreRetriever):
+    filter_func: Callable = Field(exclude=True)
 
-    def __init__(self, feedback: Feedback, threshold: float, *args, **kwargs):
+    def __init__(
+        self, filter_func: Callable[[Document], bool], *args, **kwargs
+    ):
         """
-        A VectorStoreRetriever that filters documents using a minimum threshold
-        on a feedback function before returning them.
+        A VectorStoreRetriever that filters documents before returning them.
 
-        - feedback: Feedback - use this feedback function to score each
-          document.
-        
-        - threshold: float - and keep documents only if their feedback value is
-          at least this threshold.
+        - filter_func: Callable[[Document], bool] - apply this filter before
+          returning documents. Will return only documents for which the filter
+          returns true.
         """
 
-        super().__init__(
-            feedback=feedback, threshold=threshold, *args, **kwargs
-        )
+        super().__init__(filter_func=filter_func, *args, **kwargs)
 
     def get_relevant_documents(self, query: str) -> List[Document]:
         # Get relevant docs using super class:
         docs = super().get_relevant_documents(query)
 
         # Evaluate the filter on each, in parallel.
         promises = (
-            (
-                doc, TP().promise(
-                    lambda doc, query: self.feedback(query, doc.page_content) >
-                    self.threshold,
-                    query=query,
-                    doc=doc
-                )
-            ) for doc in docs
+            (doc, TP().promise(self.filter_func, query=query, doc=doc))
+            for doc in docs
         )
         results = ((doc, promise.get()) for (doc, promise) in promises)
         filtered = map(first, filter(second, results))
 
         # Return only the filtered ones.
         return list(filtered)
 
     @staticmethod
-    def of_retriever(retriever: VectorStoreRetriever, **kwargs):
-        return WithFeedbackFilterDocuments(**kwargs, **retriever.dict())
+    def of_retriever(retriever: VectorStoreRetriever, filter_func: Callable):
+        return WithFilterDocuments(filter_func=filter_func, **retriever.dict())
```

## trulens_eval/ux/components.py

```diff
@@ -1,52 +1,36 @@
 from typing import Dict
-
 import streamlit as st
-
-from trulens_eval.schema import Record
-from trulens_eval.schema import RecordChainCall
-from trulens_eval.tru_db import get_calls_by_stack
-from trulens_eval.tru_db import JSON
-
-
-def render_call_frame(frame: RecordChainCall) -> str:  # markdown
-
-    return (
-        f"{frame.path}.___{frame.method.method_name}___\n"
-        f"(`{frame.method.module_name}.{frame.method.class_name}`)"
-    )
+from trulens_eval.tru_db import JSON, get_calls_by_stack
 
 
-def draw_calls(record: Record, index: int) -> None:
-    """
-    Draw the calls recorded in a `record`.
-    """
+def render_call_frame(frame_json: JSON):
+    return f"{'.'.join(frame_json['path'])}.___{frame_json['method_name']}___\n(`{frame_json['module_name']}.{frame_json['class_name']}`)"
 
-    calls = record.calls
 
-    chain_step = 0
+def render_calls(record_json: JSON):
+    calls = get_calls_by_stack(record_json)
+    for call_stack, calls_json in calls.items():
+        for call_json in calls_json:
 
-    for call in calls:
-        chain_step += 1
-        top = call.chain_stack[-1]
+            args = call_json['args']
+            rets = call_json['rets']
 
-        if chain_step != index:
-            continue
+            frame = call_stack[-1]
 
-        with st.expander(label=render_call_frame(top)):
-            args = call.args
-            rets = call.rets
+            with st.expander(label=render_call_frame(frame)):
+                #st.header(render_call_frame(frame))
 
-            for frame in call.chain_stack[0:-2]:
-                st.write("Via " + render_call_frame(frame))
+                # for frame in call_stack[0:-2]:
+                #     st.write(render_call_frame(frame))
 
-            st.subheader(f"Inputs:")
-            if isinstance(args, Dict):
-                st.json(args)
-            else:
-                st.write(args)
+                st.subheader(f"Inputs:")
+                if isinstance(args, Dict):
+                    st.json(args)
+                else:
+                    st.write(args)
 
-            st.subheader(f"Outputs:")
-            if isinstance(rets, Dict):
-                st.json(rets)
-            else:
-                st.write(rets)
+                st.subheader(f"Outputs:")
+                if isinstance(rets, Dict):
+                    st.json(rets)
+                else:
+                    st.write(rets)
```

## Comparing `trulens_eval-0.1.2.dist-info/METADATA` & `trulens_eval-0.1.2a0.dist-info/METADATA`

 * *Files 18% similar despite different names*

```diff
@@ -1,16 +1,15 @@
 Metadata-Version: 2.1
 Name: trulens-eval
-Version: 0.1.2
+Version: 0.1.2a0
 Summary: Library with langchain instrumentation to evaluate LLM based applications.
 Home-page: https://www.trulens.org
 Author: Truera Inc
 Author-email: all@truera.com
 License: MIT
-Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
 Classifier: Operating System :: OS Independent
 Classifier: Development Status :: 3 - Alpha
 Classifier: License :: OSI Approved :: MIT License
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 Requires-Dist: cohere (>=4.4.1)
@@ -30,372 +29,276 @@
 Requires-Dist: streamlit-aggrid (>=0.3.4.post3)
 Requires-Dist: streamlit-extras (>=0.2.7)
 Requires-Dist: tinydb (>=4.7.1)
 Requires-Dist: transformers (>=4.10.0)
 Requires-Dist: typing-inspect (==0.8.0)
 Requires-Dist: typing-extensions (==4.5.0)
 Requires-Dist: frozendict (>=2.3.8)
-Requires-Dist: munch (>=3.0.0)
 
 # Welcome to TruLens-Eval!
 
 ![TruLens](https://www.trulens.org/Assets/image/Neural_Network_Explainability.png)
 
 Evaluate and track your LLM experiments with TruLens. As you work on your models and prompts TruLens-Eval supports the iterative development and of a wide range of LLM applications by wrapping your application to log key metadata across the entire chain (or off chain if your project does not use chains) on your local machine.
 
-Using feedback functions, you can objectively evaluate the quality of the responses provided by an LLM to your requests. This is completed with minimal latency, as this is achieved in a sequential call for your application, and evaluations are logged to your local machine. Finally, we provide an easy to use Streamlit dashboard run locally on your machine for you to better understand your LLM’s performance.
+Using feedback functions, you can objectively evaluate the quality of the responses provided by an LLM to your requests. This is completed with minimal latency, as this is achieved in a sequential call for your application, and evaluations are logged to your local machine. Finally, we provide an easy to use streamlit dashboard run locally on your machine for you to better understand your LLM’s performance.
 
 ![Architecture Diagram](https://www.trulens.org/Assets/image/TruLens_Architecture.png)
 
-## Quick Usage
-To quickly play around with the TruLens Eval library, download this notebook: [quickstart.ipynb](https://github.com/truera/trulens/blob/main/docs/trulens_eval/quickstart.ipynb).
+# Quick Usage
+To quickly play around with the TruLens Eval library, download this notebook: [trulens_eval_quickstart.ipynb](https://github.com/truera/trulens/blob/main/trulens_eval/trulens_eval_quickstart.ipynb).
 
 
 
-## Installation and Setup
+# Installation and Setup
 
-Install the trulens-eval pip package from PyPI.
+Install trulens-eval from pypi.
 
-```bash
-    pip install trulens-eval
 ```
-
-### API Keys
-
-Our example chat app and feedback functions call external APIs such as OpenAI or HuggingFace. You can add keys by setting the environment variables. 
-
-#### In Python
-
-```python
-import os
-os.environ["OPENAI_API_KEY"] = "..."
+pip install trulens-eval
 ```
 
-#### In Terminal
+Imports from langchain to build app, trulens for evaluation
 
-```bash
-export OPENAI_API_KEY = "..."
+```python
+from IPython.display import JSON
+# imports from langchain to build app
+from langchain import PromptTemplate
+from langchain.chains import LLMChain
+from langchain.chat_models import ChatOpenAI
+from langchain.prompts.chat import ChatPromptTemplate
+from langchain.prompts.chat import HumanMessagePromptTemplate
+# imports from trulens to log and get feedback on chain
+from trulens_eval.tru import Tru
+from trulens_eval import tru_chain
+tru = Tru()
 ```
 
+## API Keys
 
-# Quickstart
-
-In this quickstart you will create a simple LLM Chain and learn how to log it and get feedback on an LLM response.
-
-## Setup
-### Add API keys
-For this quickstart you will need Open AI and Huggingface keys
+Our example chat app and feedback functions call external APIs such as OpenAI or Huggingface. You can add keys by setting the environment variables. 
 
+### In Python
 
 ```python
 import os
 os.environ["OPENAI_API_KEY"] = "..."
-os.environ["HUGGINGFACE_API_KEY"] = "..."
 ```
+### In Terminal
 
-### Import from LangChain and TruLens
-
-
-```python
-from IPython.display import JSON
-
-# Imports main tools:
-from trulens_eval import TruChain, Feedback, Huggingface, Tru
-tru = Tru()
-
-# imports from langchain to build app
-from langchain.chains import LLMChain
-from langchain.llms import OpenAI
-from langchain.prompts.chat import ChatPromptTemplate, PromptTemplate
-from langchain.prompts.chat import HumanMessagePromptTemplate
+```bash
+export OPENAI_API_KEY = "..."
 ```
 
-### Create Simple LLM Application
-
-This example uses a LangChain framework and OpenAI LLM
+## Create a basic LLM chain to evaluate
 
+This example uses langchain and OpenAI, but the same process can be followed with any framework and model provider. Once you've created your chain, just call TruChain to wrap it. Doing so allows you to capture the chain metadata for logging.
 
 ```python
 full_prompt = HumanMessagePromptTemplate(
     prompt=PromptTemplate(
-        template=
-        "Provide a helpful response with relevant background information for the following: {prompt}",
-        input_variables=["prompt"],
+        template="Provide a helpful response with relevant background information for the following: {prompt}",
+            input_variables=["prompt"],
+        )
     )
-)
-
 chat_prompt_template = ChatPromptTemplate.from_messages([full_prompt])
 
-llm = OpenAI(temperature=0.9, max_tokens=128)
+chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.9)
 
-chain = LLMChain(llm=llm, prompt=chat_prompt_template, verbose=True)
+chain = LLMChain(llm=chat, prompt=chat_prompt_template)
+
+# wrap with truchain to instrument your chain
+tc = tru_chain.TruChain(chain)
 ```
 
-### Send your first request
+## Set up logging and instrumentation
 
+Make the first call to your LLM Application. The instrumented chain can operate like the original but can also produce a log or "record" of the chain execution.
 
 ```python
-prompt_input = '¿que hora es?'
+prompt_input = 'que hora es?'
+gpt3_response, record = tc.call_with_record(prompt_input)
 ```
 
+We can log the records but first we need to log the chain itself.
 
 ```python
-llm_response = chain(prompt_input)
+tru.add_chain(chain_json=tc.json)
+```
 
-display(llm_response)
+Now we can log the record:
+```python
+tru.add_record(
+    prompt=prompt_input, # prompt input
+    response=gpt3_response['text'], # LLM response
+    record_json=record # record is returned by the TruChain wrapper
+)
 ```
 
-## Initialize Feedback Function(s)
+# Evaluate Quality
 
+Following the request to your app, you can then evaluate LLM quality using feedback functions. This is completed in a sequential call to minimize latency for your application, and evaluations will also be logged to your local machine.
 
+To get feedback on the quality of your LLM, you can use any of the provided feedback functions or add your own.
+
+To assess your LLM quality, you can provide the feedback functions to tru.run_feedback() in a list as shown below. Here we'll just add a simple language match checker.
 ```python
+from trulens_eval.tru_feedback import Feedback, Huggingface
+
+os.environ["HUGGINGFACE_API_KEY"] = "..."
+
 # Initialize Huggingface-based feedback function collection class:
 hugs = Huggingface()
 
 # Define a language match feedback function using HuggingFace.
 f_lang_match = Feedback(hugs.language_match).on(
     text1="prompt", text2="response"
 )
-```
-
-## Instrument chain for logging with TruLens
-
-
-```python
-truchain = TruChain(chain,
-    chain_id='Chain3_ChatApplication',
-    feedbacks=[f_lang_match],
-    tru = tru)
-```
 
+# Run feedack functions. This might take a moment if the public api needs to load the language model used by the feedback function.
+feedback_result = f_lang_match.run_on_record(
+    chain_json=tc.json, record_json=record
+)
 
-```python
-# Instrumented chain can operate like the original:
-llm_response = truchain(prompt_input)
+JSON(feedback_result)
 
-display(llm_response)
+# We can also run a collection of feedback functions
+feedback_results = tru.run_feedback_functions(
+    record_json=record,
+    feedback_functions=[f_lang_match]
+)
+display(feedback_results)
 ```
 
-## Explore in a Dashboard
-
-
+After capturing feedback, you can then log it to your local database
 ```python
-tru.run_dashboard() # open a local streamlit app to explore
-
-# tru.run_dashboard(_dev=True) # if running from repo
-# tru.stop_dashboard() # stop if needed
-```
-
-### Chain Leaderboard
-
-Understand how your LLM application is performing at a glance. Once you've set up logging and evaluation in your application, you can view key performance statistics including cost and average feedback value across all of your LLM apps using the chain leaderboard. As you iterate new versions of your LLM application, you can compare their performance across all of the different quality metrics you've set up.
-
-Note: Average feedback values are returned and displayed in a range from 0 (worst) to 1 (best).
-
-![Chain Leaderboard](https://www.trulens.org/Assets/image/Leaderboard.png)
-
-To dive deeper on a particular chain, click "Select Chain".
-
-### Understand chain performance with Evaluations
-
-To learn more about the performance of a particular chain or LLM model, we can select it to view its evaluations at the record level. LLM quality is assessed through the use of feedback functions. Feedback functions are extensible methods for determining the quality of LLM responses and can be applied to any downstream LLM task. Out of the box we provide a number of feedback functions for assessing model agreement, sentiment, relevance and more.
-
-The evaluations tab provides record-level metadata and feedback on the quality of your LLM application.
-
-![Evaluations](https://www.trulens.org/Assets/image/Leaderboard.png)
-
-### Deep dive into full chain metadata
-
-Click on a record to dive deep into all of the details of your chain stack and underlying LLM, captured by tru_chain.
-
-![Explore a Chain](https://www.trulens.org/Assets/image/Chain_Explore.png)
-
-If you prefer the raw format, you can quickly get it using the "Display full chain json" or "Display full record json" buttons at the bottom of the page.
-
-Note: Feedback functions evaluated in the deferred manner can be seen in the "Progress" page of the TruLens dashboard.
-
-## Or view results directly in your notebook
-
-
-```python
-tru.get_records_and_feedback(chain_ids=[])[0] # pass an empty list of chain_ids to get all
+tru.add_feedback(feedback_results)
 ```
 
-# Logging
-
-## Automatic Logging
-
-The simplest method for logging with TruLens is by wrapping with TruChain and including the tru argument, as shown in the quickstart.
-
-This is done like so:
-
-
+## Automatic logging
+The above logging and feedback function evaluation steps can be done by TruChain.
 ```python
-truchain = TruChain(
+tc = tru_chain.TruChain(
     chain,
     chain_id='Chain1_ChatApplication',
+    feedbacks=[f_lang_match],
     tru=tru
 )
-truchain("This will be automatically logged.")
+# Note: providing `db: TruDB` causes the above constructor to log the wrapped chain in the database specified.
+# Note: any `feedbacks` specified here will be evaluated and logged whenever the chain is used.
+
+tc("This will be automatically logged.")
 ```
 
-Feedback functions can also be logged automatically by providing them in a list to the feedbacks arg.
+## Out-of-band Feedback evaluation
 
+In the above example, the feedback function evaluation is done in the same process as the chain evaluation. The alternative approach is the use the provided persistent evaluator started via `tru.start_deferred_feedback_evaluator`. Then specify the `feedback_mode` for `TruChain` as `deferred` to let the evaluator handle the feedback functions.
 
+For demonstration purposes, we start the evaluator here but it can be started in another process.
 ```python
-truchain = TruChain(
+tc: tru_chain.TruChain = tru_chain.TruChain(
     chain,
     chain_id='Chain1_ChatApplication',
-    feedbacks=[f_lang_match], # feedback functions
-    tru=tru
+    feedbacks=[f_lang_match],
+    tru=tru,
+    feedback_mode="deferred"
 )
-truchain("This will be automatically logged.")
-```
-
-## Manual Logging
 
-### Wrap with TruChain to instrument your chain
-
-
-```python
-tc = tru_chain.TruChain(chain, chain_id='Chain1_ChatApplication')
-```
-
-### Set up logging and instrumentation
-
-Making the first call to your wrapped LLM Application will now also produce a log or "record" of the chain execution.
-
-
-
-```python
-prompt_input = 'que hora es?'
-gpt3_response, record = tc(prompt_input)
-```
-
-We can log the records but first we need to log the chain itself.
-
-
-```python
-tru.add_chain(chain_json=truchain.json)
+tru.start_evaluator()
+tc("This will be logged by deferred evaluator.")
+tru.stop_evaluator()
 ```
 
-Then we can log the record:
-
 
+## Run the dashboard!
 ```python
-tru.add_record(
-    prompt=prompt_input, # prompt input
-    response=gpt3_response['text'], # LLM response
-    record_json=record # record is returned by the TruChain wrapper
-)
+tru.run_dashboard() # open a streamlit app to explore
+# tru.stop_dashboard() # stop if needed
 ```
 
-### Evaluate Quality
-
-Following the request to your app, you can then evaluate LLM quality using feedback functions. This is completed in a sequential call to minimize latency for your application, and evaluations will also be logged to your local machine.
-
-To get feedback on the quality of your LLM, you can use any of the provided feedback functions or add your own.
-
-To assess your LLM quality, you can provide the feedback functions to `tru.run_feedback()` in a list provided to `feedback_functions`.
-
+## Chain Leaderboard: Quickly identify quality issues.
 
+Understand how your LLM application is performing at a glance. Once you've set up logging and evaluation in your application, you can view key performance statistics including cost and average feedback value across all of your LLM apps using the chain leaderboard. As you iterate new versions of your LLM application, you can compare their performance across all of the different quality metrics you've set up.
 
-```python
-feedback_results = tru.run_feedback_functions(
-    record_json=record,
-    feedback_functions=[f_lang_match]
-)
-display(feedback_results)
-```
+Note: Average feedback values are returned and displayed in a range from 0 (worst) to 1 (best).
 
-After capturing feedback, you can then log it to your local database.
+![Chain Leaderboard](https://www.trulens.org/Assets/image/Leaderboard.png)
 
+To dive deeper on a particular chain, click "Select Chain".
 
-```python
-tru.add_feedback(feedback_results)
-```
+## Understand chain performance with Evaluations
+ 
+To learn more about the performance of a particular chain or LLM model, we can select it to view its evaluations at the record level. LLM quality is assessed through the use of feedback functions. Feedback functions are extensible methods for determining the quality of LLM responses and can be applied to any downstream LLM task. Out of the box we provide a number of feedback functions for assessing model agreement, sentiment, relevance and more.
 
-### Out-of-band Feedback evaluation
+The evaluations tab provides record-level metadata and feedback on the quality of your LLM application.
 
-In the above example, the feedback function evaluation is done in the same process as the chain evaluation. The alternative approach is the use the provided persistent evaluator started via `tru.start_deferred_feedback_evaluator`. Then specify the `feedback_mode` for `TruChain` as `deferred` to let the evaluator handle the feedback functions.
 
-For demonstration purposes, we start the evaluator here but it can be started in another process.
+![Evaluations](https://www.trulens.org/Assets/image/Evaluations.png)
 
+Click on a record to dive deep into all of the details of your chain stack and underlying LLM, captured by tru_chain.
 
-```python
-truchain: TruChain = TruChain(
-    chain,
-    chain_id='Chain1_ChatApplication',
-    feedbacks=[f_lang_match],
-    tru=tru,
-    feedback_mode="deferred"
-)
+![TruChain Details](https://www.trulens.org/Assets/image/Chain_Explore.png)
 
-tru.start_evaluator()
-truchain("This will be logged by deferred evaluator.")
-tru.stop_evaluator()
-```
+If you prefer the raw format, you can quickly get it using the "Display full chain json" or "Display full record json" buttons at the bottom of the page.
 
-# Out-of-the-box Feedback Functions
+## Out-of-the-box Feedback Functions
 See: <https://www.trulens.org/trulens_eval/api/tru_feedback/>
 
-## Relevance
+### Relevance
 
 This evaluates the *relevance* of the LLM response to the given text by LLM prompting.
 
 Relevance is currently only available with OpenAI ChatCompletion API.
 
-## Sentiment
+### Sentiment
 
 This evaluates the *positive sentiment* of either the prompt or response.
 
 Sentiment is currently available to use with OpenAI, HuggingFace or Cohere as the model provider.
 
 * The OpenAI sentiment feedback function prompts a Chat Completion model to rate the sentiment from 1 to 10, and then scales the response down to 0-1.
 * The HuggingFace sentiment feedback function returns a raw score from 0 to 1.
-* The Cohere sentiment feedback function uses the classification endpoint and a small set of examples stored in `feedback_prompts.py` to return either a 0 or a 1.
+* The Cohere sentiment feedback function uses the classification endpoint and a small set of examples stored in feedback_prompts.py to return either a 0 or a 1.
 
-## Model Agreement
+### Model Agreement
 
-Model agreement uses OpenAI to attempt an honest answer at your prompt with system prompts for correctness, and then evaluates the agreement of your LLM response to this model on a scale from 1 to 10. The agreement with each honest bot is then averaged and scaled from 0 to 1.
+Model agreement uses OpenAI to attempt an honest answer at your prompt with system prompts for correctness, and then evaluates the aggreement of your LLM response to this model on a scale from 1 to 10. The agreement with each honest bot is then averaged and scaled from 0 to 1.
 
-## Language Match
+### Language Match
 
 This evaluates if the language of the prompt and response match.
 
 Language match is currently only available to use with HuggingFace as the model provider. This feedback function returns a score in the range from 0 to 1, where 1 indicates match and 0 indicates mismatch.
 
-## Toxicity
+### Toxicity
 
 This evaluates the toxicity of the prompt or response.
 
 Toxicity is currently only available to be used with HuggingFace, and uses a classification endpoint to return a score from 0 to 1. The feedback function is negated as not_toxicity, and returns a 1 if not toxic and a 0 if toxic.
 
-## Moderation
+### Moderation
 
 The OpenAI Moderation API is made available for use as feedback functions. This includes hate, hate/threatening, self-harm, sexual, sexual/minors, violence, and violence/graphic. Each is negated (ex: not_hate) so that a 0 would indicate that the moderation rule is violated. These feedback functions return a score in the range 0 to 1.
 
 # Adding new feedback functions
 
-Feedback functions are an extensible framework for evaluating LLMs. You can add your own feedback functions to evaluate the qualities required by your application by updating `trulens_eval/tru_feedback.py`. If your contributions would be useful for others, we encourage you to contribute to TruLens!
+Feedback functions are an extensible framework for evaluating LLMs. You can add your own feedback functions to evaluate the qualities required by your application by updating trulens_eval/tru_feedback.py. If your contributions would be useful for others, we encourage you to contribute to trulens!
 
 Feedback functions are organized by model provider into Provider classes.
 
 The process for adding new feedback functions is:
 1. Create a new Provider class or locate an existing one that applies to your feedback function. If your feedback function does not rely on a model provider, you can create a standalone class:
 
-
 ```python
 class StandAlone(Provider):
     def __init__(self):
-        pass
+    pass
 ```
 
-2. Add a new feedback function method to your selected class. Your new method can either take a single text (str) as a parameter or both prompt (str) and response (str). It should return a float between 0 (worst) and 1 (best).
-
+2. Add a new feedback function method to your selected class. Your new method can either take a single text (str) as a parameter or both promopt (str) and response (str). It should return a float between 0 (worst) and 1 (best).
 
 ```python
 def feedback(self, text: str) -> float:
         """
         Describe how the model works
 
         Parameters:
@@ -403,9 +306,7 @@
             Can also be prompt (str) and response (str).
 
         Returns:
             float: A value between 0 (worst) and 1 (best).
         """
         return float
 ```
-
-
```

## Comparing `trulens_eval-0.1.2.dist-info/RECORD` & `trulens_eval-0.1.2a0.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -1,29 +1,28 @@
 trulens_eval/Example_TruBot.py,sha256=vUPkVRoR0DBTIyp7UifHP6TdEQU0ppI6YGFAfLOulHw,4782
-trulens_eval/Leaderboard.py,sha256=jDxZ7qpB5zb7oDf0PbCodYmBeBXm_jy-5lsXR_p4NVM,2890
-trulens_eval/__init__.py,sha256=09ojEKp5w2djcgeNtExL1TgrnXnOKNqBNODhSrPW5GI,749
+trulens_eval/Leaderboard.py,sha256=AEPQhBQkqG11WqcQoxtyGIaGkXsNIBrsRBlu9nL8pGw,2422
+trulens_eval/__init__.py,sha256=ZOO2u2rZt-FKf4P7sPppGUmSzekw09NF5OZPCeYwvuk,407
 trulens_eval/benchmark.py,sha256=LHVnqYTudYpOJ2iry7De41jLfO2FImZqprUlaUkOiKA,5401
 trulens_eval/feedback_prompts.py,sha256=DgW4_f_4g018tYNwca1D1taJhhdwaf2fDR9J8s0Upls,3443
-trulens_eval/keys.py,sha256=n8v1LSi9u25RZe1EkhNULdUmMrvupce9C96i0rNYcnE,985
-trulens_eval/provider_apis.py,sha256=Yq0iih0F9i6rZBiYvxXqLcgrsC_SRiYLYFe3EqAaMPY,3376
-trulens_eval/schema.py,sha256=_PdNyY6ry9X6lDQxPOmKHvcwXgbYXZV9W1rkuyMRqaE,16001
-trulens_eval/slackbot.py,sha256=w7T3_kI5C8bdhCxTB9GB3WQU9p7dQXo-Atp-3VlZkjs,11263
-trulens_eval/tru.py,sha256=e2ncPMZRK0IrXi2Seb39Qs8Opd2WsleqnWndHubmhGE,10308
-trulens_eval/tru_chain.py,sha256=SzMcXkkeTtlsbBdHgo-VzUjVwrzrOSGK8sPdM4DFgcg,20398
-trulens_eval/tru_db.py,sha256=jzwFDuv887zPBZ9xNjz8n8Hr2Hws9sSEO0cUoHw91FA,18525
-trulens_eval/tru_feedback.py,sha256=mMYGUpExCj8MH_7Kf9gJYgPbylhKNOQoNl7HTIkgTvw,28881
-trulens_eval/util.py,sha256=mpHDgPO2dwXT1rg-ajAhXbZ3skXN-iM5KWDReOSEk-0,28697
+trulens_eval/keys.py,sha256=5HwSGVImj1742PRYo0iV9SC_92VO-3B3f7vH8503fd4,949
+trulens_eval/provider_apis.py,sha256=Ed9-qEAJywdXage73NZIhuBBUAOrTqhLzMqIPvbWvuI,3341
+trulens_eval/slackbot.py,sha256=u180sBMLVLgI0KW3oQI8P3Tz5p89_-zcxVU7NqzWCtQ,11179
+trulens_eval/test_tru_chain.py,sha256=fEpZzCB1OaCfQ3AVe7yjShxxxFJrmR9E660o_47yZIU,5455
+trulens_eval/tru.py,sha256=IjwAsyOXkEFONizvRoHzygwW_G8HeN3gqUKrqSMI6Vo,10449
+trulens_eval/tru_chain.py,sha256=8oHTL2p4UdBCDvkVxA2-QflOzzgeS44gtJ2o3AIZWMI,22796
+trulens_eval/tru_db.py,sha256=u-3Dyo9XggCpwJLS4rU0alCRIJC980uGuGp80aUWf4g,29397
+trulens_eval/tru_feedback.py,sha256=yiX7DSlOquQ34rZIGoLhZ7u_pxbCe58tDCjdJZCrp9g,30628
+trulens_eval/util.py,sha256=-K_kx2jsYUPQb7Ar63qZlxOJptYFrdAw_lLDXn4Gw1c,4067
 trulens_eval/examples/App_TruBot.py,sha256=vUPkVRoR0DBTIyp7UifHP6TdEQU0ppI6YGFAfLOulHw,4782
 trulens_eval/examples/trubot.py,sha256=kaI-FR5hqhtTx9fd2hyQjbTGzEedM0Ic4eSfMSyrnqc,11074
-trulens_eval/pages/Evaluations.py,sha256=6UvmQQqrrobV_HyliqTwugR31VKqdgK4ksvQfLcEZps,11975
-trulens_eval/pages/Progress.py,sha256=mY_XIhlPXut9VZXMLRjfbaDE9BK-eO8BWuv8SAcAKqc,1518
+trulens_eval/pages/Evaluations.py,sha256=YGqYck7esPDCfU26aQbRne7kRs1_6vE1F62bpzMxEGs,10278
+trulens_eval/pages/Progress.py,sha256=w_CF6QaKBI-6_kFIVVX-bNimtXbGB5UUd6xACfIMty4,1323
 trulens_eval/pages/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 trulens_eval/tests/test_tru_chain.py,sha256=fEpZzCB1OaCfQ3AVe7yjShxxxFJrmR9E660o_47yZIU,5455
-trulens_eval/utils/langchain.py,sha256=H6ZFtM8DMQTaEsrvGPkGq6pJm5fgV36mnI0G9IBTrLA,1803
+trulens_eval/utils/langchain.py,sha256=NocH5hnzxkBRmT_TxtOBh7cI6WmKLZUVJSJDnza0dnw,1484
 trulens_eval/ux/add_logo.py,sha256=Pwl6mfzAX1VSi2VTOZsMBoCstaBuVGoSrs7P5tomP4U,915
-trulens_eval/ux/components.py,sha256=KB4pb8kYt3cPK_Vt7p8i05eB4akyXIvtdzjnSlGdYyI,1273
-trulens_eval/ux/styles.py,sha256=23_Chwh8W8JzJiur_wa2GHJYkQ9rYovTWIEgLdUKuRM,1213
+trulens_eval/ux/components.py,sha256=WanlzqME2NSQ4KnLGx7Y9gglhEhpgC5mj5Hvw66HysY,1142
 trulens_eval/ux/trulens_logo.svg,sha256=92RLTgG0YDPEtZcQWWI7aXTYZAW4wAOAkIIgKUbTiW8,29567
-trulens_eval-0.1.2.dist-info/METADATA,sha256=UmYy2FgrK3HUbvu1k2BaTkn2SosfsYioYTHlh4xlYcA,13424
-trulens_eval-0.1.2.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
-trulens_eval-0.1.2.dist-info/top_level.txt,sha256=AKIBExe5S-v-TbsBrhe1ctF06PubKoYmWNS9rJ1Rb_o,13
-trulens_eval-0.1.2.dist-info/RECORD,,
+trulens_eval-0.1.2a0.dist-info/METADATA,sha256=ub40Py0dGdBwJ_Jzod-mwzvKsLgG5pWs1RnnejIWs88,12734
+trulens_eval-0.1.2a0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+trulens_eval-0.1.2a0.dist-info/top_level.txt,sha256=AKIBExe5S-v-TbsBrhe1ctF06PubKoYmWNS9rJ1Rb_o,13
+trulens_eval-0.1.2a0.dist-info/RECORD,,
```

