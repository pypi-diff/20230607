# Comparing `tmp/dtlpy-1.77.18-py3-none-any.whl.zip` & `tmp/dtlpy-1.78.17-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,10 +1,10 @@
-Zip file size: 531783 bytes, number of entries: 229
+Zip file size: 532756 bytes, number of entries: 229
 -rw-r--r--  2.0 unx    19847 b- defN 23-May-23 16:53 dtlpy/__init__.py
--rw-r--r--  2.0 unx       20 b- defN 23-May-24 11:29 dtlpy/__version__.py
+-rw-r--r--  2.0 unx       20 b- defN 23-Jun-07 20:16 dtlpy/__version__.py
 -rw-r--r--  2.0 unx     2871 b- defN 23-May-15 16:11 dtlpy/exceptions.py
 -rw-r--r--  2.0 unx     5654 b- defN 23-May-15 16:11 dtlpy/new_instance.py
 -rw-r--r--  2.0 unx      976 b- defN 23-May-15 16:11 dtlpy/assets/__init__.py
 -rw-r--r--  2.0 unx    24081 b- defN 23-May-15 16:11 dtlpy/assets/lock_open.png
 -rw-r--r--  2.0 unx     1380 b- defN 23-May-15 16:11 dtlpy/assets/main.py
 -rw-r--r--  2.0 unx      267 b- defN 23-May-15 16:11 dtlpy/assets/main_partial.py
 -rw-r--r--  2.0 unx      149 b- defN 23-May-15 16:11 dtlpy/assets/mock.json
@@ -60,33 +60,33 @@
 -rw-r--r--  2.0 unx     8999 b- defN 23-May-15 16:11 dtlpy/entities/codebase.py
 -rw-r--r--  2.0 unx     4978 b- defN 23-May-15 16:11 dtlpy/entities/command.py
 -rw-r--r--  2.0 unx    44074 b- defN 23-May-15 16:11 dtlpy/entities/dataset.py
 -rw-r--r--  2.0 unx     1186 b- defN 23-May-15 16:11 dtlpy/entities/directory_tree.py
 -rw-r--r--  2.0 unx    12627 b- defN 23-May-23 16:53 dtlpy/entities/dpk.py
 -rw-r--r--  2.0 unx     7197 b- defN 23-May-15 16:11 dtlpy/entities/driver.py
 -rw-r--r--  2.0 unx    12377 b- defN 23-May-15 16:11 dtlpy/entities/execution.py
--rw-r--r--  2.0 unx     4372 b- defN 23-May-15 16:11 dtlpy/entities/feature.py
+-rw-r--r--  2.0 unx     4192 b- defN 23-Jun-07 17:54 dtlpy/entities/feature.py
 -rw-r--r--  2.0 unx     4587 b- defN 23-May-15 16:11 dtlpy/entities/feature_set.py
 -rw-r--r--  2.0 unx    18485 b- defN 23-May-23 16:53 dtlpy/entities/filters.py
 -rw-r--r--  2.0 unx     5366 b- defN 23-May-23 16:53 dtlpy/entities/integration.py
 -rw-r--r--  2.0 unx    27922 b- defN 23-May-15 16:11 dtlpy/entities/item.py
 -rw-r--r--  2.0 unx     3869 b- defN 23-May-15 16:11 dtlpy/entities/label.py
 -rw-r--r--  2.0 unx     2516 b- defN 23-May-15 16:11 dtlpy/entities/links.py
 -rw-r--r--  2.0 unx     5865 b- defN 23-May-23 16:53 dtlpy/entities/message.py
 -rw-r--r--  2.0 unx    18385 b- defN 23-May-23 16:53 dtlpy/entities/model.py
--rw-r--r--  2.0 unx    35907 b- defN 23-May-23 16:53 dtlpy/entities/node.py
+-rw-r--r--  2.0 unx    36116 b- defN 23-Jun-07 17:54 dtlpy/entities/node.py
 -rw-r--r--  2.0 unx    29275 b- defN 23-May-15 16:11 dtlpy/entities/ontology.py
 -rw-r--r--  2.0 unx     9893 b- defN 23-May-15 16:11 dtlpy/entities/organization.py
--rw-r--r--  2.0 unx    26028 b- defN 23-May-15 16:11 dtlpy/entities/package.py
+-rw-r--r--  2.0 unx    26199 b- defN 23-Jun-07 17:54 dtlpy/entities/package.py
 -rw-r--r--  2.0 unx      211 b- defN 23-May-15 16:11 dtlpy/entities/package_defaults.py
 -rw-r--r--  2.0 unx     5917 b- defN 23-May-23 16:53 dtlpy/entities/package_function.py
--rw-r--r--  2.0 unx     4035 b- defN 23-May-15 16:11 dtlpy/entities/package_module.py
+-rw-r--r--  2.0 unx     4035 b- defN 23-Jun-07 16:38 dtlpy/entities/package_module.py
 -rw-r--r--  2.0 unx     5694 b- defN 23-May-15 16:11 dtlpy/entities/package_slot.py
 -rw-r--r--  2.0 unx     5848 b- defN 23-May-15 16:11 dtlpy/entities/paged_entities.py
--rw-r--r--  2.0 unx    17574 b- defN 23-May-15 16:11 dtlpy/entities/pipeline.py
+-rw-r--r--  2.0 unx    19306 b- defN 23-Jun-07 17:54 dtlpy/entities/pipeline.py
 -rw-r--r--  2.0 unx     7215 b- defN 23-May-15 16:11 dtlpy/entities/pipeline_execution.py
 -rw-r--r--  2.0 unx    14301 b- defN 23-May-15 16:11 dtlpy/entities/project.py
 -rw-r--r--  2.0 unx     9541 b- defN 23-May-15 16:11 dtlpy/entities/recipe.py
 -rw-r--r--  2.0 unx     3273 b- defN 23-May-15 16:11 dtlpy/entities/reflect_dict.py
 -rw-r--r--  2.0 unx     5033 b- defN 23-May-15 16:11 dtlpy/entities/resource_execution.py
 -rw-r--r--  2.0 unx    27221 b- defN 23-May-15 16:11 dtlpy/entities/service.py
 -rw-r--r--  2.0 unx     8467 b- defN 23-May-15 16:11 dtlpy/entities/setting.py
@@ -139,61 +139,61 @@
 -rw-r--r--  2.0 unx     1388 b- defN 23-May-15 16:11 dtlpy/examples/upload_segmentation_annotations_from_mask_image.py
 -rw-r--r--  2.0 unx     2610 b- defN 23-May-15 16:11 dtlpy/examples/upload_yolo_format_annotations.py
 -rw-r--r--  2.0 unx      829 b- defN 23-May-15 16:11 dtlpy/miscellaneous/__init__.py
 -rw-r--r--  2.0 unx     3489 b- defN 23-May-15 16:11 dtlpy/miscellaneous/dict_differ.py
 -rw-r--r--  2.0 unx     7971 b- defN 23-May-15 16:11 dtlpy/miscellaneous/git_utils.py
 -rw-r--r--  2.0 unx      428 b- defN 23-May-15 16:11 dtlpy/miscellaneous/json_utils.py
 -rw-r--r--  2.0 unx     4808 b- defN 23-May-15 16:11 dtlpy/miscellaneous/list_print.py
--rw-r--r--  2.0 unx     5118 b- defN 23-May-15 16:11 dtlpy/miscellaneous/zipping.py
+-rw-r--r--  2.0 unx     5337 b- defN 23-Jun-07 17:54 dtlpy/miscellaneous/zipping.py
 -rw-r--r--  2.0 unx      799 b- defN 23-May-15 16:11 dtlpy/ml/__init__.py
--rw-r--r--  2.0 unx    34615 b- defN 23-May-15 16:11 dtlpy/ml/base_model_adapter.py
+-rw-r--r--  2.0 unx    35692 b- defN 23-Jun-07 17:54 dtlpy/ml/base_model_adapter.py
 -rw-r--r--  2.0 unx    20037 b- defN 23-May-15 16:11 dtlpy/ml/metrics.py
 -rw-r--r--  2.0 unx    12484 b- defN 23-May-15 16:11 dtlpy/ml/predictions_utils.py
 -rw-r--r--  2.0 unx     2784 b- defN 23-May-15 16:11 dtlpy/ml/summary_writer.py
 -rw-r--r--  2.0 unx     2444 b- defN 23-May-15 16:11 dtlpy/ml/train_utils.py
 -rw-r--r--  2.0 unx     1883 b- defN 23-May-23 16:53 dtlpy/repositories/__init__.py
 -rw-r--r--  2.0 unx     2966 b- defN 23-May-15 16:11 dtlpy/repositories/analytics.py
 -rw-r--r--  2.0 unx    35175 b- defN 23-May-15 16:11 dtlpy/repositories/annotations.py
--rw-r--r--  2.0 unx    10299 b- defN 23-May-15 16:11 dtlpy/repositories/apps.py
--rw-r--r--  2.0 unx    19528 b- defN 23-May-15 16:11 dtlpy/repositories/artifacts.py
+-rw-r--r--  2.0 unx    10299 b- defN 23-Jun-07 20:16 dtlpy/repositories/apps.py
+-rw-r--r--  2.0 unx    19554 b- defN 23-Jun-07 17:54 dtlpy/repositories/artifacts.py
 -rw-r--r--  2.0 unx    25410 b- defN 23-May-15 16:11 dtlpy/repositories/assignments.py
 -rw-r--r--  2.0 unx     8195 b- defN 23-May-15 16:11 dtlpy/repositories/bots.py
 -rw-r--r--  2.0 unx    25183 b- defN 23-May-15 16:11 dtlpy/repositories/codebases.py
 -rw-r--r--  2.0 unx     5233 b- defN 23-May-15 16:11 dtlpy/repositories/commands.py
--rw-r--r--  2.0 unx    42429 b- defN 23-May-15 16:11 dtlpy/repositories/datasets.py
+-rw-r--r--  2.0 unx    42482 b- defN 23-Jun-07 17:54 dtlpy/repositories/datasets.py
 -rw-r--r--  2.0 unx    41101 b- defN 23-May-15 16:11 dtlpy/repositories/downloader.py
 -rw-r--r--  2.0 unx    13793 b- defN 23-May-23 16:53 dtlpy/repositories/dpks.py
--rw-r--r--  2.0 unx    10299 b- defN 23-May-15 16:11 dtlpy/repositories/drivers.py
--rw-r--r--  2.0 unx    30254 b- defN 23-May-15 16:11 dtlpy/repositories/executions.py
+-rw-r--r--  2.0 unx    10284 b- defN 23-Jun-07 17:54 dtlpy/repositories/drivers.py
+-rw-r--r--  2.0 unx    30254 b- defN 23-Jun-07 17:54 dtlpy/repositories/executions.py
 -rw-r--r--  2.0 unx     7888 b- defN 23-May-15 16:11 dtlpy/repositories/feature_sets.py
--rw-r--r--  2.0 unx    10078 b- defN 23-May-15 16:11 dtlpy/repositories/features.py
+-rw-r--r--  2.0 unx     9166 b- defN 23-Jun-07 17:54 dtlpy/repositories/features.py
 -rw-r--r--  2.0 unx    11460 b- defN 23-May-23 16:53 dtlpy/repositories/integrations.py
 -rw-r--r--  2.0 unx    37804 b- defN 23-May-15 16:11 dtlpy/repositories/items.py
 -rw-r--r--  2.0 unx     3080 b- defN 23-May-23 16:53 dtlpy/repositories/messages.py
--rw-r--r--  2.0 unx    28904 b- defN 23-May-23 16:53 dtlpy/repositories/models.py
+-rw-r--r--  2.0 unx    29021 b- defN 23-Jun-07 17:54 dtlpy/repositories/models.py
 -rw-r--r--  2.0 unx     3061 b- defN 23-May-15 16:11 dtlpy/repositories/nodes.py
 -rw-r--r--  2.0 unx    19525 b- defN 23-May-15 16:11 dtlpy/repositories/ontologies.py
--rw-r--r--  2.0 unx    22953 b- defN 23-May-15 16:11 dtlpy/repositories/organizations.py
+-rw-r--r--  2.0 unx    22957 b- defN 23-Jun-07 17:54 dtlpy/repositories/organizations.py
 -rw-r--r--  2.0 unx    86474 b- defN 23-May-23 16:53 dtlpy/repositories/packages.py
--rw-r--r--  2.0 unx    11759 b- defN 23-May-15 16:11 dtlpy/repositories/pipeline_executions.py
--rw-r--r--  2.0 unx    22315 b- defN 23-May-15 16:11 dtlpy/repositories/pipelines.py
--rw-r--r--  2.0 unx    22184 b- defN 23-May-23 16:53 dtlpy/repositories/projects.py
+-rw-r--r--  2.0 unx    11873 b- defN 23-Jun-07 17:54 dtlpy/repositories/pipeline_executions.py
+-rw-r--r--  2.0 unx    23316 b- defN 23-Jun-07 17:54 dtlpy/repositories/pipelines.py
+-rw-r--r--  2.0 unx    22184 b- defN 23-Jun-07 17:54 dtlpy/repositories/projects.py
 -rw-r--r--  2.0 unx    15703 b- defN 23-May-23 16:53 dtlpy/repositories/recipes.py
 -rw-r--r--  2.0 unx     5374 b- defN 23-May-15 16:11 dtlpy/repositories/resource_executions.py
 -rw-r--r--  2.0 unx    63949 b- defN 23-May-15 16:11 dtlpy/repositories/services.py
 -rw-r--r--  2.0 unx    12296 b- defN 23-May-15 16:11 dtlpy/repositories/settings.py
--rw-r--r--  2.0 unx    46623 b- defN 23-May-24 11:29 dtlpy/repositories/tasks.py
+-rw-r--r--  2.0 unx    46625 b- defN 23-Jun-07 17:54 dtlpy/repositories/tasks.py
 -rw-r--r--  2.0 unx    11420 b- defN 23-May-15 16:11 dtlpy/repositories/times_series.py
 -rw-r--r--  2.0 unx    21961 b- defN 23-May-15 16:11 dtlpy/repositories/triggers.py
 -rw-r--r--  2.0 unx     9251 b- defN 23-May-15 16:11 dtlpy/repositories/upload_element.py
 -rw-r--r--  2.0 unx    30682 b- defN 23-May-15 16:11 dtlpy/repositories/uploader.py
 -rw-r--r--  2.0 unx     9033 b- defN 23-May-15 16:11 dtlpy/repositories/webhooks.py
 -rw-r--r--  2.0 unx      904 b- defN 23-May-15 16:11 dtlpy/services/__init__.py
 -rw-r--r--  2.0 unx     5022 b- defN 23-May-15 16:11 dtlpy/services/aihttp_retry.py
--rw-r--r--  2.0 unx    65015 b- defN 23-May-15 16:11 dtlpy/services/api_client.py
+-rw-r--r--  2.0 unx    65015 b- defN 23-Jun-07 20:16 dtlpy/services/api_client.py
 -rw-r--r--  2.0 unx     1515 b- defN 23-May-15 16:11 dtlpy/services/api_reference.py
 -rw-r--r--  2.0 unx     2810 b- defN 23-May-15 16:11 dtlpy/services/async_utils.py
 -rw-r--r--  2.0 unx     1036 b- defN 23-May-15 16:11 dtlpy/services/calls_counter.py
 -rw-r--r--  2.0 unx     3593 b- defN 23-May-15 16:11 dtlpy/services/check_sdk.py
 -rw-r--r--  2.0 unx     3694 b- defN 23-May-15 16:11 dtlpy/services/cookie.py
 -rw-r--r--  2.0 unx     6332 b- defN 23-May-15 16:11 dtlpy/services/create_logger.py
 -rw-r--r--  2.0 unx     3686 b- defN 23-May-15 16:11 dtlpy/services/events.py
@@ -213,19 +213,19 @@
 -rw-r--r--  2.0 unx     6451 b- defN 23-May-15 16:11 dtlpy/utilities/local_development/local_session.py
 -rw-r--r--  2.0 unx      124 b- defN 23-May-15 16:11 dtlpy/utilities/reports/__init__.py
 -rw-r--r--  2.0 unx     5927 b- defN 23-May-15 16:11 dtlpy/utilities/reports/figures.py
 -rw-r--r--  2.0 unx     2639 b- defN 23-May-15 16:11 dtlpy/utilities/reports/report.py
 -rw-r--r--  2.0 unx      734 b- defN 23-May-15 16:11 dtlpy/utilities/videos/__init__.py
 -rw-r--r--  2.0 unx    24072 b- defN 23-May-15 16:11 dtlpy/utilities/videos/video_player.py
 -rw-r--r--  2.0 unx    21875 b- defN 23-May-15 16:11 dtlpy/utilities/videos/videos.py
--rwxr-xr-x  2.0 unx       10 b- defN 23-May-15 16:11 dtlpy-1.77.18.data/scripts/dlp
--rwxr-xr-x  2.0 unx       37 b- defN 23-May-15 16:11 dtlpy-1.77.18.data/scripts/dlp.bat
--rwxr-xr-x  2.0 unx     4267 b- defN 23-May-24 11:30 dtlpy-1.77.18.data/scripts/dlp.py
+-rwxr-xr-x  2.0 unx       10 b- defN 23-May-15 16:11 dtlpy-1.78.17.data/scripts/dlp
+-rwxr-xr-x  2.0 unx       37 b- defN 23-May-15 16:11 dtlpy-1.78.17.data/scripts/dlp.bat
+-rwxr-xr-x  2.0 unx     4267 b- defN 23-Jun-07 20:17 dtlpy-1.78.17.data/scripts/dlp.py
 -rw-r--r--  2.0 unx        0 b- defN 23-May-15 16:11 tests/features/__init__.py
--rw-r--r--  2.0 unx     9394 b- defN 23-May-23 16:53 tests/features/environment.py
--rw-r--r--  2.0 unx    11356 b- defN 23-May-24 11:30 dtlpy-1.77.18.dist-info/LICENSE
--rw-r--r--  2.0 unx     3022 b- defN 23-May-24 11:30 dtlpy-1.77.18.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-May-24 11:30 dtlpy-1.77.18.dist-info/WHEEL
--rw-r--r--  2.0 unx       43 b- defN 23-May-24 11:30 dtlpy-1.77.18.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       12 b- defN 23-May-24 11:30 dtlpy-1.77.18.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    20661 b- defN 23-May-24 11:30 dtlpy-1.77.18.dist-info/RECORD
-229 files, 2154769 bytes uncompressed, 499031 bytes compressed:  76.9%
+-rw-r--r--  2.0 unx     9395 b- defN 23-Jun-07 17:54 tests/features/environment.py
+-rw-r--r--  2.0 unx    11356 b- defN 23-Jun-07 20:17 dtlpy-1.78.17.dist-info/LICENSE
+-rw-r--r--  2.0 unx     3022 b- defN 23-Jun-07 20:17 dtlpy-1.78.17.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jun-07 20:17 dtlpy-1.78.17.dist-info/WHEEL
+-rw-r--r--  2.0 unx       43 b- defN 23-Jun-07 20:17 dtlpy-1.78.17.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       12 b- defN 23-Jun-07 20:17 dtlpy-1.78.17.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    20660 b- defN 23-Jun-07 20:17 dtlpy-1.78.17.dist-info/RECORD
+229 files, 2158387 bytes uncompressed, 500004 bytes compressed:  76.8%
```

## zipnote {}

```diff
@@ -648,41 +648,41 @@
 
 Filename: dtlpy/utilities/videos/video_player.py
 Comment: 
 
 Filename: dtlpy/utilities/videos/videos.py
 Comment: 
 
-Filename: dtlpy-1.77.18.data/scripts/dlp
+Filename: dtlpy-1.78.17.data/scripts/dlp
 Comment: 
 
-Filename: dtlpy-1.77.18.data/scripts/dlp.bat
+Filename: dtlpy-1.78.17.data/scripts/dlp.bat
 Comment: 
 
-Filename: dtlpy-1.77.18.data/scripts/dlp.py
+Filename: dtlpy-1.78.17.data/scripts/dlp.py
 Comment: 
 
 Filename: tests/features/__init__.py
 Comment: 
 
 Filename: tests/features/environment.py
 Comment: 
 
-Filename: dtlpy-1.77.18.dist-info/LICENSE
+Filename: dtlpy-1.78.17.dist-info/LICENSE
 Comment: 
 
-Filename: dtlpy-1.77.18.dist-info/METADATA
+Filename: dtlpy-1.78.17.dist-info/METADATA
 Comment: 
 
-Filename: dtlpy-1.77.18.dist-info/WHEEL
+Filename: dtlpy-1.78.17.dist-info/WHEEL
 Comment: 
 
-Filename: dtlpy-1.77.18.dist-info/entry_points.txt
+Filename: dtlpy-1.78.17.dist-info/entry_points.txt
 Comment: 
 
-Filename: dtlpy-1.77.18.dist-info/top_level.txt
+Filename: dtlpy-1.78.17.dist-info/top_level.txt
 Comment: 
 
-Filename: dtlpy-1.77.18.dist-info/RECORD
+Filename: dtlpy-1.78.17.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## dtlpy/__version__.py

```diff
@@ -1 +1 @@
-version = '1.77.18'
+version = '1.78.17'
```

## dtlpy/entities/feature.py

```diff
@@ -125,22 +125,14 @@
             _json['org'] = self.org_id
         if self.refs is not None:
             _json['refs'] = self.refs
         if self.data_type is not None:
             _json['dataType'] = self.data_type
         return _json
 
-    def update(self):
-        """
-        Update Feature Vector changes to platform
-
-        :return: Feature entity
-        """
-        return self.features.update(feature=self)
-
     def delete(self):
         """
         Delete Feature Vector object
 
         :return: True
         """
         return self.features.delete(feature_id=self.id)
```

## dtlpy/entities/node.py

```diff
@@ -89,14 +89,15 @@
                  name: str,
                  display_name: str,
                  port_id: str = None,
                  color: tuple = None,
                  port_percentage: int = None,
                  action: str = None,
                  default_value=None,
+                 variable_name: str = None,
                  actions: list = None):
         """
         Pipeline Node
 
         :param entities.PackageInputType input_type: entities.PackageInputType of the input type of the pipeline
         :param str name: name of the input
         :param str display_name: of the input
@@ -110,14 +111,16 @@
         self.port_id = port_id if port_id else str(uuid.uuid4())
         self.input_type = input_type
         self.name = name
         self.color = color
         self.display_name = display_name
         self.port_percentage = port_percentage
         self.default_value = default_value
+        self.variable_name = variable_name
+
         if action is not None:
             warnings.warn('action param has been deprecated in version 1.80', DeprecationWarning)
             if actions is None:
                 actions = []
             actions.append(action)
         self.actions = actions
 
@@ -132,24 +135,26 @@
             port_id=_json.get('portId', None),
             input_type=_json.get('type', None),
             name=_json.get('name', None),
             color=_json.get('color', None),
             display_name=_json.get('displayName', None),
             port_percentage=_json.get('portPercentage', None),
             default_value=_json.get('defaultValue', None),
+            variable_name=_json.get('variableName', None),
             actions=_json.get('actions', None),
         )
 
     def to_json(self):
         _json = {
             'portId': self.port_id,
             'type': self.input_type,
             'name': self.name,
             'color': self.color,
             'displayName': self.display_name,
+            'variableName': self.variable_name,
             'portPercentage': self.port_percentage,
         }
 
         if self.actions:
             _json['actions'] = self.actions
         if self.default_value:
             _json['defaultValue'] = self.default_value
@@ -158,14 +163,15 @@
 
 
 class PipelineNodeType(str, Enum):
     TASK = 'task'
     CODE = 'code'
     FUNCTION = 'function'
     STORAGE = 'storage'
+    ML = 'ml'
 
 
 class PipelineNameSpace:
     def __init__(self, function_name, project_name, module_name=None, service_name=None, package_name=None):
         self.function_name = function_name
         self.project_name = project_name
         self.module_name = module_name
```

## dtlpy/entities/package.py

```diff
@@ -573,28 +573,30 @@
                         available_methods=None,
                         output_type=entities.AnnotationType.CLASSIFICATION,
                         input_type='image',
                         default_configuration: dict = None):
         """
         Create ML metadata for the package
         :param cls: ModelAdapter class, to get the list of available_methods
-        :param available_methods: available user function on the adapter.  ['load', 'save', 'predict', 'train', 'evaluate']
+        :param available_methods: available user function on the adapter.  ['load', 'save', 'predict', 'train']
         :param output_type: annotation type the model create, e.g. dl.AnnotationType.CLASSIFICATION
         :param input_type: input file type the model gets, one of ['image', 'video', 'txt']
         :param default_configuration: default service configuration for the deployed services
         :return:
         """
+        user_implemented_methods = ['load', 'save', 'predict', 'train']
         if available_methods is None:
             # default
-            available_methods = ['predict', 'train', 'evaluate']
+            available_methods = user_implemented_methods
 
         if cls is not None:
+            # TODO dont check if function is on the adapter - check if the functions is implemented (not raise NotImplemented)
             available_methods = [
                 {name: 'BaseModelAdapter' not in getattr(cls, name).__qualname__}
-                for name in ['predict', 'train', 'evaluate']
+                for name in user_implemented_methods
             ]
         if default_configuration is None:
             default_configuration = dict()
         metadata = {
             'system': {'ml': {'defaultConfiguration': default_configuration,
                               'outputType': output_type,
                               'inputType': input_type,
```

## dtlpy/entities/pipeline.py

```diff
@@ -3,15 +3,15 @@
 import traceback
 from enum import Enum
 from typing import List
 import attr
 from .node import PipelineNode, PipelineConnection, TaskNode, CodeNode, FunctionNode, DatasetNode
 from .. import repositories, entities
 from ..services.api_client import ApiClient
-
+from .package_function import PackageInputType
 logger = logging.getLogger(name='dtlpy')
 
 
 class PipelineResumeOption(str, Enum):
     TERMINATE_EXISTING_CYCLES = 'terminateExistingCycles',
     RESUME_EXISTING_CYCLES = 'resumeExistingCycles'
 
@@ -68,14 +68,49 @@
 
         if self.default_resume_option is not None:
             _json['lastUpdate'] = self.default_resume_option
 
         return _json
 
 
+class Variable(entities.DlEntity):
+    """
+    Pipeline Variables
+    """
+    id: str = entities.DlProperty(location=['id'], _type=str)
+    created_at: str = entities.DlProperty(location=['createdAt'], _type=str)
+    updated_at: str = entities.DlProperty(location=['updatedAt'], _type=str)
+    reference: str = entities.DlProperty(location=['reference'], _type=str)
+    creator: str = entities.DlProperty(location=['creator'], _type=str)
+    variable_type: PackageInputType = entities.DlProperty(location=['type'], _type=PackageInputType)
+    name: str = entities.DlProperty(location=['name'], _type=str)
+    value = entities.DlProperty(location=['value'])
+
+    @classmethod
+    def from_json(cls, _json):
+        """
+        Turn platform representation of variable into a pipeline variable entity
+
+        :param dict _json: platform representation of pipeline variable
+        :return: pipeline variable entity
+        :rtype: dtlpy.entities.pipeline.PipelineVariables
+        """
+
+        inst = cls(_dict=_json.copy())
+        return inst
+
+    def to_json(self):
+        """
+        :return: variable of pipeline
+        :rtype: dict
+        """
+        _json = self._dict.copy()
+        return _json
+
+
 class PipelineAverages:
     def __init__(
             self,
             avg_time_per_execution: float,
             avg_execution_per_day: float
     ):
         self.avg_time_per_execution = avg_time_per_execution
@@ -166,14 +201,16 @@
     # platform
     id = attr.ib()
     name = attr.ib()
     creator = attr.ib()
     org_id = attr.ib()
     connections = attr.ib()
     settings = attr.ib(type=PipelineSettings)
+    variables = attr.ib(type=List[Variable])
+
     status = attr.ib(type=CompositionStatus)
 
     # name change
     created_at = attr.ib()
     updated_at = attr.ib(repr=False)
     start_nodes = attr.ib()
     project_id = attr.ib()
@@ -225,14 +262,18 @@
         """
         if project is not None:
             if project.id != _json.get('projectId', None):
                 logger.warning('Pipeline has been fetched from a project that is not belong to it')
                 project = None
 
         connections = [PipelineConnection.from_json(_json=con) for con in _json.get('connections', list())]
+        json_variables = _json.get('variables', list())
+        variables = list()
+        if json_variables:
+            variables = [Variable.from_json(_json=v) for v in json_variables]
         settings = PipelineSettings.from_json(_json=_json.get('settings', dict()))
         inst = cls(
             created_at=_json.get('createdAt', None),
             updated_at=_json.get('updatedAt', None),
             project_id=_json.get('projectId', None),
             org_id=_json.get('orgId', None),
             composition_id=_json.get('compositionId', None),
@@ -244,14 +285,15 @@
             connections=connections,
             start_nodes=_json.get('startNodes', None),
             url=_json.get('url', None),
             preview=_json.get('preview', None),
             description=_json.get('description', None),
             revisions=_json.get('revisions', None),
             settings=settings,
+            variables=variables,
             status=_json.get('status', None),
             original_settings=settings
         )
         for node in _json.get('nodes', list()):
             inst.nodes.add(node=cls.pipeline_node(node))
         inst.is_fetched = is_fetched
         return inst
@@ -293,25 +335,28 @@
                                                         attr.fields(Pipeline).project_id,
                                                         attr.fields(Pipeline).composition_id,
                                                         attr.fields(Pipeline).url,
                                                         attr.fields(Pipeline).preview,
                                                         attr.fields(Pipeline).description,
                                                         attr.fields(Pipeline).revisions,
                                                         attr.fields(Pipeline).settings,
+                                                        attr.fields(Pipeline).variables,
                                                         attr.fields(Pipeline)._original_settings
                                                         ))
 
         _json['projectId'] = self.project_id
         _json['createdAt'] = self.created_at
         _json['updatedAt'] = self.updated_at
         _json['compositionId'] = self.composition_id
         _json['startNodes'] = self.start_nodes
         _json['orgId'] = self.org_id
         _json['nodes'] = [node.to_json() for node in self.nodes]
         _json['connections'] = [con.to_json() for con in self.connections]
+        if self.variables:
+            _json['variables'] = [v.to_json() for v in self.variables]
         _json['url'] = self.url
 
         settings_json = self.settings.to_json()
         if settings_json:
             _json['settings'] = settings_json
 
         if self.preview is not None:
@@ -344,15 +389,15 @@
     @_repositories.default
     def set_repositories(self):
         reps = namedtuple('repositories',
                           field_names=['projects', 'pipelines', 'pipeline_executions', 'triggers', 'nodes'])
 
         r = reps(
             projects=repositories.Projects(client_api=self._client_api),
-            pipelines=repositories.Pipelines(client_api=self._client_api),
+            pipelines=repositories.Pipelines(client_api=self._client_api, project=self._project),
             pipeline_executions=repositories.PipelineExecutions(
                 client_api=self._client_api, project=self._project, pipeline=self
             ),
             triggers=repositories.Triggers(client_api=self._client_api, pipeline=self),
             nodes=repositories.Nodes(client_api=self._client_api, pipeline=self)
         )
         return r
@@ -389,14 +434,15 @@
         """
         Update pipeline changes to platform
 
         :return: pipeline entity
         """
         return self.pipelines.update(pipeline=self)
 
+
     def delete(self):
         """
         Delete pipeline object
 
         :return: True
         """
         return self.pipelines.delete(pipeline=self)
```

## dtlpy/miscellaneous/zipping.py

```diff
@@ -43,14 +43,18 @@
             ignore_lines += ignore_directories
         spec = pathspec.PathSpec.from_lines(pathspec.patterns.GitWildMatchPattern, ignore_lines)
 
         # init zip file
         zip_file = zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED)
         try:
             for root, dirs, files in os.walk(directory):
+                # remove dirs to avoid going file by file
+                for d in dirs:
+                    if spec.match_file(os.path.relpath(os.path.join(root, d), directory)):
+                        dirs.remove(d)
                 for file in files:
                     filepath = os.path.join(root, file)
                     if not spec.match_file(os.path.relpath(filepath, directory)):
                         Zipping.__add_to_zip_file(directory, filepath, ignore_max_file_size, zip_file)
         finally:
             zip_file.close()
```

## dtlpy/ml/base_model_adapter.py

```diff
@@ -24,14 +24,17 @@
         self.logger = logger
         # entities
         self._model_entity = None
         self._package = None
         self.package_name = None
         self.model = None
         self.bucket_path = None
+        # funcs
+        self.item_to_batch_mapping = {'text': self._item_to_text,
+                                      'image': self._item_to_image}
         if model_entity is not None:
             self.load_from_model(model_entity=model_entity)
 
     ##################
     # Configurations #
     ##################
 
@@ -131,24 +134,32 @@
             Virtual method - need to implement
 
         :param batch: `np.ndarray`
         :return: `list[dl.AnnotationCollection]` each collection is per each image / item in the batch
         """
         raise NotImplementedError("Please implement 'predict' method in {}".format(self.__class__.__name__))
 
-    def evaluate(self, data_path, on_batch_end_callback: typing.Callable, **kwargs):
-        """ Model evaluation
-
-            Virtual method - need to implement
+    def evaluate(self, model: entities.Model, dataset: entities.Dataset):
+        """
+        This function evaluates the model prediction on a dataset (with GT annotations).
+        The evaluation process will upload the scores and metrics to the platform.
 
-        :param data_path: local directory with set to predict
-        :param on_batch_end_callback: Callable, run after batch end
-        :return: `list[dl.AnnotationCollection]` each collection is per each image / item in the batch
+        :param model: The model to evaluate (annotation.metadata.system.model.name
+        :param dataset: Dataset where the model predicted and uploaded its annotations
+        :return:
         """
-        raise NotImplementedError("Please implement 'evaluate' method in {}".format(self.__class__.__name__))
+        try:
+            from dtlpymetrics.scoring import ScoringAndMetrics
+        except (ImportError, ModuleNotFoundError):
+            logger.error('Import Error! Cant import dtlpymetrics. Need install the Dataloop Metrics App (from GitHub)')
+            raise
+        compare_types = model.output_type
+        success, response = ScoringAndMetrics.create_model_score(model=model,
+                                                                 dataset=dataset,
+                                                                 compare_types=compare_types)
 
     def convert_from_dtlpy(self, data_path, **kwargs):
         """ Convert Dataloop structure data to model structured
 
             Virtual method - need to implement
 
             e.g. take dlp dir structure and construct annotation file
@@ -156,15 +167,30 @@
         :param data_path: `str` local File System directory path where we already downloaded the data from dataloop platform
         :return:
         """
         raise NotImplementedError("Please implement 'convert_from_dtlpy' method in {}".format(self.__class__.__name__))
 
     #################
     # DTLPY METHODS #
-    #################
+    ################
+    def prepare_item_func(self, item: entities.Item):
+        """
+        Prepare the Dataloop item before calling the `predict` function with a batch.
+        A user can override this function to load item differently
+        Default will load the item according the input_type (mapping type to function is in self.item_to_batch_mapping)
+
+        :param item:
+        :return: preprocessed: the var with the loaded item information (e.g. ndarray for image, dict for json files etc)
+        """
+        # Item to batch func
+        if self.model_entity.input_type in self.item_to_batch_mapping:
+            processed = self.item_to_batch_mapping[self.model_entity.input_type](item)
+        else:
+            processed = self._item_to_item(item)
+        return processed
 
     def prepare_data(self,
                      dataset: entities.Dataset,
                      # paths
                      root_path=None,
                      data_path=None,
                      output_path=None,
@@ -221,15 +247,15 @@
                 ret_list = dataset.items.download(filters=filters,
                                                   local_path=data_subset_base_path,
                                                   annotation_options=annotation_options)
 
         self.convert_from_dtlpy(data_path=data_path, **kwargs)
         return root_path, data_path, output_path
 
-    def load_from_model(self, model_entity=None, local_path=None, overwrite=False, **kwargs):
+    def load_from_model(self, model_entity=None, local_path=None, overwrite=True, **kwargs):
         """ Loads a model from given `dl.Model`.
             Reads configurations and instantiate self.model_entity
             Downloads the model_entity bucket (if available)
 
         :param model_entity:  `str` dl.Model entity
         :param local_path:  `str` directory path in local FileSystem to download the model_entity to
         :param overwrite: `bool` (default False) if False does not downloads files with same name else (True) download all
@@ -284,40 +310,36 @@
     # ===============
     # SERVICE METHODS
     # ===============
 
     @entities.Package.decorators.function(display_name='Predict Items',
                                           inputs={'items': 'Item[]'},
                                           outputs={'items': 'Item[]', 'annotations': 'Annotation[]'})
-    def predict_items(self, items: list, upload_annotations=True, conf_threshold=0, batch_size=16, **kwargs):
+    def predict_items(self, items: list, upload_annotations=True, conf_threshold=0, batch_size=None, **kwargs):
         """
         Run the predict function on the input list of items (or single) and return the items and the predictions.
         Each prediction is by the model output type (package.output_type) and model_info in the metadata
 
         :param items: `List[dl.Item]` list of items to predict
         :param upload_annotations: `bool` uploads the predictions on the given items
         :param conf_threshold: `float` returns and uploads annotation only above this threshold
         :param batch_size: `int` size of batch to run a single inference
 
         :return: `List[dl.Item]`, `List[List[dl.Annotation]]`
         """
-
+        if batch_size is None:
+            batch_size = self.configuration.get('batch_size', 4)
         input_type = self.model_entity.input_type
         self.logger.debug(
             "Predicting {} items, using batch size {}. input type: {}".format(len(items), batch_size, input_type))
         pool = ThreadPoolExecutor(max_workers=16)
         annotations = list()
         for i_batch in tqdm.tqdm(range(0, len(items), batch_size), desc='predicting', unit='bt', leave=None):
             batch_items = items[i_batch: i_batch + batch_size]
-            if input_type == 'image':
-                batch = list(pool.map(self._prepare_items_image_batch, batch_items))
-            elif input_type == 'text':
-                batch = list(pool.map(self._prepare_items_text_batch, batch_items))
-            else:
-                raise ValueError('Unknown inputType: {} (from model_entity.input_type'.format(input_type))
+            batch = list(pool.map(self.prepare_item_func, batch_items))
             batch_collections = self.predict(batch, **kwargs)
             if upload_annotations is True:
                 self.logger.debug(
                     "Uploading items' annotation for model {!r}.".format(self.model_entity.name))
                 try:
                     batch_collections = list(pool.map(partial(self._upload_model_annotations),
                                                       batch_items, batch_collections))
@@ -328,52 +350,52 @@
                 # function needs to return `List[List[dl.Annotation]]`
                 # convert annotation collection to a list of dl.Annotation for each batch
                 if isinstance(collection, entities.AnnotationCollection):
                     annotations.extend([annotation for annotation in collection.annotations])
                 else:
                     logger.warning(f'RETURN TYPE MAY BE INVALID: {type(collection)}')
                     annotations.extend(collection)
+            # TODO call the callback
+
         pool.shutdown()
         return items, annotations
 
     @entities.Package.decorators.function(display_name='Predict Dataset with DQL',
                                           inputs={'dataset': 'Dataset',
                                                   'filters': 'Json'})
     def predict_dataset(self,
-                        dataset: entities.Dataset, filters: entities.Filters = None,
-                        with_upload=True, cleanup=False, batch_size=16, output_shape=None, **kwargs):
+                        dataset: entities.Dataset,
+                        filters: entities.Filters = None,
+                        with_upload=True,
+                        cleanup=False,
+                        batch_size=None,
+                        **kwargs):
         """
         Predicts all items given
 
         :param dataset: Dataset entity to predict
         :param filters: Filters entity for a filtering before predicting
         :param with_upload: `bool` uploads the predictions back to the given items
         :param cleanup: `bool` if set removes existing predictions with the same package-model name (default: False)
         :param batch_size: `int` size of batch to run a single inference
-        :param output_shape: `tuple` (width, height) of resize needed per image
 
         :return: `List[dl.AnnotationCollection]` where all annotation in the collection are of type package.output_type
                                                  and has prediction fields (model_info)
         """
-        # TODO: do we want to add score filtering here?
-        self.logger.debug(
-            "Predicting dataset (name:{}, id:{}, using batch size {}. Reshaping to: {}".format(dataset.name,
-                                                                                               dataset.id,
-                                                                                               batch_size,
-                                                                                               output_shape))
-        if filters is not None:
+        self.logger.debug("Predicting dataset (name:{}, id:{}, using batch size {}".format(dataset.name,
+                                                                                           dataset.id,
+                                                                                           batch_size))
+        if filters is not None and isinstance(filters, dict):
             filters = entities.Filters(custom_filter=filters)
         pages = dataset.items.list(filters=filters, page_size=batch_size)
-        for page in tqdm.tqdm(pages, total=pages.items_count, desc='predicting', unit='bt', leave=None):
-            self.predict_items(items=page.items,
-                               with_upload=with_upload,
-                               cleanup=cleanup,
-                               batch_size=batch_size,
-                               output_shape=output_shape,
-                               **kwargs)
+        self.predict_items(items=list(pages.all()),
+                           with_upload=with_upload,
+                           cleanup=cleanup,
+                           batch_size=batch_size,
+                           **kwargs)
         return True
 
     @entities.Package.decorators.function(display_name='Train a Model',
                                           inputs={'model': entities.Model},
                                           outputs={'model': entities.Model})
     def train_model(self,
                     model: entities.Model,
@@ -453,63 +475,60 @@
 
     @entities.Package.decorators.function(display_name='Evaluate a Model',
                                           inputs={'model': entities.Model,
                                                   'dataset': entities.Dataset})
     def evaluate_model(self,
                        model: entities.Model,
                        dataset: entities.Dataset,
-                       cleanup=False,
+                       filters: entities.Filters,
+                       #
                        progress: utilities.Progress = None,
                        context: utilities.Context = None):
         """
         Evaluate a model.
         data will be downloaded from the dataset and query
         configuration is as defined in dl.Model.configuration
         upload annotations and calculate metrics vs GT
         """
-        output_path = None
         try:
             logger.info(
                 f"Received model: {model.id} for evaluation on dataset (name: {dataset.name}, id: {dataset.id}")
+            if context is not None:
+                if 'system' not in model.metadata:
+                    model.metadata['system'] = dict()
+                model.metadata['system']['evaluateExecutionId'] = context.execution_id
+            model.update()
             ##########################
             # load model and weights #
             ##########################
-            logger.info("Loading Adapter with: {n} ({i!r})".format(n=model.name, i=model.id))
+            logger.info(f"Loading Adapter with: {model.name} ({model.id!r})")
             self.load_from_model(dataset=dataset,
                                  model_entity=model)
 
-            ################
-            # prepare data #
-            ################
-            root_path, data_path, output_path = self.prepare_data(
-                dataset=dataset,
-                root_path=os.path.join('tmp', model.id)
-            )
-            # Start the Train
-            logger.info("Training {p_name!r} with model {m_name!r} on data {d_path!r}".
-                        format(p_name=self.package_name, m_name=model.id, d_path=data_path))
-            if progress is not None:
-                progress.update(message='starting evaluation')
-
-            def on_batch_end_callback(i_epoch, n_epoch):
-                if progress is not None:
-                    progress.update(progress=int(100 * (i_epoch + 1) / n_epoch),
-                                    message='finished epoch: {}/{}'.format(i_epoch, n_epoch))
+            ##############
+            # Predicting #
+            ##############
+            self.predict_dataset(dataset=dataset,
+                                 filters=filters,
+                                 with_upload=True)
 
-            self.evaluate(data_path=data_path,
-                          on_batch_end_callback=on_batch_end_callback)
+            ##############
+            # Evaluating #
+            ##############
+            if progress is not None:
+                progress.update(message='calculating metrics',
+                                progress=98)
+            self.evaluate(model=model, dataset=dataset)
+            #########
+            # Done! #
+            #########
             if progress is not None:
                 progress.update(message='finishing evaluation',
                                 progress=99)
 
-            ###########
-            # cleanup #
-            ###########
-            if cleanup:
-                shutil.rmtree(output_path, ignore_errors=True)
         except Exception:
             model.status = 'failed'
             model.update()
             raise
         return self.model
 
     # =============
@@ -518,41 +537,58 @@
 
     def _upload_model_annotations(self, item: entities.Item, predictions):
         """
         Utility function that upload prediction to dlp platform based on the package.output_type
         :param predictions: `dl.AnnotationCollection`
         :param cleanup: `bool` if set removes existing predictions with the same package-model name
         """
-        if not isinstance(predictions, entities.AnnotationCollection):
+        if not (isinstance(predictions, entities.AnnotationCollection) or isinstance(predictions, list)):
             raise TypeError('predictions was expected to be of type {}, but instead it is {}'.
                             format(entities.AnnotationCollection, type(predictions)))
         model_info_name = "{}-{}".format(self.package_name, self.model_entity.name)
         # if cleanup:
         #     clean_filter = entities.Filters(field='type',
         #                                     values=self.model_entity.output_type,
         #                                     resource=entities.FiltersResource.ANNOTATION)
         #     clean_filter.add(field='metadata.user.model.name', values=model_info_name)
         #     item.annotations.delete(filters=clean_filter)
         annotations = item.annotations.upload(annotations=predictions)
         return annotations
 
     @staticmethod
-    def _prepare_items_image_batch(item):
+    def _item_to_image(item):
+        """
+        Preprocess items before cvalling the `predict` functions.
+        Convert item to numpy array
+
+        :param item:
+        :return:
+        """
         buffer = item.download(save_locally=False)
         image = np.asarray(Image.open(buffer))
         return image
 
     @staticmethod
-    def _prepare_items_text_batch(item):
+    def _item_to_item(item):
+        """
+        Default item to batch function.
+        This function should prepare a single item for the predict function, e.g. for images, it loads the image as numpy array
+        :param item:
+        :return:
+        """
+        return item
+
+    @staticmethod
+    def _item_to_text(item):
         buffer = item.download(save_locally=False)
         text = buffer.read().decode()
         return text
 
     @staticmethod
-    def _prepare_uris_image_batch(data_uri):
+    def _uri_to_image(data_uri):
         # data_uri = "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAS4AAAEuCAYAAAAwQP9DAAAU80lEQVR4Xu2da+hnRRnHv0qZKV42LDOt1eyGULoSJBGpRBFprBJBQrBJBBWGSm8jld5WroHUCyEXKutNu2IJ1QtXetULL0uQFCu24WoRsV5KpYvGYzM4nv6X8zu/mTnznPkcWP6XPTPzzOf7/L7/OXPmzDlOHBCAAAScETjOWbyECwEIQEAYF0kAAQi4I4BxuZOMgCEAAYyLHIAABNwRwLjcSUbAEIAAxkUOQAAC7ghgXO4kI2AIQADjIgcgAAF3BDAud5IRMAQggHGRAxCAgDsCGJc7yQgYAhDAuMgBCEDAHQGMy51kBAwBCGBc5AAEIOCOAMblTjIChgAEMC5yAAIQcEcA43InGQFDAAIYFzkAAQi4I4BxuZOMgCEAAYyLHIAABNwRwLjcSUbAEIAAxkUOQAAC7ghgXO4kI2AIQADjIgcgAAF3BDAud5IRMAQggHGRAxCAgDsCGJc7yQgYAhDAuMgBCEDAHQGMy51kBAwBCGBc5AAEIOCOAMblTjIChgAEMC5yAAIQcEcA43InGQFDAAIYFzkAAQi4I4BxuZOMgCEAAYyLHIAABNwRwLjcSUbAEIAAxkUOQAAC7ghgXO4kI2AIQADjIgcgAAF3BDAud5IRMAQggHGRAxDwTeDTkr4s6UxJ/5F0QNK3JD3lu1tbR49xLVld+jYXgcskvSTpIkmnS/qgpJMk/Tv8bHHZ7+PXPw6M5kRJx0t6Ijkv9uUsSW+U9Iykczfp4K8lfXiuztdoF+OqQZk2vBEwUzFTsK9mQNFkotGkhvFeSc+G86NRtdDfd0h6tIVASsSAcZWgSp0eCJjJ7JR0SRgZ2SjHDMp+38Jho7PXTAzkBUmvn1jWRTGMy4VMBJmBgBnSpZLsMs7+paOodao3k/hLqCBe8j0cfj4Yvtp8k/1fPLaaf4pxxXPSS8r4/Vsl3SXp5EHgNjo8JukDkg6v06nWy2JcrSvUX3xmKjYSipdqF0h6V/jgp6Mh+2DHf0YpnSd6p6TTkjml7UZRL4bLPasnmo7VHb+PKsQ20rZTQ6ql1lclfXODxr4u6Ru1gpizHYxrTvq0beZkE9cfkXRxxcu0pyXZaMiMKX71dBfua5sY1Psk/baHtMK4elC5rT5eFS7Z7Otmd8VyRDwcRZkxmUlFo8rRxlx13Clpz6Dxn0r61FwB1W4X46pNvM/27PLPPmhmVhvNLUWTiaZil1/xEswMx/7fbv9bWfs5nfcxommdceQU55eWSNxGihcmHbMRZK45Oxe8MK75ZYofaku8MyQ9J+mQpKNJMqbzLfeHkIeTuPP35JUIbCSVToRvNrKyftqCSfs3nE9qqT+txWKT8OmxT9LnWguyZDwYV0m6m9dtH+SbJNlamw+tGIIl7Va6/VPS8xusP4rN2JojG8E8NrhUS+d4ht/bbfkTJP0umGk6ER7PtfkVmwR/wzaXgEck7Q1mNcfE9oq4mzx9aFxXB55NBlsiKIyrBNXt67xB0q3bn7aYM+xSxkZVNjez5Eu4GoLZ5fb+pCFb/mB/LLo6MK555LaRyUPzND251VUWRJpRxTt2cUJ8csMUfBUBG61en/ymu8tE6zvGNd+nwuao7N8PJO0Kz7JZNDbH9aSkv4fQ0su2RyS9VtKD4dJtOClt5+4Il4Fpz+KkdqzLnpuzdrY74vnppWG6ujx9xMXOsUWPjw8WW27XBv+/GgH7Q2Dzh/G4NoxkV6vF+dkYV1sCRoNpKyqiaYmA/TGxxbXxsD963d3YwLhaSkligcDWBIZTDHajo+RauGb1wLialYbAIPB/BO6Q9Pnkt7dJshs93R0YV3eS02HHBGz+8Owk/vN6nU/EuBxnMaF3RWC4DOJ7kr7UFYGksxhXr8rTb28Eho/5dDvaMuEwLm/pS7w9EhiOtu4Oz332yOLlPmNc3UpPx50QsCUytlg5vXvY5RKIVC+My0n2Ema3BG4Oz7VGAN2PthhxdftZoOOOCKQLTu1RKlvL1f3D6Yy4HGUwoXZHwLaq+X7S6xvDzhrdgRh2GOPqPgUA0DCB9LlE27tsu73zG+5K3tAwrrw8qQ0CuQjYZLztmRaP7vbc2gokxpUrzagHAnkJpNvXMNoasMW48iYbtUEgF4F0Up7RFsaVK6+oBwLFCKST8t3uAMGlYrH8omIIFCFg21zvDjV3uwMExlUkt6gUAkUIDCflu34mcTPCzHEVyT0qhcBkAumLVJiU3wQjxjU5vygIgSIE0l0gutxPfgxVjGsMJc6BQB0C9kC1vW4sHvbik/RlKXWicNAKxuVAJELshkC6fY29sdzecs6xAQGMi7SAQDsE7IW5e0I4PJe4hS4YVztJSyQQsF0fdgYM3E3EuPhEQKB5Aumrx7ibuI1cjLiaz2cC7IRAugyCy0SMq5O0p5veCaSr5blMxLi85zPxd0LgGUmnSOIycYTgXCqOgMQpEChMwJY93MfdxPGUMa7xrDgTAqUIxGUQ7Ck/kjDGNRIUp0GgIIG49xaXiSMhY1wjQXEaBAoRSFfLczdxJGSMayQoToNAIQLpannuJo6EjHGNBMVpEChEgMvECWAxrgnQKAKBTAS4TJwIEuOaCI5iEMhAgMvEiRAxrongKAaBDAS4TJwIEeOaCI5iEFiTQPpQNXcTV4SJca0IjNMhkIlA+sJX7iauCBXjWhEYp0MgE4G49xaLTicAxbgmQKMIBNYkkL6CjPcmToCJcU2ARhEIrEkgfVP1Lkn2Zh+OFQhgXCvA4lQIZCIQl0EckWSjL44VCWBcKwLjdAhkIHBY0vmS9kmy0RfHigQwrhWBcToE1iSQLoO4QtK9a9bXZXGMq0vZ6fSMBOLe8rb3ll0m8sLXCWJgXBOgUQQCaxA4KOlStmheg6AkjGs9fpSGwKoEXgoFbpF086qFOf9/BDAuMgEC9Qike8tfLslGXxwTCGBcE6BRBAITCdgI66ZQls/eRIiMuNYAR1EITCAQ57ful2SjL46JBHD9ieAoBoEJBJjfmgBtoyIYVyaQVAOBbQik67eulmRvruaYSADjmgiOYhBYkUBcv2XFdrB+a0V6g9MxrvX4URoCYwnwfOJYUiPOw7hGQOIUCGQgEPff4vnEDDAxrgwQqQIC2xBI99+6VpKNvjjWIIBxrQGPohAYSSDdf4ttmkdC2+o0jCsDRKqAwDYEmN/KnCIYV2agVAeBDQgclfQW9t/KlxsYVz6W1ASBjQiw/1aBvMC4CkClSggkBOLziey/lTEtMK6MMKkKAhsQsBdhXMj+W3lzA+PKy5PaIJASOF3SsfAL3ladMTcwrowwqQoCAwK8hqxQSmBchcBSLQTCg9S7Jdn8lo2+ODIRwLgygaQaCGxAwF6EcRrLIPLnBsaVnyk1QsAIXCVpf0DBNjaZcwLjygyU6iAQCOyVdH34nm1sMqcFxpUZKNVBIBCIu0HcHUZfgMlIAOPKCJOqIBAIpKvl2Q2iQFpgXAWgUmX3BLhMLJwCGFdhwFTfJQEuEwvLjnEVBkz13RHgpRgVJMe4KkCmia4IpA9Vs+i0kPQYVyGwVNstgQcl7WLRaVn9Ma6yfKm9LwLsvVVJb4yrEmia6YJAvJvIs4mF5ca4CgOm+q4I8GxiJbkxrkqgaWbxBNJnE22OyzYQ5ChEAOMqBJZquyMQ124dkWTvUeQoSADjKgiXqrshcJmk+0Jv2em0guwYVwXINLF4Agck2YaBdvDC1wpyY1wVINPEognYZeHvJZ0g6RFJFyy6t410DuNqRAjCcEvgBkm3huhvl3Sd2544ChzjciQWoTZJIL5+zILjbmIliTCuSqBpZpEE0tePsei0osQYV0XYNLU4Aunrx/ZJsp85KhDAuCpAponFErhT0p7QO5ZBVJQZ46oIm6YWR4D5rZkkxbhmAk+z7gkwvzWjhBjXjPBp2jWBz0i6K/TgN5Iucd0bZ8FjXM4EI9xmCMSdTi2gn0gyI+OoRADjqgSaZhZHIH3Mh1eQVZYX46oMnOYWQyDuBmEdulzSwcX0zEFHMC4HIhFikwReSqLiwerKEmFclYHT3CIIpNvYWIf4HFWWFeCVgdPcIgh8R9JXQk/+KulNi+iVo05gXI7EItRmCPxS0kdDNLalzXuaiayTQDCuToSmm9kI2MJT25751FDjLZJsaQRHRQIYV0XYNLUIAvdIujLpCXcUZ5AV45oBOk26JvCMpFNCD+zO4vGue+M0eIzLqXCEPQuBdBsbC+BeSVfMEknnjWJcnScA3V+JwJOS3pyUuFqSraDnqEwA46oMnOZcE0gXnVpH+PzMJCfgZwJPsy4JYFyNyIZxNSIEYbggMDSuHZKechH5woLEuBYmKN0pSoARV1G84yvHuMaz4sy+CQzvKB6VdE7fSObrPcY1H3ta9kVgeEeRt/rMqB/GNSN8mnZFYHiZyIr5GeXDuGaET9NuCFwlaX8SLTtCzCwdxjWzADTvgkC6v7wFfJukG1xEvtAgMa6FCku3shL4s6QzkxpZMZ8V7+qVYVyrM6NEfwSel3Ri0m3Wb82cAxjXzALQfPMEhvNbf5D07uajXniAGNfCBaZ7axN4VNLbk1pulLR37VqpYC0CGNda+Ci8cAK22+mxQR95o08DomNcDYhACM0SGK6Wt3cpmnFxzEwA45pZAJpvmsBwtTyXiY3IhXE1IgRhNElguFqey8RGZMK4GhGCMJojMLybyGViQxJhXA2JQShNEbhT0p4kIlbLNyQPxtWQGITSFAH2l29KjlcHg3E1LA6hzUrgxcGe8nxWZpUD42oIP6E0SuAiSQ8NYtsl6eFG4+0uLP6KdCc5HR5BYKOFp+y/NQJcrVMwrlqkaccTgQckXTwI+DJJ93vqxJJjxbiWrC59m0LgfEmHBwX/JemEKZVRpgwBjKsMV2r1S8BGVvcNwv+spB/67dLyIse4lqcpPVqPwEbGxcaB6zHNXhrjyo6UCp0TuFLSPYM+XCPpx877tajwMa5FyUlnMhCwveRvHdTDjqcZwOasAuPKSZO6lkDggKTdSUeOSDp3CR1bUh8wriWpSV9yEPiHpJOSinhGMQfVzHVgXJmBUp17AsOtbFgx36CkGFeDohDSbASGj/r8TdIZs0VDw5sSwLhIDgi8QmC4VfPdkmxfLo7GCGBcjQlCOLMSGO7BxVbNs8qxeeMYV6PCENYsBGyX051JyzxYPYsM2zeKcW3PiDP6ITCcmGf9VqPaY1yNCkNY1QkMJ+YPSbLfcTRIAONqUBRCmoXA8BlF1m/NIsO4RjGucZw4a/kEhncUebC6Yc0xrobFIbSqBIbPKDK/VRX/ao1hXKvx4uzlEtgr6frQvUckXbDcrvrvGcblX0N6kIdAaly/kPTxPNVSSwkCGFcJqtTpkUC6+JSFp40riHE1LhDhVSNwUNKloTUm5qthn9YQxjWNG6WWRyA1LlbMN64vxtW4QIRXjcBTkk4LrWFc1bBPawjjmsaNUssjkD7ug3E1ri/G1bhAhFeNQGpcbB5YDfu0hjCuadwotTwCqXGdJ8l2iuBolADG1agwhFWdQGpcfC6q41+tQQRajRdnL5dANK6nJZ2+3G4uo2cY1zJ0pBfrEbDXjz0WquB1ZOuxrFIa46qCmUYaJ/AJST8PMf5K0scaj7f78DCu7lMAAJLSnSFul3QdVNomgHG1rQ/R1SGQPmDNGq46zNdqBeNaCx+FF0LgYUkXhr6wFMKBqBiXA5EIsTgB7igWR5y3AYwrL09q80cg3WueF8A60Q/jciIUYRYjcLOkm0Lt7MNVDHPeijGuvDypzR+BdH6LZxSd6IdxORGKMIsQsBXyx0LNLDwtgrhMpRhXGa7U6oNA+kqyfZLsZw4HBDAuByIRYjEC6T7zbNdcDHP+ijGu/Eyp0Q+BuOspD1b70ezlSDEuZ4IRbjYCF0l6KNTGZWI2rHUqwrjqcKaV9gikj/lwmdiePltGhHE5E4xwsxGIyyC4TMyGtF5FGFc91rTUFoEXJL1OEqvl29JlVDQY1yhMnLQwAuljPl+QdMfC+rf47mBci5eYDm5AIJ3fYjcIhymCcTkUjZDXJhDnt1gtvzbKeSrAuObhTqvzEUj3l78t7H46XzS0PIkAxjUJG4UcE0i3aWYZhFMhMS6nwhH2ZAIHJO0Opcn/yRjnLYhw8/Kn9foE4m6nhyTZ6nkOhwQwLoeiEfJkAryGbDK6tgpiXG3pQTRlCaS7nfJ8YlnWRWvHuIripfLGCLCNTWOCTA0H45pKjnIeCaTbNPP+RI8KclfFsWqEPpVAnJi38jsk2X5cHA4JMOJyKBohTyaQGhe5Pxnj/AURb34NiKAOgXTjQLayqcO8WCsYVzG0VNwYgXRHCNZwNSbOquFgXKsS43yvBOxlr98OwT8g6f1eO0Lc7DlPDvRD4LuSvhi6+zNJn+yn68vrKSOu5WlKjzYmkD6jaKMv25OLwykBjMupcIS9MoH4KjIryK4QK+NrqwDG1ZYeRFOGQDoxby2whqsM52q1YlzVUNPQjAR+JOma0P5zkk6eMRaazkAA48oAkSqaJ/CEpLNClM9KOrX5iAlwSwIYFwmydAJnS3p80MlzJB1deseX3D+Ma8nq0rdIwF6K8bbww58k7QSNbwIYl2/9iH4cAdtA0O4k2rFf0r3jinFWqwQwrlaVIS4IQGBTAhgXyQEBCLgjgHG5k4yAIQABjIscgAAE3BHAuNxJRsAQgADGRQ5AAALuCGBc7iQjYAhAAOMiByAAAXcEMC53khEwBCCAcZEDEICAOwIYlzvJCBgCEMC4yAEIQMAdAYzLnWQEDAEIYFzkAAQg4I4AxuVOMgKGAAQwLnIAAhBwRwDjcicZAUMAAhgXOQABCLgjgHG5k4yAIQABjIscgAAE3BHAuNxJRsAQgADGRQ5AAALuCGBc7iQjYAhAAOMiByAAAXcEMC53khEwBCCAcZEDEICAOwIYlzvJCBgCEMC4yAEIQMAdAYzLnWQEDAEIYFzkAAQg4I4AxuVOMgKGAAQwLnIAAhBwRwDjcicZAUMAAhgXOQABCLgjgHG5k4yAIQABjIscgAAE3BHAuNxJRsAQgADGRQ5AAALuCGBc7iQjYAhAAOMiByAAAXcEMC53khEwBCCAcZEDEICAOwIYlzvJCBgCEMC4yAEIQMAdAYzLnWQEDAEIYFzkAAQg4I4AxuVOMgKGAAQwLnIAAhBwR+C/doIhTZIi/uMAAAAASUVORK5CYII="
         image_b64 = data_uri.split(",")[1]
         binary = base64.b64decode(image_b64)
         image = np.asarray(Image.open(io.BytesIO(binary)))
         return image
 
     ##############################
@@ -571,14 +607,15 @@
         except (ImportError, ModuleNotFoundError) as err:
             raise RuntimeError(
                 '{} depends on extenral package. Please install '.format(self.__class__.__name__)) from err
 
         import os
         import time
         import json
+
         class DumpHistoryCallback(keras.callbacks.Callback):
             def __init__(self, dump_path):
                 super().__init__()
                 if os.path.isdir(dump_path):
                     dump_path = os.path.join(dump_path,
                                              '__view__training-history__{}.json'.format(time.strftime("%F-%X")))
                 self.dump_file = dump_path
```

## dtlpy/repositories/artifacts.py

```diff
@@ -269,15 +269,15 @@
                         if not m_artifact.is_fetched:
                             m_artifact = self.items_repository.get(item_id=m_artifact.id)
                         model_remote_root = m_artifact.filename.split('/')
                         model_remote_root = '/'.join(model_remote_root[:4])
                         # remove the prefix with relpath
                         local_dst = os.path.join(local_path,
                                                  os.path.relpath(m_artifact.filename, model_remote_root))
-                        if not os.path.isfile(local_dst):
+                        if not os.path.isfile(local_dst) or overwrite:
                             # need_to_download
                             # 1. download to temp folder
                             temp_dir = tempfile.mkdtemp()
                             local_temp_file = m_artifact.download(
                                 local_path=temp_dir,
                                 overwrite=overwrite,
                                 to_items_folder=False,
@@ -290,15 +290,15 @@
                             # clean temp dir
                             if os.path.isdir(temp_dir):
                                 shutil.rmtree(temp_dir)
                         artifact.append(local_path)
                     elif isinstance(m_artifact, entities.LinkArtifact):
                         # remove the prefix with relpath
                         local_dst = os.path.join(local_path, m_artifact.filename)
-                        if not os.path.isfile(local_dst):
+                        if not os.path.isfile(local_dst) or overwrite:
                             # need_to_download
                             # 1. download to temp folder
                             temp_dir = tempfile.mkdtemp()
                             response = requests.get(m_artifact.url, stream=True)
                             local_temp_file = os.path.join(temp_dir, m_artifact.filename)
                             with open(local_temp_file, "wb") as handle:
                                 for data in response.iter_content(chunk_size=8192):
```

## dtlpy/repositories/datasets.py

```diff
@@ -362,15 +362,15 @@
                                                          json_req=patch)
         if success:
             logger.info('Dataset was updated successfully')
             return dataset
         else:
             raise exceptions.PlatformException(response)
 
-    @_api_reference.add(path='/datasets/{}/directoryTree', method='get')
+    @_api_reference.add(path='/datasets/{id}/directoryTree', method='get')
     def directory_tree(self,
                        dataset: entities.Dataset = None,
                        dataset_name: str = None,
                        dataset_id: str = None):
         """
         Get dataset's directory tree.
 
@@ -382,15 +382,15 @@
         :param str dataset_name: The Name of the dataset
         :param str dataset_id: The Id of the dataset
         :return: DirectoryTree
 
         **Example**:
 
         .. code-block:: python
-
+            directory_tree = dataset.directory_tree
             directory_tree = project.datasets.directory_tree(dataset='dataset_entity')
         """
         if dataset is None and dataset_name is None and dataset_id is None:
             raise exceptions.PlatformException('400', 'Must provide dataset, dataset name or dataset id')
         if dataset_id is None:
             if dataset is None:
                 dataset = self.get(dataset_name=dataset_name)
```

## dtlpy/repositories/drivers.py

```diff
@@ -51,15 +51,15 @@
             _json = response.json()
             driver = self._getDriverClass(_json).from_json(client_api=self._client_api,
                                                            _json=_json)
         else:
             raise exceptions.PlatformException(response)
         return driver
 
-    @_api_reference.add(path='/drivers?projectId={id}', method='get')
+    @_api_reference.add(path='/drivers', method='get')
     def list(self) -> miscellaneous.List[entities.Driver]:
         """
         Get the project's drivers list.
 
         **Prerequisites**: You must be in the role of an *owner* or *developer*.
 
         :return: List of Drivers objects
```

## dtlpy/repositories/executions.py

 * *Ordering differences only*

```diff
@@ -595,19 +595,19 @@
                                                      project=self._project,
                                                      service=self._service)
             if timeout is None:
                 timeout = execution.service.execution_timeout + 60
             if execution.latest_status['status'] not in ['inProgress', 'created', 'in-progress', 'rerun']:
                 break
             elapsed = int(time.time()) - start
-            sleep_time = np.minimum(timeout - elapsed, 2 ** i)
-            time.sleep(sleep_time)
             i += 1
             if i > 18 or elapsed > timeout:
                 break
+            sleep_time = np.minimum(timeout - elapsed, 2 ** i)
+            time.sleep(sleep_time)
         if execution is None:
             raise ValueError('Nothing to wait for')
         if elapsed >= timeout:
             raise TimeoutError("execution wait() got timeout. id: {!r}, status: {}".format(
                 execution.id, execution.latest_status))
         return execution
```

## dtlpy/repositories/features.py

```diff
@@ -200,38 +200,14 @@
         # check response
         if success:
             logger.debug("Feature deleted successfully")
             return success
         else:
             raise exceptions.PlatformException(response)
 
-    @_api_reference.add(path='/features/vectors/{id}', method='patch')
-    def update(self, feature: entities.Feature) -> entities.Feature:
-        """
-        Update Feature Vector changes to platform
-
-        :param entities.Feature feature: Feature object to update
-        :return: dl.Feature entity
-        """
-        # payload
-        payload = feature.to_json()
-
-        # request
-        success, response = self._client_api.gen_request(req_type='patch',
-                                                         path="{}/{}".format(self.URL, feature.id),
-                                                         json_req=payload)
-
-        # exception handling
-        if not success:
-            raise exceptions.PlatformException(response)
-
-        # return entity
-        return entities.Feature.from_json(_json=response.json(),
-                                          client_api=self._client_api)
-
     def _build_entities_from_response(self, response_items) -> miscellaneous.List[entities.Item]:
         pool = self._client_api.thread_pools(pool_name='entity.create')
         jobs = [None for _ in range(len(response_items))]
         # return triggers list
         for i_item, item in enumerate(response_items):
             jobs[i_item] = pool.submit(entities.Feature._protected_from_json,
                                        **{'client_api': self._client_api,
```

## dtlpy/repositories/models.py

```diff
@@ -385,15 +385,17 @@
         :param dtlpy.entities.filters.Filters train_filter: Filters entity or a dictionary to define the items' scope in the specified dataset_id for the model train
         :param dtlpy.entities.filters.Filters validation_filter: Filters entity or a dictionary to define the items' scope in the specified dataset_id for the model validation
         :return: dl.Model which is a clone version of the existing model
         """
         from_json = {"name": model_name,
                      "packageId": from_model.package_id,
                      "configuration": from_model.configuration,
-                     "metadata": from_model.metadata}
+                     "metadata": from_model.metadata,
+                     "outputType": from_model.output_type,
+                     "inputType": from_model.input_type}
         if project_id is None:
             project_id = self.project.id
         from_json['projectId'] = project_id
         if dataset is not None:
             if labels is None:
                 labels = list(dataset.labels_flat_dict.keys())
             from_json['datasetId'] = dataset.id
@@ -422,15 +424,15 @@
                                                          path='/ml/models/{}/clone'.format(from_model.id),
                                                          json_req=from_json)
         if not success:
             raise exceptions.PlatformException(response)
         new_model = entities.Model.from_json(_json=response.json(),
                                              client_api=self._client_api,
                                              project=self._project,
-                                             package=from_model.package)
+                                             package=from_model._package)
 
         if new_model._dataset is not None and new_model._dataset.readonly is False:
             logger.warning(
                 "Model is using an unlocked dataset {!r}. Make it readonly for training reproducibility".format(
                     new_model.dataset.name))
 
         return new_model
```

## dtlpy/repositories/organizations.py

```diff
@@ -206,15 +206,15 @@
             # return good jobs
             organization = miscellaneous.List([r[1] for r in results if r[0] is True])
         else:
             logger.error('Platform error getting organization')
             raise exceptions.PlatformException(response)
         return organization
 
-    @_api_reference.add(path='/orgs/{id}', method='get')
+    @_api_reference.add(path='/orgs/{org_id}', method='get')
     def get(self,
             organization_id: str = None,
             organization_name: str = None,
             fetch: bool = None) -> entities.Organization:
         """
         Get Organization object to be able to use it in your code.
```

## dtlpy/repositories/pipeline_executions.py

```diff
@@ -52,15 +52,15 @@
         if not isinstance(pipeline, entities.Pipeline):
             raise ValueError('Must input a valid pipeline entity')
         self._pipeline = pipeline
 
     ###########
     # methods #
     ###########
-    @_api_reference.add(path='/pipelines/{pipeline_id}/executions/{executionId}', method='get')
+    @_api_reference.add(path='/pipelines/{pipelineId}/executions/{executionId}', method='get')
     def get(self,
             pipeline_execution_id: str,
             pipeline_id: str = None
             ) -> entities.PipelineExecution:
         """
         Get Pipeline Execution object
 
@@ -211,15 +211,18 @@
         """
         if pipeline_id is None:
             if self._pipeline is None:
                 raise exceptions.PlatformException('400', 'Please provide pipeline id')
             pipeline_id = self._pipeline.id
 
         payload = dict()
-        if isinstance(execution_input, dict):
+        if execution_input is None:
+            # support pipeline executions without any input
+            pass
+        elif isinstance(execution_input, dict):
             payload['input'] = execution_input
         else:
             if not isinstance(execution_input, list):
                 execution_input = [execution_input]
             if len(execution_input) > 0 and isinstance(execution_input[0], entities.FunctionIO):
                 payload['input'] = dict()
                 for single_input in execution_input:
```

## dtlpy/repositories/pipelines.py

```diff
@@ -278,22 +278,20 @@
         .. code-block:: python
 
             is_deleted = project.pipelines.delete(pipeline_id='pipeline_id')
        """
         # get id and name
         if pipeline_id is None:
             if pipeline is None:
-                pipeline = self.get(pipeline_id=pipeline_id, pipeline_name=pipeline_name)
+                pipeline = self.get(pipeline_name=pipeline_name)
             pipeline_id = pipeline.id
 
         # request
-        success, response = self._client_api.gen_request(
-            req_type="delete",
-            path="/pipelines/{}".format(pipeline_id)
-        )
+        success, response = self._client_api.gen_request(req_type="delete",
+                                                         path="/pipelines/{}".format(pipeline_id))
 
         # exception handling
         if not success:
             raise exceptions.PlatformException(response)
 
         # return results
         return True
@@ -331,14 +329,41 @@
         # return entity
         return entities.Pipeline.from_json(
             _json=response.json(),
             client_api=self._client_api,
             project=self._project
         )
 
+    def __update_variables(self, pipeline: entities.Pipeline):
+        pipeline_json = pipeline.to_json()
+        variables = pipeline_json['variables']
+
+        for var in variables:
+            if var.get('reference', None) is None:
+                var['reference'] = pipeline.id
+
+        # payload
+        payload = {'variables': variables}
+
+        # request
+        success, response = self._client_api.gen_request(
+            req_type='patch',
+            path='/pipelines/{}/variables'.format(pipeline.id),
+            json_req=payload
+        )
+        if not success:
+            raise exceptions.PlatformException(response)
+
+        # return entity
+        return entities.Pipeline.from_json(
+            _json=response.json(),
+            client_api=self._client_api,
+            project=self._project
+        )
+
     @_api_reference.add(path='/pipelines/{pipelineId}', method='patch')
     def update(self,
                pipeline: entities.Pipeline = None
                ) -> entities.Pipeline:
         """
         Update pipeline changes to platform.
 
@@ -358,15 +383,14 @@
         # payload
         payload = pipeline.to_json()
 
         # update settings
         if pipeline.settings_changed():
             self.update_settings(pipeline=pipeline, settings=pipeline.settings)
 
-        # request
         success, response = self._client_api.gen_request(
             req_type='patch',
             path='/pipelines/{}'.format(pipeline.id),
             json_req=payload
         )
 
         # exception handling
@@ -423,15 +447,15 @@
                                                    _json=response.json(),
                                                    project=self.project)
         else:
             raise exceptions.PlatformException(response)
         assert isinstance(pipeline, entities.Pipeline)
         return pipeline
 
-    @_api_reference.add(path='/pipelines/{compositionsId}/install', method='post')
+    @_api_reference.add(path='/pipelines/{pipelineId}/install', method='post')
     def install(self, pipeline: entities.Pipeline = None, resume_option: entities.PipelineResumeOption = None):
         """
         Install (start) a pipeline.
 
         **prerequisites**: You must be an *owner* or *developer* to use this method.
 
         :param dtlpy.entities.pipeline.Pipeline pipeline: pipeline entity
@@ -454,15 +478,19 @@
             path='/pipelines/{}/install'.format(pipeline.id),
             json_req=payload
         )
 
         if not success:
             raise exceptions.PlatformException(response)
 
-    @_api_reference.add(path='/pipelines/{compositionsId}/uninstall', method='post')
+        return entities.Pipeline.from_json(client_api=self._client_api,
+                                           _json=response.json(),
+                                           project=self.project)
+
+    @_api_reference.add(path='/pipelines/{pipelineId}/uninstall', method='post')
     def pause(self, pipeline: entities.Pipeline = None, keep_triggers_active: bool = None):
         """
         Pause a pipeline.
 
         **prerequisites**: You must be an *owner* or *developer* to use this method.
 
         :param dtlpy.entities.pipeline.Pipeline pipeline: pipeline entity
```

## dtlpy/repositories/projects.py

```diff
@@ -175,15 +175,15 @@
                                                          path=url,
                                                          json_req=payload)
 
         if not success:
             raise exceptions.PlatformException(response)
         return True
 
-    @_api_reference.add(path='projects /{project_id}/members/{user_id}', method='post')
+    @_api_reference.add(path='/projects/{project_id}/members/{user_id}', method='post')
     def add_member(self, email: str, project_id: str, role: entities.MemberRole = entities.MemberRole.DEVELOPER):
         """
         Add a member to the project.
 
         **Prerequisites**: You must be in the role of an *owner* to add a member to a project.
 
         :param str email: member email
@@ -209,15 +209,15 @@
                                                          path=url_path,
                                                          json_req=payload)
         if not success:
             raise exceptions.PlatformException(response)
 
         return response.json()
 
-    @_api_reference.add(path='projects /{project_id}/members/{user_id}', method='patch')
+    @_api_reference.add(path='/projects/{project_id}/members/{user_id}', method='patch')
     def update_member(self, email: str, project_id: str, role: entities.MemberRole = entities.MemberRole.DEVELOPER):
         """
         Update member's information/details in the project.
 
         **Prerequisites**: You must be in the role of an *owner* to update a member.
 
         :param str email: member email
@@ -243,15 +243,15 @@
                                                          path=url_path,
                                                          json_req=payload)
         if not success:
             raise exceptions.PlatformException(response)
 
         return response.json()
 
-    @_api_reference.add(path='projects /{project_id}/members/{user_id}', method='delete')
+    @_api_reference.add(path='/projects/{project_id}/members/{user_id}', method='delete')
     def remove_member(self, email: str, project_id: str):
         """
         Remove a member from the project.
 
         **Prerequisites**: You must be in the role of an *owner* to delete a member from a project.
 
         :param str email: member email
@@ -269,15 +269,15 @@
         success, response = self._client_api.gen_request(req_type='delete',
                                                          path=url_path)
         if not success:
             raise exceptions.PlatformException(response)
 
         return response.json()
 
-    @_api_reference.add(path='projects /{id}/members', method='get')
+    @_api_reference.add(path='/projects/{id}/members', method='get')
     def list_members(self, project: entities.Project, role: entities.MemberRole = None):
         """
         Get a list of the project members.
 
         **Prerequisites**: You must be in the role of an *owner* to list project members.
 
         :param dtlpy.entities.project.Project project: Project object
```

## dtlpy/repositories/tasks.py

```diff
@@ -787,15 +787,15 @@
             filters._ref_task_id = task_id
             filters._ref_op = op
             return dataset.items.update(filters=filters)
         finally:
             if filters is not None:
                 filters._nullify_refs()
 
-    @_api_reference.add(path='annotationtasks/{id}/addToTask', method='post')
+    @_api_reference.add(path='/annotationtasks/{id}/addToTask', method='post')
     def add_items(self,
                   task: entities.Task = None,
                   task_id=None,
                   filters: entities.Filters = None,
                   items=None,
                   assignee_ids=None,
                   query=None,
@@ -883,15 +883,15 @@
                                                    .format(response))
         else:
             raise exceptions.PlatformException(response)
 
         assert isinstance(task, entities.Task)
         return task
 
-    @_api_reference.add(path='annotationtasks/{id}/removeFromTask', method='post')
+    @_api_reference.add(path='/annotationtasks/{id}/removeFromTask', method='post')
     def remove_items(self,
                      task: entities.Task = None,
                      task_id=None,
                      filters: entities.Filters = None,
                      query=None,
                      items=None,
                      wait=True):
```

## tests/features/environment.py

```diff
@@ -264,15 +264,15 @@
 
 @fixture
 def drivers_delete(context):
     if not hasattr(context, 'to_delete_drivers_ids'):
         return
 
     all_deleted = True
-    time.sleep(7)  # Wait for datasets to delete
+    time.sleep(25)  # Wait for datasets to delete
     for driver_id in context.to_delete_drivers_ids:
         try:
             context.project.drivers.delete(driver_id=driver_id, sure=True, really=True)
         except context.dl.exceptions.NotFound:
             pass
         except:
             all_deleted = False
```

## Comparing `dtlpy-1.77.18.data/scripts/dlp.py` & `dtlpy-1.78.17.data/scripts/dlp.py`

 * *Files identical despite different names*

## Comparing `dtlpy-1.77.18.dist-info/LICENSE` & `dtlpy-1.78.17.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `dtlpy-1.77.18.dist-info/METADATA` & `dtlpy-1.78.17.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: dtlpy
-Version: 1.77.18
+Version: 1.78.17
 Summary: SDK and CLI for Dataloop platform
 Home-page: https://github.com/dataloop-ai/dtlpy
 Author: Dataloop Team
 Author-email: info@dataloop.ai
 License: Apache License 2.0
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: 3
```

## Comparing `dtlpy-1.77.18.dist-info/RECORD` & `dtlpy-1.78.17.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 dtlpy/__init__.py,sha256=rcq_EJ5f43HoNbDkt9No2t6OBTbVVkrE6aGhUTK3K34,19847
-dtlpy/__version__.py,sha256=AZYRpSLHVVC6PZ8duuqJuHeOapajW1ynmsUF7Fuh_Lo,20
+dtlpy/__version__.py,sha256=i19AaQ9j8AcTWpkaxaeEl-gd3R5t0NBwh32tCinWk3c,20
 dtlpy/exceptions.py,sha256=EQCKs3pwhwZhgMByQN3D3LpWpdxwcKPEEt-bIaDwURM,2871
 dtlpy/new_instance.py,sha256=6dljwr1Zo25ng57PMn8j0jjRD1hJ5LwlvBTBNmyc3Qw,5654
 dtlpy/assets/__init__.py,sha256=D_hAa6NM8Zoy32sF_9b7m0b7I-BQEyBFg8-9Tg2WOeo,976
 dtlpy/assets/lock_open.png,sha256=vXHune4YF__fINPQ2l61G2zI3BeJPX_z5gkwzUNFAxs,24081
 dtlpy/assets/main.py,sha256=N1JUsx79qnXI7Hx22C8JOzHJdGHxvrXeTx5UZAxvJfE,1380
 dtlpy/assets/main_partial.py,sha256=d8Be4Whg9Tb2VFiT85-57_L9IvxRipQXiZ83SxFs0Ro,267
 dtlpy/assets/mock.json,sha256=aByh4XlsFQJM2pOjmd7bd9zT1LSOj5pfutZDHwt8c_8,149
@@ -59,33 +59,33 @@
 dtlpy/entities/codebase.py,sha256=pwRkAq2GV0wvmzshg89IAmE-0I2Wsy_-QNOu8OV8uqc,8999
 dtlpy/entities/command.py,sha256=SJNGDonOHILhDyJlixt0ZYJngJBTCANjyGmLfkOfjVI,4978
 dtlpy/entities/dataset.py,sha256=QxXKeVUgBa7oYwSkeevLc4E7p1pRKZRJ4fLpeH_rhAg,44074
 dtlpy/entities/directory_tree.py,sha256=Rni6pLSWytR6yeUPgEdCCRfTg_cqLOdUc9uCqz9KT-Q,1186
 dtlpy/entities/dpk.py,sha256=WgGkABFxSL9Y9pBxA9i0pD4xaI372gU6pbUEUzmNkIs,12627
 dtlpy/entities/driver.py,sha256=O_QdK1EaLjQyQkmvKsmkNgmvmMb1mPjKnJGxK43KrOA,7197
 dtlpy/entities/execution.py,sha256=e_az-Pln_ZlLsxmVsx3t4Kc034j7MniIm1o1cSO-4Ew,12377
-dtlpy/entities/feature.py,sha256=zPv9zLtpLwYquUA9hgOHGZFbRzNjtmFDLMEg0ZF8XME,4372
+dtlpy/entities/feature.py,sha256=D7_Kki6oh4tyseg7ufSSyvchwgF4a8cH2vsLHlcv3zA,4192
 dtlpy/entities/feature_set.py,sha256=-WtAIzQBOYd1yBjqEfvm0bHlovdqDdfBitAjg2XLszs,4587
 dtlpy/entities/filters.py,sha256=dGDjLTulqPoqEdx3moCxYMPkBz1ocYLv2HmA0PUP2rA,18485
 dtlpy/entities/integration.py,sha256=qXN9cTp4j8uCODOEu8DZ1It-6joRSxIsgucZcCsrPQs,5366
 dtlpy/entities/item.py,sha256=AnNfe5ZQURq5YiougoQRsVxTDkU-jSplzBKWC10mEtw,27922
 dtlpy/entities/label.py,sha256=ycDYavIgKhz806plIX-64c07_TeHpDa-V7LnfFVe4Rg,3869
 dtlpy/entities/links.py,sha256=FAmEwHtsrqKet3c0UHH9u_gHgG6_OwF1-rl4xK7guME,2516
 dtlpy/entities/message.py,sha256=ApJuaKEqxATpXjNYUjGdYPu3ibQzEMo8-LtJ_4xAcPI,5865
 dtlpy/entities/model.py,sha256=US7omMZktHFdSC3KYKwWieY6V6LsZQNURYWCdMTLHiI,18385
-dtlpy/entities/node.py,sha256=JZDXiZo6ocvKSlq7CRGx3UlMf8_VeGm6eWQZvsIqghA,35907
+dtlpy/entities/node.py,sha256=-cZPaFHwbU4Cuwd5PYMiskrP4HJncXM51llyNjW9en8,36116
 dtlpy/entities/ontology.py,sha256=v1gaVSWwsz-2-HqChPInLBVavcb65NOjDTxixwOXVlw,29275
 dtlpy/entities/organization.py,sha256=AMkx8hNIIIjnu5pYlNjckMRuKt6H3lnOAqtEynkr7wg,9893
-dtlpy/entities/package.py,sha256=kP6fS9dmh4lY2ck4lS_hcrGfIUAeleUJFNMNPvI1ufM,26028
+dtlpy/entities/package.py,sha256=jQ2gzaqc8UTLOJ90Hj4jCIPBu59Joo-e2GDY443Syxw,26199
 dtlpy/entities/package_defaults.py,sha256=wTD7Z7rGYjVy8AcUxTFEnkOkviiJaLVZYvduiUBKNZo,211
 dtlpy/entities/package_function.py,sha256=AdXMw5e5a7TT9oaxPqfu2ERDgYTmH_sm6GgpGDu67b8,5917
 dtlpy/entities/package_module.py,sha256=MBaJ5j8eCERsP-s1SIO8_daTU1gEqcaDSpUBu_gUTAk,4035
 dtlpy/entities/package_slot.py,sha256=0dkTUN1wfjyAkOtdoFa6vGZBuZ_UoenpePaSFX9HOYA,5694
 dtlpy/entities/paged_entities.py,sha256=A6_D0CUJsN52dBG6yn-oHHzjuVDkBNejTG5r-KxWOxI,5848
-dtlpy/entities/pipeline.py,sha256=Ttw5jWE1EHCfy0q55weGdHcLkylcQqJLmB3OZTyGL_E,17574
+dtlpy/entities/pipeline.py,sha256=E6yOyuJ6xAR67M4aia-_PcrDpFUFYNR1kYZPnoSqblk,19306
 dtlpy/entities/pipeline_execution.py,sha256=c8TBWDFWGTnIXIavmMGHaxUBKgTDMJhbQJhK6q-3i_k,7215
 dtlpy/entities/project.py,sha256=FCGKA-pV-AOaeD5b2S6jEX-TAZSgVM2N-vTSEn0Av-k,14301
 dtlpy/entities/recipe.py,sha256=RzevJdJAeH65Xm0bL2_5rzST_4UodSewUBT-Wj3_UU0,9541
 dtlpy/entities/reflect_dict.py,sha256=2NaSAL-CO0T0FYRYFQlaSpbsoLT2Q18AqdHgQSLX5Y4,3273
 dtlpy/entities/resource_execution.py,sha256=1HuVV__U4jAUOtOkWlWImnM3Yts8qxMSAkMA9sBhArY,5033
 dtlpy/entities/service.py,sha256=AcyTskHUYUvhcUBWTkI8YLzW9UknYFgQQz37VtDxKK4,27221
 dtlpy/entities/setting.py,sha256=lnEyXKgSgpwQDNZAiOs27gguwp7oaXc8XjFAkM4ajpE,8467
@@ -138,53 +138,53 @@
 dtlpy/examples/upload_segmentation_annotations_from_mask_image.py,sha256=JQGc8wQ3zTRRlVcRLs223UwCYCAfChKlvU0QOPEqezI,1388
 dtlpy/examples/upload_yolo_format_annotations.py,sha256=PDLhC5pBGrC68Pix-7I7SgdaCweYNZPgJxg0h4ssWyc,2610
 dtlpy/miscellaneous/__init__.py,sha256=twbvfsKdiNHNR-vUuy8nUlY3vuUVaSnm-wO83yQdeFY,829
 dtlpy/miscellaneous/dict_differ.py,sha256=POJbKR0YyWPf5gFADFpIaNFj9gt2aVBTNof7GJNxTCw,3489
 dtlpy/miscellaneous/git_utils.py,sha256=CT_CCDsqDqu_bY3cLcOSU6k3Zr6w40t8GJULLUtAJ_U,7971
 dtlpy/miscellaneous/json_utils.py,sha256=0P4YTlL6o_L7AUrvAeqkqA46MZZK_hDdTrdnmI59y6g,428
 dtlpy/miscellaneous/list_print.py,sha256=leEg3RodgYfH5t_0JG8VuM8NiesR8sJLK_mRSttL5pY,4808
-dtlpy/miscellaneous/zipping.py,sha256=dZ2FlakSQsN4Syo26WRS3jQ4j86Ny1bhQR6o2bi8Itc,5118
+dtlpy/miscellaneous/zipping.py,sha256=GMdPhAeHQXeMS5ClaiKWMJWVYQLBLAaJUWxvdYrL4Ro,5337
 dtlpy/ml/__init__.py,sha256=vPkyXpc9kcWWZ_PxyPEOsjKBJdEbowLkZr8FZIb_OBM,799
-dtlpy/ml/base_model_adapter.py,sha256=W-4jMZvyUAbl3n4zsjR7Vjmh5u9Ug3KMv0L8Y9UeEq0,34615
+dtlpy/ml/base_model_adapter.py,sha256=qtYdi_QuTulJI3dSRwLPj1AMQiiwoP0hFFTpzaP4e0M,35692
 dtlpy/ml/metrics.py,sha256=BG2E-1Mvjv2e2No9mIJKVmvzqBvLqytKcw3hA7wVUNc,20037
 dtlpy/ml/predictions_utils.py,sha256=He_84U14oS2Ss7T_-Zj5GDiBZwS-GjMPURUh7u7DjF8,12484
 dtlpy/ml/summary_writer.py,sha256=dehDi8zmGC1sAGyy_3cpSWGXoGQSiQd7bL_Thoo8yIs,2784
 dtlpy/ml/train_utils.py,sha256=avvT_TbwJ0Q23mwwHRf0cu6Wt4LU72plEb_lx5oxc1U,2444
 dtlpy/repositories/__init__.py,sha256=tUw86r7hI6f4BSas7_csOCfEso4ABUcdzx8WfR4ipw4,1883
 dtlpy/repositories/analytics.py,sha256=dQPCYTPAIuyfVI_ppR49W7_GBj0033feIm9Gd7LW1V0,2966
 dtlpy/repositories/annotations.py,sha256=aDRphlzfjJbDfoq3gnJIZDn2Eyk1Eptg24ZlGMnkDNU,35175
 dtlpy/repositories/apps.py,sha256=eU43v1CirGir6EnZBoesihkcm95JOzke_P8cFLUAwi4,10299
-dtlpy/repositories/artifacts.py,sha256=FnMzo_dHq8hulqJOHCNtmZk4m2eBuh4i0hz8JD4--zs,19528
+dtlpy/repositories/artifacts.py,sha256=TsZe2OY_sBhQKUzLBLuz37QrfNi1uMAOQGSlePWBlIU,19554
 dtlpy/repositories/assignments.py,sha256=L5pxRfmiZxHMWPTvGFds4mV9lAdDU_ZzXoTuyJCmMgc,25410
 dtlpy/repositories/bots.py,sha256=x1uOCl-wti83zFZii727-MwufRoChYeuRkOsRyixfPk,8195
 dtlpy/repositories/codebases.py,sha256=fEO9v1nElDZedFfFvSch0Jmby-WF2ZZcXAW9pRJHXo0,25183
 dtlpy/repositories/commands.py,sha256=nfk-KFYArfekyFMsnJwj6IgT7VawVcJQs6GGO8q26_U,5233
-dtlpy/repositories/datasets.py,sha256=Quewq3jJs9UcE2_CC9qXXoWorPvFll7Q1sOEIWmkw-k,42429
+dtlpy/repositories/datasets.py,sha256=1l6IswUtyZ8f9lwLUSrArj_QMVt6hYH-6JR670Zm3is,42482
 dtlpy/repositories/downloader.py,sha256=JpTtDl6acbWDPx469sFg1fhr6VhmJmBuSHxEe-YkJ0U,41101
 dtlpy/repositories/dpks.py,sha256=4Cw-avBe1HmVP5I-YSeJOhbRBGcxYUZHD7e-S87DeQI,13793
-dtlpy/repositories/drivers.py,sha256=xQ4P211GyPGJ0zDr1jzj3CJNvtCJrSer-DrgbXWXcAI,10299
-dtlpy/repositories/executions.py,sha256=7-YBf2fRJIz-XUrb-bOlKU1fMa3QvRqX6NqesxtHB6I,30254
+dtlpy/repositories/drivers.py,sha256=A7M6QDd5jHf_3KsOaQrJKaK5WhlHZoB5hYEBnJdKsKk,10284
+dtlpy/repositories/executions.py,sha256=xxDC1I8sGD4LmZ87XcNe8gnZSgdDfPVjjJzWAhc92BE,30254
 dtlpy/repositories/feature_sets.py,sha256=o1D40Z1plQmTd0FK8kEic4cTiv82Q8lSSP6TFG3q0C0,7888
-dtlpy/repositories/features.py,sha256=9Vtza-R2lAH4b0s06tqP7EHN14iyw3soV-8QDKuP5y8,10078
+dtlpy/repositories/features.py,sha256=NwjarF3q3HMHTLrxLRv28E9_T3mkulhWVUVeaE2B67k,9166
 dtlpy/repositories/integrations.py,sha256=hufdGFisdgwCekY-gTuST8WTKkR59LRSCnRxdYphkA4,11460
 dtlpy/repositories/items.py,sha256=60MIcLPOZ4WWIEFkvClwky3xClP-xJxCdPavfYO02Jg,37804
 dtlpy/repositories/messages.py,sha256=CAYQgqwvIZnHU_HVgLhNkwnMCAi4sKsvT7FsSPGnd28,3080
-dtlpy/repositories/models.py,sha256=44pTwxcJUOP58XUh_AZ-RCnfqUkhdx96xtWMYgYLZtI,28904
+dtlpy/repositories/models.py,sha256=7hxM6kBb-Jp2vetuqek3Lk2CHceN7hPYmiCFwGMj2Jw,29021
 dtlpy/repositories/nodes.py,sha256=xXJm_YA0vDUn0dVvaGeq6ORM0vI3YXvfjuylvGRtkxo,3061
 dtlpy/repositories/ontologies.py,sha256=3Xy30zB6BgdayCdGtNtj1fmhrrVD1WJXoTTeYc0iwQ4,19525
-dtlpy/repositories/organizations.py,sha256=mdWLjyplabNuzlL5zcSCaZrlmp61YeYKFc_K0kpkHmI,22953
+dtlpy/repositories/organizations.py,sha256=5-l9MXKGm3-gYkaK_-KbJsGuZ98Kf5uV95BWXowRZuI,22957
 dtlpy/repositories/packages.py,sha256=QJuQIngoy43z5brTRJKPg1eyKU7TGmAr7Zcop_KZyiE,86474
-dtlpy/repositories/pipeline_executions.py,sha256=MLbB9seOY-244lvQtzAaDOpd3iW8tJ_t4Gg1RfOyJmo,11759
-dtlpy/repositories/pipelines.py,sha256=LVDYpJrnEuZq_4KRZyio8MwnE4WontpsZIHcpVEpHEQ,22315
-dtlpy/repositories/projects.py,sha256=6MGngDVzhlAuE76eh27qL0B7qHWiZ3o-V0PfiXIYJSk,22184
+dtlpy/repositories/pipeline_executions.py,sha256=8QepaHRFrGZ4gN0VVsJAp96Ot8Z6OMvWryOswC7L6vQ,11873
+dtlpy/repositories/pipelines.py,sha256=gHPYXHZhVtwtKFeRFd_BNt25B-SmLk29ttWZ_TvlqWQ,23316
+dtlpy/repositories/projects.py,sha256=wO0IKj58WiDzOr5TjkA5rgsmXKZVGCx_klrNuwKQJoo,22184
 dtlpy/repositories/recipes.py,sha256=Jxzt9xOELDepRvxXpvjpuiqHyRSrvTqsoHxhnkWoigQ,15703
 dtlpy/repositories/resource_executions.py,sha256=PyzsbdJxz6jf17Gx13GZmqdu6tZo3TTVv-DypnJ_sY0,5374
 dtlpy/repositories/services.py,sha256=qeb9vt_SDHIyUICablPJOQE_wQABK5Un8Dm5IbnUmEc,63949
 dtlpy/repositories/settings.py,sha256=pvqNse0ANCdU3NSLJEzHco-PZq__OIsPSPVJveB9E4I,12296
-dtlpy/repositories/tasks.py,sha256=Z6vyBh4R2eCMsj0deUpIKiUC1nHSZk4utaRw_fxfkn8,46623
+dtlpy/repositories/tasks.py,sha256=4QxWeRqdS9pzVlQuziAIXlKRUIMHkVWIKI91AP9lTEQ,46625
 dtlpy/repositories/times_series.py,sha256=m-bKFEgiZ13yQNelDjBfeXMUy_HgsPD_JAHj1GVx9fU,11420
 dtlpy/repositories/triggers.py,sha256=fvClYSHLO6YcMq9MCaQZXOBfkYzmMS5R7pbDTXHCtPo,21961
 dtlpy/repositories/upload_element.py,sha256=nauOOTniDzawUTbpcZzOqfqDTRVcOMTQJJNHR1Cj7LQ,9251
 dtlpy/repositories/uploader.py,sha256=jeIX0i0oQxB04kyYSfpxhTgazTM-c-2QRUvP9g03Xw0,30682
 dtlpy/repositories/webhooks.py,sha256=IIpxOJ-7KeQp1TY9aJZz-FuycSjAoYx0TDk8z86KAK8,9033
 dtlpy/services/__init__.py,sha256=VfVJy2otIrDra6i7Sepjyez2ujiE6171ChQZp-YgxsM,904
 dtlpy/services/aihttp_retry.py,sha256=tgntZsAY0dW9v08rkjX1T5BLNDdDd8svtgn7nH8DSGU,5022
@@ -212,18 +212,18 @@
 dtlpy/utilities/local_development/local_session.py,sha256=4rcu5T9wD8hJxQNBsY6_Fapp0dBWNozVXjVuVGTeb-s,6451
 dtlpy/utilities/reports/__init__.py,sha256=e4eFLMchZSuo_q593NLAVoSdZ9KECf5W0nQ3TdIpWSg,124
 dtlpy/utilities/reports/figures.py,sha256=FP60Ha0qwyYk24CFa9nVHG2iqDUQ8oAvq-TyuPX1CqM,5927
 dtlpy/utilities/reports/report.py,sha256=3nEsNnIWmdPEsd21nN8vMMgaZVcPKn9iawKTTeOQg2A,2639
 dtlpy/utilities/videos/__init__.py,sha256=SV3w51vfPuGBxaMeNemx6qEMHw_C4lLpWNGXMvdsKSY,734
 dtlpy/utilities/videos/video_player.py,sha256=LCxg0EZ_DeuwcT7U_r7MRC6Q19s0xdFb7x5Gk39PRms,24072
 dtlpy/utilities/videos/videos.py,sha256=Dj916B4TQRIhI7HZVevl3foFrCsPp0eeWwvGbgX3-_A,21875
-dtlpy-1.77.18.data/scripts/dlp,sha256=-F0vSCWuSOOtgERAtsPMPyMmzitjhB7Yeftg_PDlDjw,10
-dtlpy-1.77.18.data/scripts/dlp.bat,sha256=QOvx8Dlx5dUbCTMpwbhOcAIXL1IWmgVRSboQqDhIn3A,37
-dtlpy-1.77.18.data/scripts/dlp.py,sha256=tEokRaDINISXnq8yNx_CBw1qM5uwjYiZoJOYGqWB3RU,4267
+dtlpy-1.78.17.data/scripts/dlp,sha256=-F0vSCWuSOOtgERAtsPMPyMmzitjhB7Yeftg_PDlDjw,10
+dtlpy-1.78.17.data/scripts/dlp.bat,sha256=QOvx8Dlx5dUbCTMpwbhOcAIXL1IWmgVRSboQqDhIn3A,37
+dtlpy-1.78.17.data/scripts/dlp.py,sha256=tEokRaDINISXnq8yNx_CBw1qM5uwjYiZoJOYGqWB3RU,4267
 tests/features/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-tests/features/environment.py,sha256=GmxR6KL9aLOiWeUKl-Yi9h9Td9Fib3iRp2NEhXTeQVI,9394
-dtlpy-1.77.18.dist-info/LICENSE,sha256=QwcOLU5TJoTeUhuIXzhdCEEDDvorGiC6-3YTOl4TecE,11356
-dtlpy-1.77.18.dist-info/METADATA,sha256=2A363VRrmfV5t7oDbUS4jl9eXU0EwAr0WcsToNh5yB4,3022
-dtlpy-1.77.18.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-dtlpy-1.77.18.dist-info/entry_points.txt,sha256=C4PyKthCs_no88HU39eioO68oei64STYXC2ooGZTc4Y,43
-dtlpy-1.77.18.dist-info/top_level.txt,sha256=ZWuLmQGUOtWAdgTf4Fbx884w1o0vBYq9dEc1zLv9Mig,12
-dtlpy-1.77.18.dist-info/RECORD,,
+tests/features/environment.py,sha256=D2Uhv9j3xZ1aGk8YmkXYcw75q8xE9ukKy6uakNJpaWU,9395
+dtlpy-1.78.17.dist-info/LICENSE,sha256=QwcOLU5TJoTeUhuIXzhdCEEDDvorGiC6-3YTOl4TecE,11356
+dtlpy-1.78.17.dist-info/METADATA,sha256=ocEYVieffR6YCjSpcYqSr99bzWrHJrbGi0lJmnWGnpc,3022
+dtlpy-1.78.17.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+dtlpy-1.78.17.dist-info/entry_points.txt,sha256=C4PyKthCs_no88HU39eioO68oei64STYXC2ooGZTc4Y,43
+dtlpy-1.78.17.dist-info/top_level.txt,sha256=ZWuLmQGUOtWAdgTf4Fbx884w1o0vBYq9dEc1zLv9Mig,12
+dtlpy-1.78.17.dist-info/RECORD,,
```

