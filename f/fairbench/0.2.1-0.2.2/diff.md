# Comparing `tmp/fairbench-0.2.1-py3-none-any.whl.zip` & `tmp/fairbench-0.2.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,34 +1,42 @@
-Zip file size: 22677 bytes, number of entries: 32
--rw-rw-rw-  2.0 fat      196 b- defN 23-May-23 12:15 fairbench/__init__.py
+Zip file size: 28231 bytes, number of entries: 40
+-rw-rw-rw-  2.0 fat        2 b- defN 23-May-31 10:14 fairbench/__init__.py
 -rw-rw-rw-  2.0 fat      512 b- defN 23-Jan-15 16:07 fairbench/accumulate.py
 -rw-rw-rw-  2.0 fat     1790 b- defN 23-Jan-15 16:07 fairbench/algorithms.py
--rw-rw-rw-  2.0 fat     1885 b- defN 23-May-18 19:58 fairbench/export.py
+-rw-rw-rw-  2.0 fat     1952 b- defN 23-Jun-01 09:25 fairbench/export.py
 -rw-rw-rw-  2.0 fat     9339 b- defN 23-May-17 23:49 fairbench/fork.py
 -rw-rw-rw-  2.0 fat     1744 b- defN 23-Jan-15 16:07 fairbench/output.py
 -rw-rw-rw-  2.0 fat     2393 b- defN 23-Jan-15 16:07 fairbench/reduction.py
 -rw-rw-rw-  2.0 fat     1146 b- defN 23-Jan-15 16:07 fairbench/reporting.py
 -rw-rw-rw-  2.0 fat       38 b- defN 23-May-14 10:33 fairbench/bench/__init__.py
 -rw-rw-rw-  2.0 fat      583 b- defN 23-May-14 15:09 fairbench/bench/loader.py
 -rw-rw-rw-  2.0 fat      122 b- defN 23-May-13 22:58 fairbench/forks/__init__.py
--rw-rw-rw-  2.0 fat     2041 b- defN 23-May-22 12:01 fairbench/forks/categorical.py
--rw-rw-rw-  2.0 fat     1903 b- defN 23-May-17 23:49 fairbench/forks/explanation.py
--rw-rw-rw-  2.0 fat    22629 b- defN 23-May-23 09:59 fairbench/forks/fork.py
+-rw-rw-rw-  2.0 fat     2130 b- defN 23-May-25 07:44 fairbench/forks/categorical.py
+-rw-rw-rw-  2.0 fat     1070 b- defN 23-Jun-02 10:01 fairbench/forks/explanation.py
+-rw-rw-rw-  2.0 fat    23803 b- defN 23-Jun-02 09:31 fairbench/forks/fork.py
 -rw-rw-rw-  2.0 fat      154 b- defN 23-Jan-14 23:08 fairbench/metrics/__init__.py
--rw-rw-rw-  2.0 fat     1816 b- defN 23-May-15 14:34 fairbench/metrics/classification.py
--rw-rw-rw-  2.0 fat     2007 b- defN 23-Jan-15 16:07 fairbench/metrics/disparate_impact.py
--rw-rw-rw-  2.0 fat     1450 b- defN 23-Jan-15 16:07 fairbench/metrics/disparate_mistreatment.py
+-rw-rw-rw-  2.0 fat     2846 b- defN 23-May-31 08:40 fairbench/metrics/classification.py
+-rw-rw-rw-  2.0 fat     2066 b- defN 23-May-31 07:41 fairbench/metrics/disparate_impact.py
+-rw-rw-rw-  2.0 fat     1494 b- defN 23-May-31 07:41 fairbench/metrics/disparate_mistreatment.py
+-rw-rw-rw-  2.0 fat      100 b- defN 23-May-31 10:15 fairbench/mitigation/__init__.py
+-rw-rw-rw-  2.0 fat     1790 b- defN 23-May-31 10:16 fairbench/mitigation/postprocessing.py
 -rw-rw-rw-  2.0 fat      207 b- defN 23-Jan-15 14:06 fairbench/reports/__init__.py
--rw-rw-rw-  2.0 fat     1255 b- defN 23-May-18 22:06 fairbench/reports/accumulate.py
+-rw-rw-rw-  2.0 fat     1478 b- defN 23-Jun-02 09:19 fairbench/reports/accumulate.py
 -rw-rw-rw-  2.0 fat     1629 b- defN 23-May-18 18:48 fairbench/reports/adhoc.py
--rw-rw-rw-  2.0 fat     2239 b- defN 23-May-18 21:23 fairbench/reports/base.py
+-rw-rw-rw-  2.0 fat     2259 b- defN 23-May-28 14:54 fairbench/reports/base.py
 -rw-rw-rw-  2.0 fat     4980 b- defN 23-May-17 23:49 fairbench/reports/reduction.py
 -rw-rw-rw-  2.0 fat      883 b- defN 23-May-14 15:09 fairbench/reports/surrogate.py
 -rw-rw-rw-  2.0 fat      155 b- defN 23-Feb-20 08:42 fairbench/reports/reduction/__init__.py
--rw-rw-rw-  2.0 fat      856 b- defN 23-May-14 19:29 fairbench/reports/reduction/expanders.py
--rw-rw-rw-  2.0 fat     1819 b- defN 23-May-18 21:31 fairbench/reports/reduction/reduce.py
--rw-rw-rw-  2.0 fat     2604 b- defN 23-Feb-20 08:40 fairbench/reports/reduction/reducers.py
--rw-rw-rw-  2.0 fat      786 b- defN 23-May-23 13:05 fairbench-0.2.1.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-May-23 13:05 fairbench-0.2.1.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       10 b- defN 23-May-23 13:05 fairbench-0.2.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     2689 b- defN 23-May-23 13:05 fairbench-0.2.1.dist-info/RECORD
-32 files, 71952 bytes uncompressed, 18355 bytes compressed:  74.5%
+-rw-rw-rw-  2.0 fat      794 b- defN 23-May-28 14:54 fairbench/reports/reduction/expanders.py
+-rw-rw-rw-  2.0 fat     1853 b- defN 23-Jun-02 08:25 fairbench/reports/reduction/reduce.py
+-rw-rw-rw-  2.0 fat     2364 b- defN 23-May-28 14:54 fairbench/reports/reduction/reducers.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-26 21:15 tests/__init__.py
+-rw-rw-rw-  2.0 fat     1394 b- defN 23-May-28 18:45 tests/test_batching.py
+-rw-rw-rw-  2.0 fat     1278 b- defN 23-Jun-01 09:09 tests/test_benchmarks.py
+-rw-rw-rw-  2.0 fat     4884 b- defN 23-Jun-07 08:03 tests/test_forks.py
+-rw-rw-rw-  2.0 fat     1572 b- defN 23-Jun-02 08:24 tests/test_reduction.py
+-rw-rw-rw-  2.0 fat     6556 b- defN 23-May-31 14:04 tests/test_reports.py
+-rw-rw-rw-  2.0 fat      809 b- defN 23-Jun-07 08:06 fairbench-0.2.2.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Jun-07 08:06 fairbench-0.2.2.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       16 b- defN 23-Jun-07 08:06 fairbench-0.2.2.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     3335 b- defN 23-Jun-07 08:06 fairbench-0.2.2.dist-info/RECORD
+40 files, 91612 bytes uncompressed, 22909 bytes compressed:  75.0%
```

## zipnote {}

```diff
@@ -48,14 +48,20 @@
 
 Filename: fairbench/metrics/disparate_impact.py
 Comment: 
 
 Filename: fairbench/metrics/disparate_mistreatment.py
 Comment: 
 
+Filename: fairbench/mitigation/__init__.py
+Comment: 
+
+Filename: fairbench/mitigation/postprocessing.py
+Comment: 
+
 Filename: fairbench/reports/__init__.py
 Comment: 
 
 Filename: fairbench/reports/accumulate.py
 Comment: 
 
 Filename: fairbench/reports/adhoc.py
@@ -78,20 +84,38 @@
 
 Filename: fairbench/reports/reduction/reduce.py
 Comment: 
 
 Filename: fairbench/reports/reduction/reducers.py
 Comment: 
 
-Filename: fairbench-0.2.1.dist-info/METADATA
+Filename: tests/__init__.py
+Comment: 
+
+Filename: tests/test_batching.py
+Comment: 
+
+Filename: tests/test_benchmarks.py
+Comment: 
+
+Filename: tests/test_forks.py
+Comment: 
+
+Filename: tests/test_reduction.py
+Comment: 
+
+Filename: tests/test_reports.py
+Comment: 
+
+Filename: fairbench-0.2.2.dist-info/METADATA
 Comment: 
 
-Filename: fairbench-0.2.1.dist-info/WHEEL
+Filename: fairbench-0.2.2.dist-info/WHEEL
 Comment: 
 
-Filename: fairbench-0.2.1.dist-info/top_level.txt
+Filename: fairbench-0.2.2.dist-info/top_level.txt
 Comment: 
 
-Filename: fairbench-0.2.1.dist-info/RECORD
+Filename: fairbench-0.2.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## fairbench/__init__.py

```diff
@@ -1,6 +1 @@
-from fairbench.forks import *
-from fairbench.metrics import *
-from fairbench.reports import *
-from fairbench.export import *
-from fairbench.algorithms import *
-from fairbench.bench import *
+
```

## fairbench/export.py

```diff
@@ -6,15 +6,18 @@
 
 def _is_fork_of_dicts(report):
     return isinstance(report[next(iter(report))], dict)
 
 
 def tojson(report: Fork):
     assert isinstance(report, Fork)
-    report = {k: v.branches() if isinstance(v, Fork) else v for k, v in report.branches().items()}
+    report = {
+        k: v.branches() if isinstance(v, Fork) else v
+        for k, v in report.branches().items()
+    }
     data = dict()
     if not _is_fork_of_dicts(report):
         report = {k: {"": v} for k, v in report.items()}
     data["header"] = ["Metric"] + [key for key in report]
     for value in report.values():
         for metric in value:
             if metric not in data:
@@ -37,22 +40,23 @@
                     + [f"{entry:.3f}".ljust(spacing) for entry in report[metric]]
                 )
                 + "\n"
             )
     print(ret)
 
 
-def visualize(report: Fork):
+def visualize(report: Fork, hold: bool = False):
     assert isinstance(report, Fork)
     report = json.loads(tojson(report))
 
     i = 1
     for metric in report:
         if metric != "header":
             plt.subplot(2, len(report) // 2, i)
             for j, case in enumerate(report["header"][1:]):
                 plt.bar(j, report[metric][j])
             plt.xticks(list(range(len(report["header"][1:]))), report["header"][1:])
             plt.title(metric)
             i += 1
     plt.tight_layout()
-    plt.show()
+    if not hold:
+        plt.show()
```

## fairbench/forks/categorical.py

```diff
@@ -43,24 +43,31 @@
 @Transform
 def binary(x):
     x = tobackend(x)
     return {"1": x, "0": 1 - x}
 
 
 class Categories:
-    def __init__(self, values: Iterable, generator=lambda data, category: data == category):
+    def __init__(
+        self, values: Iterable, generator=lambda data, category: data == category
+    ):
         self.categories = list(values)
         self.generator = generator
 
     def __call__(self, other):
         return self.__matmul__(other)
 
     def __matmul__(self, other):
         assert not isinstance(other, Fork)
-        return Categorical({str(category): self.generator(other, category) for category in self.categories})
+        return Categorical(
+            {
+                str(category): self.generator(other, category)
+                for category in self.categories
+            }
+        )
 
 
 @Transform
 def categories(x):
     assert isinstance(x, Iterable)
     if isinstance(x, Mapping):
         return Categorical(x)
```

## fairbench/forks/explanation.py

```diff
@@ -1,66 +1,33 @@
 from typing import Any
 import eagerpy as ep
+from objwrap import Wrapper
 
 
 def tofloat(value):
     if isinstance(value, ep.Tensor):
         return float(value.raw)
     return float(value)
 
 
-class Explainable:
+class Explainable(Wrapper):
     def __init__(self, value, explain: Any = None, desc: str = None, **kwargs):
         from fairbench.forks import Fork
 
-        self.value = value
+        if value.__class__.__name__ == "Future":
+            value = value.result()
+
+        assert isinstance(value, float) or isinstance(value, int) or "tensor" in value.__class__.__name__.lower() or "array" in value.__class__.__name__, "Can not set non-numeric as explainable"
+        assert explain is None or not kwargs, "Cannot create explainable with both todict and a Fork"
+        super().__init__(value)
         self.explain = Fork(kwargs) if explain is None else explain
         self.desc = desc
-        if (
-            not isinstance(value, float)
-            and not isinstance(value, int)
-            and "tensor" not in value.__class__.__name__.lower()
-            and "array" not in value.__class__.__name__
-        ):
-            raise Exception("Can not set non-numeric as explainable", value)
-        if explain is not None and kwargs:
-            raise Exception("Cannot create explainable with both todict and a Fork")
 
     def __float__(self):
-        return tofloat(self.value)
+        return tofloat(self.__value__())
+
+    @property
+    def value(self):
+        return self.__value__()
 
     def numpy(self):
         return self.value.numpy()
-
-    def __str__(self):
-        return str(self.value)
-
-    def __repr__(self):
-        return self.__str__()
-
-    def sum(self):
-        return self.value.sum()
-
-    def __sub__(self, other):
-        if isinstance(other, Explainable):
-            other = other.value
-        return self.value - other
-
-    def __mul__(self, other):
-        if isinstance(other, Explainable):
-            other = other.value
-        return self.value * other
-
-    def __rmul__(self, other):
-        if isinstance(other, Explainable):
-            other = other.value
-        return other * self.value
-
-    def __add__(self, other):
-        if isinstance(other, Explainable):
-            other = other.value
-        return self.value + other
-
-    def __radd__(self, other):
-        if isinstance(other, Explainable):
-            other = other.value
-        return other + self.value
```

## fairbench/forks/fork.py

```diff
@@ -106,36 +106,45 @@
                 if k in branches:
                     raise TypeError(f"Branch {k} provided multiple times")
                 branches[k] = v
         self._branches = dict()
         for k, v in branches.items():
             if isinstance(v, dict) and v.__class__.__name__ == "Categorical":
                 for k2, v2 in v.items():
-                    self._branches[str(k2) if _separator is None else k + _separator + str(k2)] = v2
+                    self._branches[
+                        str(k2) if _separator is None else k + _separator + str(k2)
+                    ] = v2
             else:
                 self._branches[k] = v
 
     def __getattribute__(self, name):
         if name in ["_branches", "_repr_html_"] or name in dir(Fork):
             return object.__getattribute__(self, name)
         if name.startswith("_"):
             raise AttributeError(name)
         if name in self._branches:
             ret = self._branches[name]
             return _result(ret)
 
-        #def method(*args, **kwargs):
+        # def method(*args, **kwargs):
         #    return call(self, name, *args, **kwargs)
-        #return method
-
-        return Fork({k: v.__getattribute__(name) if isinstance(v, Fork) else call(v, "__getattribute__", name) for k, v in self._branches.items()})
+        # return method
 
+        return Fork(
+            {
+                k: v.__getattribute__(name)
+                if isinstance(v, Fork)
+                else call(v, "__getattribute__", name)
+                for k, v in self._branches.items()
+            }
+        )
 
     def extract(self, *args):
         import fairbench as fb
+
         ret = dict()
         for arg in args:
             ret = ret | fb.todict(**{arg: self[arg]})
         return ret
 
     def branches(self, branch_names=None, zero_mask=False):
         return {
@@ -201,28 +210,28 @@
         keys = None
         for k, v in self.branches().items():
             assert isinstance(v, dict)
             v_keys = set(v.keys())
             if keys is None:
                 keys = v_keys
             else:
-                assert len(v_keys-keys) == 0
-                assert len(keys-v_keys) == 0
+                assert len(v_keys - keys) == 0
+                assert len(keys - v_keys) == 0
         return len(keys)
 
     def __iter__(self):
         keys = None
         for k, v in self.branches().items():
             assert isinstance(v, dict)
             v_keys = set(v.keys())
             if keys is None:
                 keys = v_keys
             else:
-                assert len(v_keys-keys) == 0
-                assert len(keys-v_keys) == 0
+                assert len(v_keys - keys) == 0
+                assert len(keys - v_keys) == 0
         return keys.__iter__()
 
     def __delitem__(self, name):
         return call(self, "__delitem__", name)
 
     def __getitem__(self, name):
         return call(self, "__getitem__", name)
@@ -247,16 +256,16 @@
 
     def __ge__(self, other):
         return call(self, "__ge__", other)
 
     def __ne__(self, other):
         return call(self, "__ne__", other)
 
-    def __neg__(self, other):
-        return call(self, "__neg__", other)
+    def __neg__(self):
+        return call(self, "__neg__")
 
     def __add__(self, other):
         return call(self, "__add__", other)
 
     def __radd__(self, other):
         return call(self, "__add__", other)
 
@@ -290,17 +299,21 @@
     def __and__(self, other):
         return call(self, "__and__", other)
 
     def __ror__(self, other):
         return call(self, "__ror__", other)
 
     def __call__(self, *args, **kwargs):
+        from fairbench import Explainable
+
         return Fork(
             **{
                 branch: value(*args, **kwargs)
+                if not isinstance(value, Explainable)
+                else value
                 for branch, value in self._branches.items()
             }
         )
 
     def __str__(self):
         return "\n".join(
             k + ": " + str(fromtensor(v)) for k, v in self.branches().items()
@@ -376,16 +389,18 @@
 def serial():
     global _client
     _client = _NoClient()
 
 
 def parallel(_wrapped_method):
     if len(inspect.getfullargspec(_wrapped_method)[0]) <= 1:
-        raise Exception("To avoid ambiguity, the @parallel decorator can be applied only to methods with at least"
-                        "two arguments.")
+        raise Exception(
+            "To avoid ambiguity, the @parallel decorator can be applied only to methods with at least"
+            "two arguments."
+        )
 
     @wraps(_wrapped_method)
     def wrapper(*args, **kwargs):
         if len(args) == 1 and not kwargs:
             argnames = inspect.getfullargspec(_wrapped_method)[0]
             arg = args[0]
             kwargs = {k: getattr(arg, k) for k in argnames if hasattr(arg, k)}
@@ -654,27 +669,55 @@
     ret = {}
     for arg in args:
         assert isinstance(arg, Fork)
         ret |= arg._branches
     return Fork(ret)
 
 
+def unit_bounded(method):
+    @wraps(method)
+    def wrapper(*args, **kwargs):
+        for iter in [args, kwargs.values()]:
+            for arg in iter:
+                if isinstance(arg, ep.Tensor):
+                    assert (
+                        arg.min() >= 0 and arg.max() <= 1
+                    ), f"{method.__name__} inputs should lie in the range [0,1]. Maybe use fairbench.categories to transform categorical data."
+        return method(*args, **kwargs)
+
+    return wrapper
+
+
 @parallel_primitive
 def call(obj, method, *args, **kwargs):
-    if method == "__getattribute__" and isinstance(obj, dict) and len(args) == 1 and len(kwargs) == 0:
+    if method == "__getattribute__":
+        obj = _result(obj)
+    if (
+        method == "__getattribute__"
+        and isinstance(obj, dict)
+        and len(args) == 1
+        and len(kwargs) == 0
+    ):
         return obj[args[0]]
     if callable(method):
         return method(obj, *args, **kwargs)
+    """
+    def run(obj, method, *args, **kwargs):
+        attr = getattr(obj, method)
+        if not callable(attr):
+            return attr
+        return attr(*args, **kwargs)
+    return _client.submit(run, obj, method, *args, **kwargs, pure=False)
+    """
     attr = getattr(obj, method)
     if not callable(attr):
         return attr
     return attr(*args, **kwargs)
 
 
-
 """
 def compare(**todict):
     for modal in todict.values():
         assert isinstance(modal, Modal)
     branches = set(
         [
             branch
```

## fairbench/metrics/classification.py

```diff
@@ -1,37 +1,41 @@
-from fairbench.forks import parallel, Explainable
+from fairbench.forks import parallel, unit_bounded
 from eagerpy import Tensor
 
 
 @parallel
+@unit_bounded
 def accuracy(predictions: Tensor, labels: Tensor, sensitive: Tensor = None):
     if sensitive is None:
         sensitive = predictions.ones_like()
     num_sensitive = sensitive.sum()
     if num_sensitive == 0:
         return 0
     return 1 - ((predictions - labels) * sensitive).abs().sum() / num_sensitive
 
 
 @parallel
+@unit_bounded
 def pr(predictions: Tensor, sensitive: Tensor = None):
     if sensitive is None:
         sensitive = predictions.ones_like()
     sum_sensitive = sensitive.sum()
     if sum_sensitive == 0:
         return sum_sensitive
     return (predictions * sensitive).sum() / sum_sensitive
 
 
 @parallel
+@unit_bounded
 def positives(predictions: Tensor, sensitive: Tensor):
     return (predictions * sensitive).mean()
 
 
 @parallel
+@unit_bounded
 def tpr(
     predictions: Tensor,
     labels: Tensor,
     sensitive: Tensor = None,
     max_prediction: float = 1,
 ):
     if sensitive is None:
@@ -41,14 +45,32 @@
     num_sensitive = (sensitive * predictions).sum()
     if num_sensitive == 0:
         return 0
     return error_sensitive.sum() / num_sensitive
 
 
 @parallel
+@unit_bounded
+def fpr(
+    predictions: Tensor,
+    labels: Tensor,
+    sensitive: Tensor = None,
+):
+    if sensitive is None:
+        sensitive = predictions.ones_like()
+    error = (predictions - labels).abs() * predictions
+    error_sensitive = error * sensitive
+    num_sensitive = (sensitive * predictions).sum()
+    if num_sensitive == 0:
+        return 0
+    return error_sensitive.sum() / num_sensitive
+
+
+@parallel
+@unit_bounded
 def tnr(
     predictions: Tensor,
     labels: Tensor,
     sensitive: Tensor = None,
     max_prediction: float = 1,
 ):
     if sensitive is None:
@@ -56,7 +78,26 @@
     negatives = max_prediction - predictions
     error = (max_prediction - (predictions - labels).abs()) * negatives
     error_sensitive = error * sensitive
     num_sensitive = (sensitive * negatives).sum()
     if num_sensitive == 0:
         return 0
     return error_sensitive.sum() / num_sensitive
+
+
+@parallel
+@unit_bounded
+def fnr(
+    predictions: Tensor,
+    labels: Tensor,
+    sensitive: Tensor = None,
+    max_prediction: float = 1,
+):
+    if sensitive is None:
+        sensitive = predictions.ones_like()
+    negatives = max_prediction - predictions
+    error = (predictions - labels).abs() * negatives
+    error_sensitive = error * sensitive
+    num_sensitive = (sensitive * negatives).sum()
+    if num_sensitive == 0:
+        return 0
+    return error_sensitive.sum() / num_sensitive
```

## fairbench/metrics/disparate_impact.py

```diff
@@ -1,13 +1,14 @@
-from fairbench.forks.fork import parallel
+from fairbench.forks.fork import parallel, unit_bounded
 from eagerpy import Tensor
 from typing import Optional
 
 
 @parallel
+@unit_bounded
 def prule(
     predictions: Tensor,
     sensitive: Tensor,
     non_sensitive: Optional[Tensor] = None,
     max_sensitive: float = 1,
 ):
     if non_sensitive is None:
@@ -23,14 +24,15 @@
     max_r = r1.maximum(r2)
     if max_r == 0:
         return max_r
     return r1.minimum(r2) / max_r
 
 
 @parallel
+@unit_bounded
 def cvdisparity(
     predictions: Tensor,
     sensitive: Tensor,
     non_sensitive: Optional[Tensor] = None,
     max_sensitive: float = 1,
 ):
     if non_sensitive is None:
@@ -43,14 +45,15 @@
         return sum_non_sensitive
     r1 = (predictions * sensitive).sum() / sum_sensitive
     r2 = (predictions * non_sensitive).sum() / sum_non_sensitive
     return r1 - r2
 
 
 @parallel
+@unit_bounded
 def eqrep(
     predictions: Tensor,
     sensitive: Tensor,
     non_sensitive: Optional[Tensor] = None,
     max_sensitive: float = 1,
 ):
     if non_sensitive is None:
```

## fairbench/metrics/disparate_mistreatment.py

```diff
@@ -1,13 +1,14 @@
-from fairbench.forks.fork import parallel
+from fairbench.forks.fork import parallel, unit_bounded
 from eagerpy import Tensor
 from typing import Optional
 
 
 @parallel
+@unit_bounded
 def dfpr(
     predictions: Tensor,
     labels: Tensor,
     sensitive: Tensor,
     non_sensitive: Optional[Tensor] = None,
 ):
     if non_sensitive is None:
@@ -20,14 +21,15 @@
     return (
         error_sensitive.sum() / num_sensitive
         - error_non_sensitive.sum() / num_non_sensitive
     )
 
 
 @parallel
+@unit_bounded
 def dfnr(
     predictions: Tensor,
     labels: Tensor,
     sensitive: Tensor,
     non_sensitive: Optional[Tensor] = None,
     max_prediction: float = 1,
 ):
```

## fairbench/reports/accumulate.py

```diff
@@ -1,8 +1,9 @@
 from fairbench.forks.fork import comparator, parallel_primitive, astensor
+from fairbench.forks.explanation import Explainable
 import eagerpy as ep
 
 
 """
 This module provides helper methods to concatenate tensors stored within Forks of tensor or Forks of dicts of tensors
 and use the final output in one report at the end.
 """
@@ -30,16 +31,20 @@
 
 
 def extract(**kwargs):
     ret = dict()
     for k, v in kwargs.items():
         try:
             if callable(v):
-                v = v()  # TODO: this is a hack to supplement the fact that object members are returns as functions by getattr on Forks
+                v = (
+                    v()
+                )  # TODO: this is a hack to supplement the fact that object members are returned as functions by getattr on Forks
         except TypeError:
             pass
         try:
             v = v[k]
         except AttributeError:
             pass
+        except IndexError:
+            pass  # for IndexError: invalid index to scalar variable. (explainables within forks)
         ret = ret | todict(**{k: v})
     return ret
```

## fairbench/reports/base.py

```diff
@@ -17,18 +17,17 @@
     return kwargs
 
 
 @comparator
 @parallel_primitive
 def report(*args, metrics: Union[Callable, Iterable, dict] = None, **kwargs):
     kwargs = reportargsparse(*args, **kwargs)
-    if metrics is None:
-        raise Exception(
-            "Cannot use fairbench.report() without explicitly declared metrics.\nUse accreport, binreport, multireport, or isecreport as ad-hoc report generation mechanisms."
-        )
+    assert (
+        metrics is not None
+    ), "Cannot use fairbench.report() without explicitly declared metrics.\nUse accreport, binreport, multireport, or isecreport as ad-hoc report generation mechanisms."
     if not isinstance(metrics, Iterable):
         metrics = [metrics]
     if not isinstance(metrics, dict):
         metrics = {metric.__name__: metric for metric in metrics}
     ret = dict()
     for name, metric in metrics.items():
         arg_names = set(inspect.getfullargspec(metric)[0])
@@ -39,16 +38,21 @@
                 if arg in arg_names
             }
         )
     return ret
 
 
 def areport(*args, metrics: Union[Callable, Iterable, dict] = None, **kwargs):
-    if metrics is None:
-        raise Exception(
-            "Cannot use fairbench.report() without explicitly declared metrics.\nUse accreport, binreport, multireport, or isecreport as ad-hoc report generation mechanisms."
-        )
+    assert (
+        metrics is not None
+    ), "Cannot use fairbench.report() without explicitly declared metrics.\nUse accreport, binreport, multireport, or isecreport as ad-hoc report generation mechanisms."
     if not isinstance(metrics, Iterable):
         return getattr(report(*args, metrics=[metrics], **kwargs), metrics.__name__)
     if not isinstance(metrics, dict):
-        return [getattr(report(*args, metrics=[metric], **kwargs), metric.__name__) for metric in metrics]
-    return {name: getattr(report(*args, metrics=[metric], **kwargs), name) for name, metric in metrics.items()}
+        return [
+            getattr(report(*args, metrics=[metric], **kwargs), metric.__name__)
+            for metric in metrics
+        ]
+    return {
+        name: getattr(report(*args, metrics={name: metric}, **kwargs), name)
+        for name, metric in metrics.items()
+    }
```

## fairbench/reports/reduction/expanders.py

```diff
@@ -1,21 +1,20 @@
 import eagerpy as ep
 from typing import Iterable
 
 
 def ratio(values: Iterable[ep.Tensor]) -> Iterable[ep.Tensor]:
-    if not isinstance(values, list):
-        raise TypeError("Can only reduce lists with fairbench.ratio.")
+    assert isinstance(values, list), "Can only reduce lists with fairbench.ratio."
     return [value1 / value2 for value1 in values for value2 in values if value2 != 0]
 
 
 def diff(values: Iterable[ep.Tensor]) -> Iterable[ep.Tensor]:
-    if not isinstance(values, list):
-        raise TypeError("Can only reduce lists with fairbench.diff.")
+    assert isinstance(values, list), "Can only reduce lists with fairbench.diff."
     return [abs(value1 - value2) for value1 in values for value2 in values]
 
 
 def todata(values: Iterable[ep.Tensor]) -> ep.Tensor:
-    if not isinstance(values, list):
-        raise TypeError("Can only reduce lists of tensors with fairbench.todata.")
+    assert isinstance(
+        values, list
+    ), "Can only reduce lists of tensors with fairbench.todata."
     values = [ep.reshape(value, (-1, 1)) for value in values]
     return ep.concatenate(values, axis=1)
```

## fairbench/reports/reduction/reduce.py

```diff
@@ -6,15 +6,22 @@
 
 
 def areduce(fork: Fork, reducer, expand=None, transform=None, branches=None):
     return reduce(fork, reducer, expand, transform, branches, name=None)
 
 
 @comparator
-def reduce(fork: Fork, reducer, expand=None, transform=None, branches=None, name: Optional[str] = ""):
+def reduce(
+    fork: Fork,
+    reducer,
+    expand=None,
+    transform=None,
+    branches=None,
+    name: Optional[str] = "",
+):
     if name == "":
         name = reducer.__name__
         if expand is not None:
             name += expand.__name__
         if transform is not None:
             name += transform.__name__
         if branches is not None:
```

## fairbench/reports/reduction/reducers.py

```diff
@@ -5,18 +5,17 @@
 def abs(value):
     if value < 0:
         return -value
     return value
 
 
 def sum(values: Iterable[ep.Tensor]) -> ep.Tensor:
-    if not isinstance(values, list):
-        raise TypeError(
-            "Can only reduce lists with fairbench.sum. Maybe you meant to use eagerpy.sum?"
-        )
+    assert isinstance(
+        values, list
+    ), "Can only reduce lists with fairbench.sum. Maybe you meant to use eagerpy.sum?"
     ret = 0
     for value in values:
         ret = ret + value
     return ret
 
 
 def mean(values: Iterable[ep.Tensor]) -> ep.Tensor:
@@ -24,64 +23,57 @@
         raise TypeError(
             "Can only reduce lists with fairbench.mean. Maybe you meant to use eagerpy.mean?"
         )
     return sum(values) / len(values)
 
 
 def identical(values: Iterable[ep.Tensor]) -> ep.Tensor:
-    if not isinstance(values, list):
-        raise TypeError(
-            "Can only reduce lists with fairbench.mean. Maybe you meant to use eagerpy.identical?"
-        )
+    assert isinstance(
+        values, list
+    ), "Can only reduce lists with fairbench.mean. Maybe you meant to use eagerpy.identical?"
     for value in values:
-        if (value - values[0]).abs().sum() != 0:
-            raise Exception(
-                "eagerpy.identical requires that the exact same tensor is placed on all branches"
-            )
+        assert (
+            value - values[0]
+        ).abs().sum() == 0, "eagerpy.identical requires that the exact same tensor is placed on all branches"
     return values[0]
 
 
 def gm(values: Iterable[ep.Tensor]) -> ep.Tensor:
-    if not isinstance(values, list):
-        raise TypeError(
-            "Can only reduce lists with fairbench.mean. Maybe you meant to use eagerpy.mean?"
-        )
-
+    assert isinstance(
+        values, list
+    ), "Can only reduce lists with fairbench.mean. Maybe you meant to use eagerpy.mean?"
     ret = 1
     for value in values:
         ret = ret * value
     return ret ** (1.0 / len(values))
 
 
 def max(values: Iterable[ep.Tensor]) -> ep.Tensor:
-    if not isinstance(values, list):
-        raise TypeError(
-            "Can only reduce lists with fairbench.max. Maybe you meant to use eagerpy.maximum?"
-        )
+    assert isinstance(
+        values, list
+    ), "Can only reduce lists with fairbench.max. Maybe you meant to use eagerpy.maximum?"
     ret = float("-inf")
     for value in values:
         if value > ret:
             ret = value
     return ret
 
 
 def budget(values: Iterable[ep.Tensor]) -> ep.Tensor:
-    if not isinstance(values, list):
-        raise TypeError(
-            "Can only reduce lists with fairbench.budget. Maybe you meant to use an eagerpy method?"
-        )
+    assert isinstance(
+        values, list
+    ), "Can only reduce lists with fairbench.budget. Maybe you meant to use an eagerpy method?"
     from math import log  # TODO: make this compatible with backpropagation
 
     # "An Intersectional Definition of Fairness"
     return log(float(max(values)))
 
 
 def min(values: Iterable[ep.Tensor]) -> ep.Tensor:
-    if not isinstance(values, list):
-        raise TypeError(
-            "Can only reduce lists with fairbench.min. Maybe you meant to use eagerpy.minimum?"
-        )
+    assert isinstance(
+        values, list
+    ), "Can only reduce lists with fairbench.min. Maybe you meant to use eagerpy.minimum?"
     ret = float("inf")
     for value in values:
         if value < ret:
             ret = value
     return ret
```

## Comparing `fairbench-0.2.1.dist-info/METADATA` & `fairbench-0.2.2.dist-info/METADATA`

 * *Files 25% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: fairbench
-Version: 0.2.1
+Version: 0.2.2
 Summary: Fairness model assessment framework
 Home-page: https://github.com/mever-team/FairBench
 Author: Emmanouil (Manios) Krasanakis
 Author-email: maniospas@hotmail.com
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
@@ -15,10 +15,11 @@
 Requires-Dist: eagerpy
 Requires-Dist: distributed
 Requires-Dist: makefun
 Requires-Dist: matplotlib
 Requires-Dist: wget
 Requires-Dist: scikit-learn
 Requires-Dist: pandas
+Requires-Dist: objwrap
 
 For tutorials, documentation, and contribution guidelines, please visit the project's homepage at https://github.com/mever-team/FairBench
```

## Comparing `fairbench-0.2.1.dist-info/RECORD` & `fairbench-0.2.2.dist-info/RECORD`

 * *Files 22% similar despite different names*

```diff
@@ -1,32 +1,40 @@
-fairbench/__init__.py,sha256=0Q1JGj_lTF1FSPMwOo7_1yCBo1dQ410nADc80kiVL5c,196
+fairbench/__init__.py,sha256=frcCV1k9oG9oKj3dpUqdJg1PxRT2RSN_XKdLCPjaYaY,2
 fairbench/accumulate.py,sha256=vyDGY-RTP-aX3ygiHc56WPefRFWQOCjg7KJ4kC3kwFg,512
 fairbench/algorithms.py,sha256=CF2WcAWQKKfNRJ4yvNqIjuNdLaCBrSIpI_XNhmfiUkY,1790
-fairbench/export.py,sha256=V1_HLb1gYE9710yb7EhZSKJySh07ChmX2Dr1oUZY0ew,1885
+fairbench/export.py,sha256=_u97-Yi8VArkwof63mcVXgnUD-i9AY1vjxtCnQ_jKcs,1952
 fairbench/fork.py,sha256=VAgJ6MeD5a6mK_BMrIJdXPxLpp7wz0wjTSy59IJOSJU,9339
 fairbench/output.py,sha256=wvusKkx72F9i0EZoWpM8vR3NYTkpJTc8rtj5oLTKkRA,1744
 fairbench/reduction.py,sha256=PkHMY4B8SuwwV7-dq_dQXW2bhx2tr8jQ7iAXy9kilVE,2393
 fairbench/reporting.py,sha256=jcZtJXjTQAstzTPMjcnQei053k_LUkVrA1oLVsfOIog,1146
 fairbench/bench/__init__.py,sha256=piA_S0SAZ96_-MxdcGO_V7pCKngOFBeAv8KGj9rkGRA,38
 fairbench/bench/loader.py,sha256=BXmttN9Ee-TA6FAb1gXMhlJutDN2B6XIea6TU0Cre14,583
 fairbench/forks/__init__.py,sha256=eBRDSxc31Pc1yKCc1VjFBvPMsBVaOzVOfXHpkYQodG0,122
-fairbench/forks/categorical.py,sha256=sOECZ2-WrbZomqLcZz7bTXJR4vKRPoXsR6OrpjlIMdw,2041
-fairbench/forks/explanation.py,sha256=rQjkNuemRDvjSp4R2Gw8Mv2o8-Fd_mrbrIM_v7opEY0,1903
-fairbench/forks/fork.py,sha256=IMtBv-d9iuAi_9A4NpQVycCcwp6cN4Hvx9ZHJm04Rdg,22629
+fairbench/forks/categorical.py,sha256=nfsA1ZjrrywMzOhEoPUF6_9oazIJWcZEI4gvil-1VIs,2130
+fairbench/forks/explanation.py,sha256=F-QBVQvnN1O25gMDugV--UiykAp4LtAcPGh512pBtis,1070
+fairbench/forks/fork.py,sha256=dvhM3s8UepBEvC6Jt2e7iiWJHcLeh0XTKwBJIlcDo84,23803
 fairbench/metrics/__init__.py,sha256=i2kNaDKTfTSEh1aGZ83ByaVXwv0gYE-guFNjj9XlF6I,154
-fairbench/metrics/classification.py,sha256=263Wz5ILnq3OxyzPopLAM-tWszcsC2v1F6SSKqqDuVo,1816
-fairbench/metrics/disparate_impact.py,sha256=rLO82wWOSDI4s5aK9WjehIbuUrCLuog7pOV8SSBVEZI,2007
-fairbench/metrics/disparate_mistreatment.py,sha256=4xtayr0k5Z7ZAtlrFevDiUjQloJ6uhqPucgxPFrwES8,1450
+fairbench/metrics/classification.py,sha256=SE0l9uAozaZ_O2sU-1u6ZBwJhnOs9dT6yBbeYgVJLh4,2846
+fairbench/metrics/disparate_impact.py,sha256=VJw9Q4cA1YqP9fTTE-aMMUzjeuk072L-HRow_vaLkZA,2066
+fairbench/metrics/disparate_mistreatment.py,sha256=ORoUcMU_Dh18Ytz4dgQn8wPyaNJ_w98m_oegxI90vc4,1494
+fairbench/mitigation/__init__.py,sha256=f-owl1xAKGweXEuVbFJGYSel3-7kccvTo6SEziKmXRk,100
+fairbench/mitigation/postprocessing.py,sha256=CF2WcAWQKKfNRJ4yvNqIjuNdLaCBrSIpI_XNhmfiUkY,1790
 fairbench/reports/__init__.py,sha256=YQm7A6K3PUB4uNOw3iDu2RUaoWa07Rsn3GJr4wZkn2M,207
-fairbench/reports/accumulate.py,sha256=xRY708vEy7B3DcyC2Tec8XLdFAv9Q_Ru7Hw1np_l64Y,1255
+fairbench/reports/accumulate.py,sha256=JMpjsfs4dc4jfMn3I0a9W-kcHzJk1hmX26qUC5BySyU,1478
 fairbench/reports/adhoc.py,sha256=7FhOawdzoK3MtY6o8wLaxKDrJ1QZmG5XTTSxra4-D-w,1629
-fairbench/reports/base.py,sha256=tCpaurqYfuV3MGkwNIAr7QFlPeFHDenC2rgPUVKm79A,2239
+fairbench/reports/base.py,sha256=_5wXT5qfDYCWMTw-e9D-HQLyFAh0IWb2ywMLzdRwdTU,2259
 fairbench/reports/reduction.py,sha256=zSo8Whi-tW8trPmN946e2gyLp6xR7b8WMZzYyPmWgIc,4980
 fairbench/reports/surrogate.py,sha256=LnV6kkNPMGL2SYEBSSIFWjkXQ8N5m2k4y5as2zUyDt8,883
 fairbench/reports/reduction/__init__.py,sha256=a3c0w4bKV3f6NXnGMPPGFRxdfg_YIrcccy_mEBSr8zk,155
-fairbench/reports/reduction/expanders.py,sha256=BR6R9VwcFjbPC8WROrrE823oGwcbjuTgXyKBWjwvs3o,856
-fairbench/reports/reduction/reduce.py,sha256=fXNdsdSQnXS9ah_E9v9DicbZcZZE23bgAWr-Jee8i-E,1819
-fairbench/reports/reduction/reducers.py,sha256=5NA8FbXLieB9iV22aqPyOrpy7MZLtFviHiedxrMGMv0,2604
-fairbench-0.2.1.dist-info/METADATA,sha256=-klzTrYG8Rfg6mi9zK5uygw675W6tZ02T0A0ZleIJQ4,786
-fairbench-0.2.1.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
-fairbench-0.2.1.dist-info/top_level.txt,sha256=V_xpM0npsVDF1PcMECNA-pE1JrkHNp3p62aiR-7iqDk,10
-fairbench-0.2.1.dist-info/RECORD,,
+fairbench/reports/reduction/expanders.py,sha256=eiytghNbie7zqZht6PtbdpXSbBn68Fb_x7JL1VYiGmY,794
+fairbench/reports/reduction/reduce.py,sha256=yrdQY7Bdr1LaFh8FwiPjZMrY13z1kqaKWbebeNXfbK8,1853
+fairbench/reports/reduction/reducers.py,sha256=zZKHhJDIdBdiMbRotxdRUGS-sKcLFqGgDZ8MebzNMrE,2364
+tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+tests/test_batching.py,sha256=bXewsKXAlDTeyZZ8OlG74-qYQ_hhiwcYJigxy0M_YKY,1394
+tests/test_benchmarks.py,sha256=RZwehxePgR6fwFCh9HWbK3rXsXQNLHomrn7M6BboiMw,1278
+tests/test_forks.py,sha256=39cnpaRVvz7xfUz2YijRIZMg9_v0FrjFM1-a0hDNGM8,4884
+tests/test_reduction.py,sha256=MK72DJomkokS-pxTgmZVXwqDd5t-9OOCVJK2zDKTObA,1572
+tests/test_reports.py,sha256=2nNSZjkrTdZl8P5bvEz8dDdRd_W7CmSvn51GDdHFPvo,6556
+fairbench-0.2.2.dist-info/METADATA,sha256=1twKoVa7RLp7Uihkuk3hbFhGTOoiFJO11P-lJ4iHn_A,809
+fairbench-0.2.2.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
+fairbench-0.2.2.dist-info/top_level.txt,sha256=lrkG910bN_2UdVUqCXaR6aeRjjXfOQX2-wSeVjhhFnM,16
+fairbench-0.2.2.dist-info/RECORD,,
```

